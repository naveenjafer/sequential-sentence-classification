<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<Document xmlns:gate="http://www.gate.ac.uk" name="A33_C03_Capturing_and_Animating_Occluded_Cloth_RHETORICAL_v1.xml">


  
    3b66423b6a961bf14efb5906c627af1994d8e9a71deed9c700fe0368008b508b
    3wxx
    http://dx.doi.org/10.1145/1275808.1276420
  
  
    
      
        <Title>Capturing and Animating Occluded Cloth</Title>
      
      Keenan Crane Ryan White† of California Berkeley
      †University
      
        
        Figure 1: We reconstruct a stationary sleeve using thousands of markers to estimate the geometry (texture added with bump mapping).
      
      <Abstract>
<Sentence inAbstract="true" rhetoricalClass="DRI_Approach">We capture the shape of moving cloth using a custom set of color markers printed on the surface of the cloth.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Outcome">The output is a sequence of triangle meshes with static connectivity and with detail at the scale of individual markers in both smooth and folded regions.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Approach">We compute markers’ coordinates in space using correspondence across multiple synchronized video cameras.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Approach">Correspondence is determined from color information in small neighborhoods and refined using a novel strain pruning process.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Approach">Final correspondence does not require neighborhood information.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Approach">We use a novel data driven hole-filling technique to fill occluded regions.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Outcome">Our results include several challenging examples: a wrinkled shirt sleeve, a dancing pair of pants, and a rag tossed onto a cup.</Sentence> <Sentence inAbstract="true" rhetoricalClass="DRI_Outcome">Finally, we demonstrate that cloth capture is reusable by animating a pair of pants using human motion capture data.</Sentence>
</Abstract>
      
        
          D.A. Forsyth University of Illinois
        
        
          Urbana Champaign
        
      
    
    
      
        <H1>1 Introduction</H1>
      
      <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We capture the motion of cloth using multiple video cameras and specially tailored garments.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">The resulting surface meshes have an isometric parameterization and maintain static connectivity over time.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Over the course of roughly half a dozen papers on cloth capture a prevailing strategy has emerged.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">First, a pattern is printed on the cloth surface such that small regions of the pattern are unique.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Next, correspondence is determined by matching regions across multiple views.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">The 3D location of a region is determined by intersecting rays through the corresponding observations in the image set ( figure 4 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Reconstruction is done independently on a frame by frame basis and the resulting data is smoothed and interpolated.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Previous work, such as <CitSpan>[Scholz et al. 2005]</CitSpan>, yields pleasing results.</Sentence>
      
        
      
      <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Little work has been done to capture garments with folds and scenes with occlusion.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In this paper we use folding to refer to local phenomena such as wrinkles around a knee and occlusion to refer to large scale effects such as one limb blocking the view of another.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Folds and occlusion are common, especially when dealing with real garments such as pants where limbs block interior views and cloth collects around joints.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Both phenomena are symptoms of the same problem: views of the surface are blocked by other parts of the surface.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">However, there is a distinction in scale and different methods are required to solve each problem.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">When a surface is heavily folded, contiguous visible regions are often small and oddly shaped.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">In these regions correspondence is essential for detailed reconstruction yet can be challenging to identify.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We solve the correspondence problem both by improving the pattern printed on the surface of the cloth and by improving the method used to match regions.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">Our method gets more information per pixel than previous methods by drawing from the full colorspace instead of a small finite set of colors in the printed pattern.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Additionally, because cloth cannot stretch much before ripping, we use strain constraints to eliminate candidates in an iterative search for correspondence.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">In combination, these two modifications eliminate the need for neighborhood information in the final iteration of our algorithm.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">As a result, we determine correspondence using regions that are 25 times smaller than in previous work ( figure 6 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Many regions on the surface are impossible to observe due to occlusion.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We fill these holes using reconstructions of the same surface region taken from other points in time.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We found that MeshIK (<CitSpan>[Sumner et al. 2005]</CitSpan>), a tool originally developed for mesh posing and animation, is appropriate for filling holes in cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In fact, MeshIK is well-suited to cloth data and we use it to bind reconstruction of our pants to motion capture data.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We suggest two tools to evaluate marker-based capture systems.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The first, markers per megapixel, is a measure of efficiency in capture systems.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge_Goal">Efficiency is important because camera resolution and bandwidth are expensive: the goal is to get more performance from the same level of equipment.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">This metric is designed to predict scaling as technology moves from the research lab to the professional studio.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The second tool is information theory: we look at the predictive power of different cues in a capture system.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">By doing simple bit calculations, we direct our design efforts more appropriately.</Sentence>
      Acquisition (Section 4)
      Mesh Processing (Section 5)
      images Process color neighborhoods Match local with reconstruct 3D Prune strain with
      
        Figure 2: We construct an animated sequence of surface meshes in two stages: acquisition and mesh processing. In acquisition, we convert raw images into a 3D point cloud. In mesh processing, we triangulate the mesh, fill the holes and apply temporal smoothing.
      
      
        <H1>2 Previous Work</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Previous work in cloth motion capture has focused on placing high density markers in correspondence between multiple views.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">The primary challenge is to increase marker density while correctly assigning correspondence between markers.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We suggest markers per megapixel as an appropriate metric for comparison ( figure 3 ) because it measures the method instead of the equipment.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Most high density full frame-rate capture has focused on cloth, however, there has been some recent work enhancing human motion capture <CitSpan>[Park and Hodgins 2006]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">These methods have far fewer markers per megapixel because they affix individual markers.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">When working with cloth, markers are typically painted on the surface.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">These markers can be broken into three categories: complex surface gradients <CitSpan>[Pritchard and Heidrich 2003; Scholz and Magnor 2004; Hasler et al. 2006]</CitSpan> (typically detected using SIFT descriptors <CitSpan>[Lowe 2004]</CitSpan>), intersecting lines <CitSpan>[Tanie et al. 2005]</CitSpan> and regions of constant color <CitSpan>[Guskov and Zhukov 2002; Guskov et al. 2003; Scholz et al. 2005]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our work falls in the third category: regions of contant color.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We evaluate previous work by examining the quality of the reconstructed cloth in still images and video.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">The most common errors are marker mismatches and are observable in reconstructions by local strain in the reconstructed surface.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Overall, we observe that constant color markers perform the best.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">
<CitSpan>[Pritchard and Heidrich 2003]</CitSpan> used cloth with unique line drawings as markers.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Their work identifies parameterization as one of the key aspects of cloth capture.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">They use a stereo camera to acquire 3D and SIFT descriptors to establish correspondence.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">These descriptors are often mismatched and require significant pruning.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">They introduce a rudimentary strain metric, as measured along the surface, to rule out incorrect matches.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">While successful, their static reconstructions show numerous correspondence errors.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">The real-time system described in <CitSpan>[Guskov et al. 2003]</CitSpan> introduces markers of constant color, resulting in significantly fewer correspondence errors than in <CitSpan>[Pritchard and Heidrich 2003]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">This system uses a Kalman smoothing filter and is heavily damped.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Additionally, the complexity of the color pattern limits the method to simple geometry.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">
<CitSpan>[Scholz et al. 2005]</CitSpan> improve upon <CitSpan>[Guskov et al. 2003]</CitSpan> by creating a non-repeating grid of color markers.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Each marker has five possible colors and all three by three groups are unique.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">This allows substantially larger sections of cloth and virtually eliminates correspondence errors.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Results include a human wearing a shirt and a skirt captured using eight 1K x 1K cameras.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">However, the range of motion is limited to avoid occlusion (e.g., arms are always held at 90 degrees to the torso).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">They use thin-plate splines to fill holes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">
<CitSpan>[White et al. 2005]</CitSpan> introduce a combined strain reduction/bundle adjustment that improves the quality of the reconstruction by minimizing strain while reconstructing the 3D location of the points on the surface of the cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">
<CitSpan>[White et al. 2006]</CitSpan> introduce the use of silhoutte cues to improve reconstruction of difficult to observe regions.</Sentence> While silhouette cues improve reconstruction, hole filling is
        point static cloud Hole Temporally connectivity Mesh fill smooth triangle mesh Section 5.1 Section 5.2 Section 5.3
        <Sentence inAbstract="false" rhetoricalClass="Sentence">Markers per Work Megapixels Markers† Megapixel Park 2006 48 ≤ 350 ≤ 7.3 Tanie 2005 10 407.9 40 Guskov 2003 0.9 ≤ 136 ≤ 148 Scholz 2005 8 ≤ 3500 ≤ 434 Sleeve 15 7557 504 Pants 2.5 2405.3 979
        more effective in many circumstances because it enforces an appropriate prior on the shape of the cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">We make three main contributions: we improve the color pattern and matching procedure to get more information per marker, we introduce strain constraints to simplify correspondence and we create a data driven hole filling technique that splices previously captured cloth into the mesh.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">As a result, our system is capable of capturing a full range of motion with folding and occlusion.</Sentence>
      
      
        <H1>3 Analyzing Acquisition Methods</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To acquire a 3D point cloud of the cloth surface, we print a colored pattern on the cloth, sew it together, and record its motion using multiple synchronized cameras.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We then reconstruct the 3D location of surface points by detecting corresponding points in multiple views ( figure 4 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge_Goal">Our goal is high marker density in the 3D reconstruction – especially in regions with high curvature.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To achieve this, we need markers that are both small in scale and highly discriminative.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These two goals are in tension: small markers are less discriminative.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In addition, we cannot increase camera resolution without bound because camera bandwidth becomes very expensive.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">As a result, we opt for the smallest markers that we can reliably detect and we make small markers more distinctive.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We combine information from three cues to establish correspondence: marker color, neighboring markers and strain constraints in the reconstruction.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Marker color and strain constraints are more useful than neighboring markers because they place fewer requirements on local cloth geometry.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Specifically, neighboring markers are observed only when the cloth is relatively flat.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">When the surface is heavily curved only small portions of the surface are visible before the cloth curves out of view.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In subsequent sections we adopt the following strategy: maximize information obtained from marker color and eliminate the information needed from neighbors.</Sentence>
        parametric domain
        
          <H2>3.1 Entropy as an Analytical Tool</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We optimize our correspondence technique by analyzing the information provided by different cues.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In this framework we can accurately minimize the number of neighbors required for correspondence and observe folds better.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We can compare our work to previous methods using this framework ( figure 6 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">It takes log 2 M bits to determine the identity of each observed marker on a garment with M total markers.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Because independent information adds linearly, we can compute the information needed to meet this threshold by adding information from the different cues: color, neighbors and strain.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">However, structural ambiguities in the pattern subtract information lost to determine which neighbor is which.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">As a result, we compute our information budget (I ) as:
          N = number of observed neighbors C = color information per marker A = information lost to structural ambiguities S = information gained from strain constraints I = (N + 1) ∗C + S − A
          As an example, imagine a rectangular grid of markers and a correspondence method that uses a single immediate neighbor.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This neighbor is one of four possible neighbors – thus it takes two bits to specify which neighbor we found (A = 2).</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">In this case, the equation reduces to I = 2 ∗C − 2 + S. Given almost any structured pattern, we can detect regions by increasing N until I &gt; log 2 (M) bits.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">However, larger marker regions have the disadvantage that curvature can cause local occlusions and prevent observation of the entire region.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our best efforts are to improve C – the number of bits from each marker observation.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We do this by picking marker color from the full colorspace instead of a small discrete set of colors.</Sentence>
          
            
            
          
        
        
          <H2>3.2 Garment Design and Color Processing</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We print a random colored pattern on the surface of cloth in an attempt to maximize the information available per pixel.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">While our pattern is composed of tesselated triangles ( figure 5 ), any shape that tiles the plane will work (squares and hexagons are also natural choices).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To maximize the density of reconstructed points, we print the smallest markers that we can reliably detect.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To maximize the information contained in the color of each marker, we print colors that span the gamut of the printer-camera response, then use a gaussian color model (section 4.1).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">From a system view, the printer-camera response is a sequence of lossy steps: we generate a color image on a computer, send the image to the printer, pose the cloth, and capture it with a camera.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Our experiments suggest that loss is largely attributable to camera response because larger markers produced substantially more information.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Illumination is also problematic and takes two forms: direct illumination on a lambertian surface and indirect illumination.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To correct for variations in direct illumination, we remove the luminosity component from our color modelling.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We do not correct for indirect illumination.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Each marker in the printed pattern has a randomly chosen color, subject to the constraint that neighboring marker colors must be dissimilar.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In the recognition stage, we detect markers by comparing colors to a known color.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These comparisons must be made in the proper color space: we photograph the surface of the printed cloth with our video cameras to minimize the effect of non-linearities in the printing process.</Sentence>
        
      
      
        <H1>4 Acquisition</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The goal of our acquisition pipeline is to compute correspondence using minimal neighborhoods.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We accomplish this through an iterative algorithm where we alternate between computing correspondence and pruning bad matches based on those correspondences.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">After each iteration we shrink the size of the neighborhood used to match.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We start with N = 3 and end with N = 0.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In the final iteration, markers are matched using color and strain alone.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This iterative approach allows us to match without neighborhoods.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This is better than label propagation methods.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">To be successful, propagation methods [Guskov et al. 2003; Scholz et al. 2005; Lin</Sentence>
        parametric domain
        1 st 2 nd 4 th <CitSpan>[Scholz iteration iteration iteration 2005]</CitSpan>
        Relative Area 15.8 11.8 4.0 100
        
          C
          Color ≥ 5 ≥ 5 ≥ 5 1.93
        
        Neighbors (N) 3 2 0 8
        
          S
          Strain 0 ∼ 7 ∼ 9 –
        
        Ambiguities (A) 1.6 1.6 0 3 Total bits (I ) 18.4 20.4 14 14.4
        
          Figure 6:
        
        <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">Our correspondence algorithm iterates from large to small regions.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">At each stage, the number of recovered bits must stay above the marker complexity (11.6 bits for our pants).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">We are able to obtain significantly more information per unit cloth surface area than previous work.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">See section 3.1 for the entropy equation and appendix B for detailed analysis.</Sentence>
        <Sentence inAbstract="false" rhetoricalClass="Sentence">and Liu 2006] require large sections of unoccluded cloth and must stop at occluding contours.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">As shown in figure 5 , occluding contours are both common and difficult to detect.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In contrast, our iterative approach relies on strain constraints – which require computing the distance between a point and a line, and color detection – which requires averaging color within a marker.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Both of these computations are easier than detecting occluding contours.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">We describe our acquisition pipeline, shown in figure 2 , below.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">Color Processing: We compare observed colors with stored values using a gaussian noise model.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our gaussian noise model has a single free parameter, the variance, which must be computed empirically for each recording setup.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This variance determines the color response for the entire setup — smaller variances mean more bits from color.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">At this stage, we compute color information for each marker and eliminate hypothetical correspondences from further consideration that have large color differences.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">Neighborhood Matching: At each iteration, we match highly distinctive neighborhoods by combining information across cues.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The size of the neighborhood is chosen so that we get more than enough bits to meet our information budget (log 2 M bits – typically 11 to 13).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">The analysis in figure 6 shows that we can set N = 3 at the start and continue until N = 0.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Because the identity of the marker is overspecified, there are few mistakes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This approach works from flat regions in the first iteration to foldy regions in the later iterations.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In the first iteration, we require three neighbors to make a match.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In heavily folded regions, often neighboring markers on the image do not neighbor on the surface of the cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">As such, these regions are not going to match.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In contrast, in the last iteration, no neighbors are necessary.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Occluding contours, which are common in heavily folded regions, no longer disrupt the matching procedure.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">3D Reconstruction: Markers that are observed in multiple views (at least 2) are reconstructed in 3D using textbook methods <CitSpan>[Hartley and Zisserman 2000]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use reprojection error to prune bad matches (reprojection errors average 0.3 pixels and we discard points with errors larger than 2 pixels).</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">Pruning with Strain: We do two separate strain pruning steps: one on reconstructed 3D points and one on marker observations in each image.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The first discards reconstructed points that cause physically unrealistic strain on the surface of the mesh and the second constrains our search for correspondence.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our strain constraint is based on the work of <CitSpan>[Provot 1995]</CitSpan> who noted that strain in cloth does not exceed 20% in practice.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Relaxing the constraint to distances in 3D (surface distance is always more than the distance in 3D), we can use strain to exclude possible correspondences.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Strain naturally fits in to our information theory framework: if strain excludes 87.5% of the possible correspondences, then strain has added 3 bits (because log 2 (1 − 0.875) = −3).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">The strain cue is described in figure 7 .</Sentence>
        A A B = possible identities for B = locations too close to A
        
          Figure 7: Top: we compute the shortest distance between a known point A and the eye ray through unidentified image point B. Bottom: in the parametric domain, this distance restricts the possible identities of B to the green region. The distance from A to B along the surface can be no shorter than the shortest distance in 3D.
        
        
          <H2>4.1 Representation</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To find correspondence, we match each image marker to a marker in the parametric domain.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To do this, we define affinities a i, j between image marker i and parametric marker j.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Each affinity is a product over different cues.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">We write c i, j ∈ <CitSpan>[0, 1]</CitSpan> for the color affinity, d(C i ,C j ) for the color distance between i and j, s i, j ∈ {0, 1} for the strain constraint, n i for the image neighbors of marker i and N j for the parametric neighbors of marker j:
          a i, j = c i, j s i, j ∏ max c k,l l∈N j k∈n i d(C i ,C j ) 2 c i, j = exp (− 2 σ 2 ) 0 if a strain constraint is violated s i, j = 1 if not
          When only one affinity for image marker i is above a theshold, then we declare a correspondence.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Initially, we learned this threshold from labelled data, but we found that changing it by several orders of magnitude had little effect on our results.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Subsequently, we use the value 10 −5(N+1) where N is the number of neighbors.</Sentence>
        
      
      
        <H1>5 Mesh Processing</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">In the acquisition process, occlusion inevitably creates holes in the reconstructed mesh ( figure 8 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge_Goal">One would like to fill these holes with real cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">One of our major contributions is a data driven approach to hole filling: we fill holes with previously observed sections of cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our work differs from <CitSpan>[Anguelov et al. 2005]</CitSpan> because our hole filling procedure does not assume a skeleton that drives the surface and our procedure estimates a single coefficient per example.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This hole filling procedure has a number of requirements: the missing section needs to be replaced by a section with the same topology; the new section needs to obey a number of point constraints around the edge of the hole, and the splicing method should respect properties of cloth (specifically strain).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We select a reconstruction technique based on deformation gradients <CitSpan>[Sumner and Popovic 2004]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In this approach, we fit deformation gradients for the missing section to a combination of deformation gradients in other observed sections.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Then, we reconstruct the point locations from the deformation gradients.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This procedure has a number of advantages.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">First, deformation gradients naturally yield cloth like properties.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Deformation gradients are the transformation matrix between triangles in two poses of the mesh.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">By penalizing elements that deviate in this matrix, we have a fairly direct penalty on large changes in scale or strain.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">In contrast, methods based on the Laplacian of the mesh (<CitSpan>[Sorkine et al. 2004]</CitSpan>) do little to penalize these strains and can show many artifacts around the edge of the mesh.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Second, deformation gradients can be converted into vertex locations by inverting a linear system, allowing us to specify vertex locations as constraints.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Methods such as <CitSpan>[Lipman et al. 2005]</CitSpan> don’t allow vertex constraints.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">Our subsequent discussion is divided into three sections: constructing a mesh from the point cloud, filling the holes in the mesh using deformation gradients, and temporally smoothing the results.</Sentence>
        Example Meshes
        constraints examples
        triangles: multiview single view (unused) unobserved errors (discarded) seam backface
        
          
        
        
          <H2>5.1 Meshing and Seams</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We produce a mesh by forming equilateral triangles for sections of cloth that are printed with a contiguous pattern by referencing the triangle stucture of markers on the cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our recovered markers are at the center of each triangle – so we average points to get out the vertices and subsequently the original mesh.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We insert artificial points where two pieces of fabric come together.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These points are created once per garment by hand clicking on photos of the each seam.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The 3D locations of these points are recreated in each frame by averaging points near the seam.</Sentence>
        
        
          <H2>5.2 Hole Filling</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use occlusion free meshes from other frames to automatically interpolate holes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For each hole in each frame, we cut out the missing region plus a ring of two triangles around the region.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We select a set of examples of the enlarged region, then use MeshIK (<CitSpan>[Sumner et al. 2005]</CitSpan>) to reconstruct the surface.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">MeshIK works by choosing a combination of deformation gradients from the examples and then solving for the missing point locations.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use the points from the ring of known triangles around the hole as constriants in MeshIK.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The most restrictive aspect of MeshIK is that it requires example meshes without holes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In practice, we never observe complete ex- ample meshes – each mesh is missing some triangles.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These holes appear in different places in different meshes and we create complete meshes in an iterative method.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">First, we fill all holes with a naive linear algorithm (specifically, we triangulate across gaps and use barycentric coordinates to place the missing points – this gets the job done, but works poorly).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Then, we do another pass through all the data, where we replace the linear sections with sections created using MeshIK on the linearly filled data.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To downweight the linear data, we select the examples with the highest percentage of viewed points in the missing section.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These frames are then used as examples in MeshIK to hole fill in the rest of the sequence.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For the pants capture, we iteratively refine a set of 27 extreme poses which were captured specifically for filling holes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The advantage of this apporach is that the example poses are chosen to capture the relevant degrees of freedom – yielding better results.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For the cloth toss sequence, we chose the simpler approach: iteratively refine the entire sequence.</Sentence>
          solution ? MeshIK
        
        
          <H2>5.3 Smoothing</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We introduce flexibility preserving smoothing – a method similar to anisotropic diffusion <CitSpan>[Perona and Malik 1990]</CitSpan> that smoothes near-rigid movement without effecting flexible deformation.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Typical temporal smoothing is dangerous because fast non-rigid movements can easily become physically implausible when blurred over time.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">However, because fast non-rigid regions of the cloth are complex, small temporal errors are often difficult to notice.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">In contrast, small errors in regions of the cloth that move rigidly are typically easy to observe.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">As a result we use flexibility preserving smoothing, a procedure that smoothes rigid movement more heavily than non-rigid movement.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To do this, we take a local region around each vertex in the mesh (typically 25 points) and compute a rigid transformation to previous and subsequent frames.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Aligning the regions with this transformation, we compute the movement of the vertices in this reference frame as a proxy for rigidity.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Large variations in location indicate non-rigid movement and consequently receive little smoothing.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Smaller variations indicates rigid movement and benefit from more substantial smoothing.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use a size adjusted gaussian to smooth in this reference frame.</Sentence>
        
      
      
        <H1>6 Results and Applications</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our video sequences were taken with synchronized firewire cameras (Foculus FO214C) with a capture resolution of 640 x 480 and a capture rate of 24 frames per second.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our still captures were taken using a digital SLR camera and then downsampled to approximate available video resolutions.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use the automated calibration technique in <CitSpan>[White and Forsyth 2005]</CitSpan>, but any standard calibration will work (<CitSpan>[Zhang 2002] and [Bouguet 2005]</CitSpan> are good choices).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In the pants sequences, we used seven lights totalling 1550 Watts to illuminate the scene.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Adequate lighting is critical: from our experience fewer lights degrade performance due to harsh shadows and dim lighting causes motion blur through slower shutter speeds.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our cloth was printed by a digital mail order fabric printing service.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">On a P4 2.4 GHz machine, acquisition takes roughly 6 minutes and mesh processing 2 minutes per frame.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Code is written in MATLAB.</Sentence>
        
          
        
        observed unobserved backface
        
          <H2>6.1 Capture Results</H2>
          <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Our capture results are best evaluated by looking at our video and figures 1,12,13.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">However, to compare against other capture techniques, it is also necessary to evaluate on several numerical criteria for each capture session: cloth pants table drop dance cloth sleeve† # cameras 6 8 18 10 resolution 640x480 640x480 900x600 1500x1000 total markers 853 3060 4793 13465 recovered 819 2405 4361 7557 percentage 96% 79% 91% 56% bits needed 9.7 11.6 12.2 13.7 color bits 6.1 5.1 6.4 4.5 strain bits 9.1 9.4 11.4 ∼ 6.6 †The sleeve example is unique because it was one of the first items we captured.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Much of the cloth is in contact with the floor and unobservable – yielding fewer bits of strain.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">In addition, the camera images were not output in a linear color space, reducing the number of color bits.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">As a result, we terminated the correspondence algorithm at N = 2.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Our pants animation is by far the most challenging, and we analyze some of the details a little more closely.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">With an average of 2405 observed markers, there were 979 3D markers per megapixel.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">If we factor out the pixels lost to background, we get 3500 3D markers per foreground megapixel or 282 foreground pixels per recovered 3D marker.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Our marker observations average 56 pixels per marker per image.</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">There are several reasons for the discrepancy: markers must be observed multiple times (approx 44% of 3D markers are observed in 3 or more views), some markers are observed but not reconstructed (due to errors or missing correspondence), and many pixels are not considered part of a marker: they lie in heavy shadow</Sentence>
          
            
          
          <Sentence inAbstract="false" rhetoricalClass="Sentence">or occupy the edge between two markers (approx 35% of pixels).</Sentence> <Sentence inAbstract="false" rhetoricalClass="Sentence">6.2 Retargeting Animations
          We use a small set of captured frames (the previous basis of the 27 examples) in combination with MeshIK to skin skeletal human motion capture data ( figure 11 ).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">This approach covers a reasonably large range of motion, but ignores cloth dynamics.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The largest challenge is that captured cloth meshes contain only points on the cloth surface, so we do not know joint locations.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Instead, we insert proxy points for knee and hip joints in each of our basis meshes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">These points are then connected to a small set of nearby triangles in the original mesh.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For each frame of animation we set the proxy points’ locations according to joint angles in the skeletal mocap data.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The resulting transformed joints are used as constraint points in MeshIK, which produces the final output meshes.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Using our MATLAB implementation of MeshIK, this process takes around 5-10 seconds per frame.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use the same 27 bases poses for MeshIK based reconstruction.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In order for a small basis to adequately express a full range of motion, each basis pose must be an extreme configuration.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For simple objects such as a cylinder, a small bend (for example) is sufficient to extrapolate to a larger bend <CitSpan>[Sumner et al. 2005]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">However, for pants the relationship is more complex: the fact that no folding occurs in a small bend does not imply that folding will be absent in a larger bend.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Conversely, if a decent amount of folding occurs in a small bend, we do not expect extreme folds in a corresponding larger bend.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">As a result, MeshIK is most useful when a basis is carefully chosen to prevent extrapolation artifacts.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">One drawback to our approach is the loss of secondary kinematic motion, such as the sway of loose cloth.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Because MeshIK does not use velocity information, the resulting animation appears damped.</Sentence>
        
      
      
        <H1>7 Discussion</H1>
        <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome_Contribution">We have brought cloth capture from constrained laboratory examples to real settings by providing robust methods for dealing with occlusion and folding.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Like human motion capture, this tool requires significant engineering effort.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Camera setup and calibration are time consuming and the equipment is costly.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">However, once these obstacles have been overcome, capturing large amounts of data is relatively easy.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">So that other researchers can benefit from our work, we are releasing our capture data at http://www.ryanmwhite.com/data.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">In our video, we show some of the uses of this data, including editing using <CitSpan>[Kircher and Garland 2006]</CitSpan> and posing using <CitSpan>[Sumner et al. 2005]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_FutureWork">Future work in cloth capture should involve more cameras, higher resolution (leading to smaller denser markers), different garments and different materials.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_FutureWork">We plan to pursue more tools to edit and repurpose captured data.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Unspecified">Finally, we would like to conclude with a discussion about cloth capture in the context of other cloth animation techniques.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Simulation and image based rendering both provide methods to generate animation of cloth (a limited simulation list includes <CitSpan>[House and Breen 2000; Terzopoulos et al. 1987; Choi and Ko 2002; Bridson et al. 2003; Baraff et al. 2003]</CitSpan> and a limited image based rendering list includes <CitSpan>[Bradley et al. 2005; White and Forsyth 2006; Lin and Liu 2006; Scholz and Magnor 2006]</CitSpan>).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">These methods have several advantages: simulation gives significant user control and produces higher resolution meshes while image based rendering techniques produce more accurate illumination.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">However, capturing large amounts of data is far easier than simulating large amounts of data and provides more control than image based rendering.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Challenge">Common simulation complaints include long computation times, significant parameter tweaking and tangling.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">In contrast, capture is relatively quick (our code is 8 minutes per frame in MATLAB); parameters are set by selecting the type of cloth <CitSpan>[Bhat et al. 2003]</CitSpan> and tangling is relatively uncommon.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Cloth capture makes it easy to capture large amounts of cloth, including fast light cloths that create instabilities in simulation.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">An added attraction of cloth capture is that complex interaction between the cloth and the body is recorded without complicated human models.</Sentence>
        References
        
          
        
      
      
        <H1>Acknowledgements</H1>
        We thank Jai Vasanth and Anthony Lobay for early support of this project, Scott Kircher and Robert Sumner for providing mesh editing binaries, and Sony Computer Entertainment America for supplying human motion capture data. This work was supported in part by a Department of Homeland Security Fellowship and an ATI Graduate Research Fellowship.

		 <H1>References</H1>
	 	
		
		 - ANGUELOV, D., SRINIVASAN, P., KOLLER, D., THRUN, S.,RODGERS, J., AND DAVIS, J. 2005. Scape: shape completion and animation of people. In SIGGRAPH. BARAFF, D., WITKIN, A., AND KASS, M. 2003. Untangling cloth. In SIGGRAPH. BHAT, K., TWIGG, C., HODGINS, J. K., KHOSLA, P., POPOVIC,Z., AND SEITZ, S. 2003. Estimating cloth simulation parameters from video. In SCA. BOUGUET, J.-Y., 2005. Camera calibration toolbox for - matlab. http://www.vision.caltech.ed u/bouguetj/calib doc/. BRADLEY, D., ROTH, G., AND BOSE, P. 2005. Augmented clothing.In Graphics Interface. BRIDSON, R., MARINO, S., AND FEDKIW, R. 2003. Simulation of clothing with folds and wrinkles. In SCA. CHOI, K.-J., AND KO, H.-S. 2002. Stable but responsive cloth.In SIGGRAPH. FORSYTH, D., AND PONCE, J. 2002. Computer Vision: a modern approach. Prentice-Hall. GUSKOV, I., AND ZHUKOV, L. 2002. Direct pattern tracking on flexible geometry. In WSCG. GUSKOV, I., KLIBANOV, S., AND BRYANT, B. 2003. Trackable surfaces. In SCA. - HARTLEY, R., AND ZISSERMAN, A. 2000. Multiple View Geometry.Cambridge University Press.  HASLER, N., ASBACH, M., ROSENHAHN, B., OHM, J.-R., AND SEIDEL, H.-P. 2006. Physically based tracking of cloth. In VMV.  HOUSE, D., AND BREEN, D., Eds. 2000. Cloth Modelling and Animation. A.K. Peters.  KIRCHER, S., AND GARLAND, M. 2006. Editing arbitrarily deforming surface animations. In SIGGRAPH.  LIN, W.-C., AND LIU, Y. 2006. Tracking dynamic near-regular textures under occlusion and rapid movements. In ECCV.  LIPMAN, Y., SORKINE, O., LEVIN, D., AND COHEN-OR, D. 2005. Linear rotation-invariant coordinates for meshes. In SIGGRAPH.  LOWE, D. 2004. Distinctive image features from scale-invariant keypoints. IJCV.  PARK, S. I., AND HODGINS, J. K. 2006. Capturing and animating skin deformation in human motion. In SIGGRAPH.  PERONA, P., AND MALIK, J. 1990. Scale-space and edge detection using anisotropic diffusion. PAMI.  PRITCHARD, D., AND HEIDRICH, W. 2003. Cloth motion capture. Eurographics.  PROVOT, X. 1995. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In Graphics Interface. SCHOLZ, V., AND MAGNOR, M. A. 2004. Cloth motion from optical flow. In VMV. SCHOLZ, V., AND MAGNOR, M. 2006. Texture replacement of garments in monocular video sequences. In Rendering Techniques. SCHOLZ, V., STICH, T., KECKEISEN, M., WACKER, M., AND MAGNOR, M. 2005. Garment motion capture using color-coded patterns. In Eurographics. SORKINE, O., COHEN-OR, D., LIPMAN, Y., ALEXA, M., R ̈OSSL, C., AND SEIDEL, H.-P. 2004. Laplacian surface editing. In Symposium of Geometry Processing. SUMNER, R. W., AND POPOVIC, J. 2004. Deformation transfer for triangle meshes. In SIGGRAPH. SUMNER, R. W., ZWICKER, M., GOTSMAN, C., AND POPOVIC, J. 2005. Mesh-based inverse kinematics. In SIGGRAPH. TANIE, H., YAMANE, K., AND NAKAMURA, Y. 2005. High marker density motion capture by retroreflective mesh suit. In ICRA. TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically deformable models. In SIGGRAPH. WHITE, R., AND FORSYTH, D., 2005. Deforming objects provide better camera calibration. UC Berkeley Technical Report. WHITE, R., AND FORSYTH, D. 2006. Retexturing single views using texture and shading. In ECCV. capture. UC Berkeley Technical Report.  WHITE, R., FORSYTH, D., AND VASANTH, J., 2006. Capturing real folds in cloth. UC Berkeley Technical Report.  ZHANG, Z. 2002. A flexible new technique for camera calibration. PAMI.                      WHITE, R., LOBAY, A., AND FORSYTH, D., 2005. Cloth
		
		
		
        
          
          Figure 13: We reconstruct cloth being tossed over a cup, a tablecloth and a pair of pants (shown in the middle of a jump). See the video for a better view of the results.
        
      
      
        A Image Processing
        <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We do some pre-processing to get marker locations and connectivity from raw images.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">We recommend readers unfamiliar with these techniques refer to <CitSpan>[Forsyth and Ponce 2002]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We start by converting each image to HSV, disregarding the luminosity (V) and using polar coordinates to compute distances in hue and saturation.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">To detect markers, our code looks for uniformly colored blobs in two stages: first regions are built by growing neighborhoods based on similarity between pixels.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This method is sensitive to image noise and can produce oversized regions when the color boundaries are smoothed.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The second stage takes the center of mass of each blob from the first stage, computes the mean color and grows a region based on distance to the mean color (it is computationally intractable to use this as the first stage of the blob detection).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The process is iterated for increasing thresholds on the affinity value in the first stage, using the portions of the image where detection failed in previous stages.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Finally, blobs are thresholded based on size.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Next, we need to determine the neighborhood relationships.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For each marker, we construct a covariate neighborhood (a fitted ellipse) and vote for links to the three closest markers with similar covariate neighborhoods.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">This measures distances appropriately in parts of the scene where the cloth is receding from view and discourages links between markers with wildly different tilts.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">All links that receive two votes (one from either side) are kept while the rest are discarded.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Links that bridge markers with conflicting color information are also discarded (typically on internal silhouettes).</Sentence>
      
      
        B Entropy Comparison
        <Sentence inAbstract="false" rhetoricalClass="DRI_Background">For more reading on information theory, consult <CitSpan>[Cover and Thomas 1991]</CitSpan>.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Our analysis is based on the information entropy definition: H(X) = − ∑ n i=1 p(x i ) · log 2 x i .</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">For <CitSpan>[Scholz et al. 2005]</CitSpan>, the equation in section 3.1 is reduced to I = 9 ∗ C − A because they use 8 neighbors and no strain constraints.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">They use 5 colors which, without errors, yields C = log 2 5 bits per marker.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">They cite an error rate of five to ten percent.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">As a result, they recover anywhere from 1.65 to 2.04 bits per marker.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">In our comparison, we use C = 1.93 bits for color information from their method (five percent error, with equal probabilities for all remaining choices).</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Note that this is effectively less than four colors!</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Second, we compute structural ambiguities in their method which account for uncertainty in observations.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">The orientation of the surface is unknown, yielding four possible directions, or two bits of structural ambiguity.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">Second, in their paper, they say that oblique views cause another bit of uncertainty.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Background">As a result A = 3 bits.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">For our work, C is an empirical observation.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Depending on the lighting and camera configuration, we get anywhere from 5 to 7 bits.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">We use the conservative estimate of C = 5 bits per marker.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">Second, our mesh is triangular and there are three possible neighborhood rotations, yielding A = log 2 3 = 1.59 bits of structural ambiguity.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Approach">When neighborhoods are not used, there is no structural ambiguity.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">Strain information is difficult to compute and depends on the geometry of the surface and the orientation of the camera.</Sentence> <Sentence inAbstract="false" rhetoricalClass="DRI_Outcome">In most cases, we observe more than 9 bits of strain information.</Sentence>
      
    
  

</Document>
