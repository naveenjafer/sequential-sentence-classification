Challenge	This paper presents a method for animating fluid using unstructured tetrahedral meshes that change at each time step.
Outcome	We show that meshes that conform well to changing boundaries and that focus computation in the visually important parts of the domain can be generated quickly and reliably using existing techniques.
Challenge	We also describe a new approach to two-way coupling of fluid and rigid bodies that, while general, benefits from remeshing.
Outcome	Overall, the method provides a flexible environment for creating complex scenes involving fluid animation.
Challenge	Although systems for physically based fluid animation have developed rapidly in recent years and can now reliably generate production-quality results, they still have some limitations.
Challenge	Simulation domains can change substantially from step to step because of deforming boundaries, moving obstacles, and evolving fluid motion, yet current systems based on fixed grids are not ideally suited to handle these situations.
Approach	When generating the mesh, we use the position and shape of boundaries as well as criteria based on the visually important parts of the fluid and velocity field to construct a sizing field that dictates the desired edge length for tetrahedra throughout the domain.
Approach	We then use an efficient and reliable meshing algorithm adapted from [ Alliez et al., 2005 ] to produce a mesh that is refined according to this field.
Approach	We use unstructured tetrahedral meshes because they conform to curved and irregular boundaries better than axis-aligned grids with the same number of grid elements and allow for precise control of refinement throughout the domain.
Approach	We transfer the physical properties of the simulation from the old mesh to the new mesh using a generalization of the semi-Lagrangian velocity advection technique that introduces no additional smoothing.
Approach	We then perform a mass conservation step that has been extended to allow a new, single-step solution of two-way coupling between fluid and rigid bodies.
Outcome	Overall, this approach provides a flexible framework for fluid simulation that opens the door to many features.
Outcome	We have implemented the system and tested it in a variety of scenarios such as the one shown in Figure 1 .
Outcome	We have found that the combination of unstructured tetrahedral domains and dynamic remeshing creates a versatile environment for the creation of complex and visually interesting fluid animations.
Background	The animation of fluids through physical simulation has become an important tool in the visual effects industry.
Background	One approach that has been popular in recent years makes use of a spatial discretization based on regular, fixed, hexahedral grids.
Background	Some examples of this approach can be found in [Foster and Metaxas, 1996], [Foster and Metaxas, 1997], [Stam, 1999], [Yngve et al., 2000], [Fedkiw et al., 2001], [Foster and Fedkiw, 2001], [Enright et al., 2002], [Carlson et al., 2002], [Feldman et al., 2003], and [Goktekin et al., 2004].
Background	The most commonly used storage scheme for these approaches is the “staggered grid” scheme.
Background	This method offsets storage of different quantities on the grid, and was first described by [Harlow and Welch, 1965].
Background	Efforts have been made to enhance these methods to allow for better conformance to irregular boundaries such as the free surface of liquids, complex obstacles, or irregularly shaped domains.
Background	[Losasso et al., 2004] described an octree-based method that retains many of the advantages of regular grids while allowing computational effort to be focused in particular parts of the simulation domain; this enables detailed tracking of moving boundaries such as liquid surfaces.
Background	Both [Carlson et al., 2004] and [Guendelman et al., 2005] have demonstrated methods for two-way coupling of obstacles to fluid.
Background	Unstructured tetrahedra have also been used for fluid simulation within the graphics community.
Background	Two examples of this are [Feldman et al., 2005a] and [Elcott et al., 2005].
Background	The first method uses a velocity-based approach while the second uses a vorticity-based formulation.
Background	It is a blend of ideas from these two papers, along with a generalization of the semi-Lagrangian velocity advection technique for moving meshes described in [Feldman et al., 2005b] that forms the heart of our method.
Background	The idea of moving meshes independent of a fixed or particle-centric coordinate system is not a new one; arbitrary Lagrangian-Eulerian (ALE) methods were designed for just this purpose.
Background	They have proven useful in the simulation of highly deformable elastic materials.
Background	ALE was first described in [Hirt et al., 1974], where it was used with finite differences to solve compressible fluid problems.
Background	[Donea et al., 1977] went on to apply ALE in a finite element setting.
Background	An excellent survey of the development of ALE methods appears in [Donea et al., 2004].
Background	Examples within the graphics literature that feature moving meshes without remeshing include [Shah et al., 2004] and [Rasmussen et al., 2004], both of which translate the grid to follow the visually important portion of the fluid.
Background	Another approach to handling changing domains is to dispense with the mesh altogether, instead using Lagrangian particles for simulation of fluids.
Background	A few examples of this approach are [Terzopoulos et al., 1989], [Desbrun and Cani, 1996], [Cani and Desbrun, 1997], [Stora et al., 1999], [Müller et al., 2003], [Premo ze et al., 2003], and [Müller et al., 2004].
Background	These meshless methods are particularly well suited to changing domains because points can move freely without concerns about mesh quality.
Approach	Because we regenerate a new simulation mesh at each time step, the viability of our method hinges on fast, high-quality, reliable tetrahedral mesh generation.
Background	While a history of unstructured mesh generation is outside the scope of this paper, [Owen, 1998] and [Teng and Wong, 2000] provide good surveys of the field.
Approach	For our mesh generator we selected the approach described in [ Alliez et al., 2005 ].
Approach	This innovative method produces meshes which conform to domains of arbitrary topology quickly and reliably.
Approach	Also, it allows for the local edge length of the tetrahedra to be specified arbitrarily throughout space, which allows us to easily perform adaptive mesh refinement from step to step.
Approach	The meshes produced by this technique are Delaunay, which provides improved gradient estimation and allows us to significantly simplify some of the expressions that arise when interpolating velocity values stored on the mesh.
Outcome	The key contribution of our method is to demonstrate the freedom granted by remeshing at each simulation time step.
Approach	The core of our system is based on the simple, efficient methods for discretizing the inviscid Euler equations on tetrahe- dral meshes described in [Elcott et al., 2005] and [Feldman et al., 2005a].
Approach	We have made a few modifications in order to combine the best aspects of both approaches that are described below.
Approach	Once we have a good discretization, we need a way to propagate information from one mesh to the next.
Background	[Feldman et al., 2005b] details a generalization of the standard semiLagrangian velocity advection technique that allows simulation state to be transferred between deforming domains without incurring additional smoothing.
Approach	We demonstrate that their approach can easily be applied to transfer information between two arbitrary, topologically unrelated meshes, which is required to achieve more general evolution of the simulation domain from step to step.
Approach	Finally, we need to quickly and reliably generate a new tetrahedral mesh for each time step that suits the current simulation conditions, such as conformance to boundaries and obstacles as well as any desired refinement.
Background	Although methods have long existed to mesh arbitrary domains, most are relatively slow in comparison to simulation running times or don’t reliably terminate under realistic conditions.
Background	The availability of efficient, versatile meshing algorithms such as [ Alliez et al., 2005 ] has made the generation of a new mesh at each time step practical.
Challenge	Also, we describe a new, single-step method to achieve two-way coupling between obstacle and fluid motion.
Approach	We use a staggered fluid state storage scheme that stores pressures at tetrahedron circumcenters and “face-normal velocities,” the component of velocity in the direction of the face normal, at the face circumcenters.
Background	Similar schemes have been used in [Botta and Hempel, 1996], [Elcott et al., 2005] and [Feldman et al., 2005a].
Background	These methods are a generalization of the staggered grid scheme originally proposed by [Harlow and Welch, 1965].
Approach	This staggered method is used to discretize the inviscid Euler equations:
Approach	In these equations, u is the fluid velocity, t time, p pressure, ρ density, and f any external forces.
Approach	The symbol denotes T the vector of differential operators = [∂/∂x, ∂/∂y, ∂/∂z] .
Approach	We account for the changes in the mesh over a time step directly during semi-Lagrangian advection (see Section 3.2).
Approach	Divergence and gradient operators are needed as part of the mass conservation step.
Approach	We make discrete estimates of these derivatives following the formulation presented in [Losasso et al., 2004] and [Elcott et al., 2005].
Approach	The divergence of a tetrahedron is computed as an area weighted sum of the tetrahedron’s face normal velocities.
Approach	The gradient at a face circumcenter in the direction of the face’s normal is computed using finite differences.
Approach	The difference in circumcenter pressures adjacent to a face is divided by the distance between these circumcenters.
Approach	In Delaunay meshes, the line connecting adjacent tetrahedra circumcenters passes through the circumcenter of the face between them and is in the direction of that face’s normal.
Approach	This property of Delaunay meshes motivates our storage scheme at circumcenters because the gradient estimate is equivalent to the gradient of a piecewise linear function that interpolates the circumcenter values.
Approach	The staggered scheme stores only the component of velocity in the face normal direction.
Approach	For both the semi-Lagrangian step and to advect smoke particles for rendering, a full velocity vector must be found at arbitrary positions in the mesh.
Approach	We interpolate velocity vectors from face normal velocities using the two-step method developed in [Elcott et al., 2005].
Approach	First, a velocity vector, u t , is computed at each tetrahedron circumcenter, then we interpolate within Voronoi cells using u t values at the cell vertices.
Approach	Velocity u t for tetrahedron t is found by solving the small linear system N t u t = z t where N t is a matrix containing 4 rows of the face normals of t and z t is a vector of the 4 face normal velocities associated with t.
Approach	For a divergence-free field, this solution has the remarkable property that interpolating back to the face circumcenters exactly recovers the original face-normal velocities.
Approach	Thus interpolating the u t velocities also exactly interpolates the face-normal velocity components, and does not incur the error one would otherwise expect from a twostep interpolation method.
Approach	To find a velocity at an arbitrary point we interpolate within the Voronoi cell using the tetrahedra velocities associated with the cell.
Approach	This interpolation is based on the method of [Warren et al., 2004], which presents a way to interpolate within a general convex polytope.
Approach	Here, σ t is the set of polytope faces that intersect at node t.
Approach	The denominator is the product of distances from x to the faces in σ t computed using the face normals,n f , and plane offsets, d f .
Approach	|N t | is the determinant of a matrix of face normals in σ.
Approach	Weights from all nodes are normalized to sum to 1 before use in the weighted sum.
Approach	To simplify this computation we take advantage of two properties: 1) in a Delaunay mesh, edges are in the direction of the Voronoi cell’s face normals and 2) the volume of tetrahedron t is 1/6|E t | where E t is a matrix formed from the three vectors of edges emanating from a common node of t.
Approach	A similar observation appears in [Ju et al., 2005], and we find that with it the velocity interpolation is quite efficient.
Approach	All quantities appearing in Equation (4) are already stored for use in other parts of the timestep, saving the need to compute the terms in Equation (3).
Approach	When advecting large numbers of particles, velocities at nodes of tetrahedra can be first be found using Equation (4) and then quickly interpolated in a linear fasion over the tetrahedra to advect the particles.
Background	The simple and stable semi-Lagrangian method has become the standard tool for advection of the velocity field for graphical applications [Stam, 1999].
Approach	This method does not rely on velocities being stored at any particular place, as long as the velocity can be interpolated throughout space.
Approach	We can extend this technique naturally to meshes which change arbitrarily at each time step as in [Feldman et al., 2005b].
Approach	This extension does not incur any additional smoothing compared to using semi-Lagrangian advection with static meshes.
Approach	Suppose at time t velocities are stored at locations x (t) (in our case, the face circumcenters), and we want to find (t) the velocity at a particular face location x i .
Approach	We trace back (t) from x i through the velocity field of the previous time step to a point x i , which has no necessary correspondence to any feature of the old mesh.
Approach	Then, we update the velocity at (t) x i to the value interpolated from the old velocity field at x i .
Approach	Because the velocities from the previous step are stored on a different mesh, we have to trace back and interpolate using this previous mesh (see Figure 2 ).
Approach	The domain boundaries, obstacles, and smoke are free to move and change from step to step of the simulation.
Approach	By regenerating the mesh at each time step we can ensure that our domain conforms well to boundaries and is refined in visually important areas.
Approach	We accomplish this by using the variational tetrahedral meshing algorithm presented in [ Alliez et al., 2005 ].
Approach	This method allows for generation of tetrahedral meshes that conform well to an arbitrary input surface mesh, have no restrictions on topology (i.e., allow nested voids), and allow for sizing of tetrahedra throughout the domain based on arbitrary criteria.
Approach	Our implementation differs from the original algorithm in a couple of details.
Approach	As in the original method, refinement of the mesh is controlled by a sizing function μ(x) that, for any point x in the simulation domain, returns the desired local edge length of the tetrahedra.
Approach	In this equation, k 0 is an offset value that controls the minimum value of the sizing field, and hence the minimum local edge length of tetrahedra.
Approach	d(x) is the distance to the closest obstacle or boundary which demands refinement, s(x) is a function of the density of smoke particles, and ω(x) is a function of the vorticity of the velocity field.
Approach	The parameters k d , k s , and k ω respectively control the weight each of these functions has on the sizing field.
Approach	These three factors are the same as those used for octree refinement in [Losasso et al., 2004].
Approach	The overall goal of the sizing field is to focus computational effort in the most visually important parts of the scene, that is, near closed boundaries, where the velocity field varies most, and where smoke is visible.
Approach	This meshing method is iterative, so the mesh from the previous simulation time step can be used as an initial guess for the node placement in the mesh at the next simulation time step.
Approach	Because there is, in general, strong temporal coherence between steps of the simulation, the sizing field does not change too much and so the nodes from the previous step are often a good initialization.
Approach	Before the algorithm proceeds, the initial node placement is corrected to match the sizing field of the current step.
Approach	One other modification we made to the algorithm is that, when optimizing the node positions, we move nodes to the average of the barycenters of the surrounding tetrahedra instead of the circumcenters.
Approach	We have found that while this tends to slightly decrease the average quality of tetrahedra in the mesh, it often leads to substantial improvements in the quality of the worst elements of the mesh, which are of more concern for numerical simulation.
Approach	Of course, remeshing takes time, so it is important to consider the impact it has on overall simulation performance.
Outcome	The time spent generating meshes for each simulation step varies, but generally accounts for less than a quarter of the overall simulation time.
Challenge	The motion of fluid and rigid bodies that mutually effect each other can be complex and visually appealing.
Challenge	The interaction occurs as a consequence of the conditions that:          1.
Challenge	The velocities in the normal direction are the same at the interface of the fluid and the rigid body surface.
Challenge	The fluid velocity is divergence free and the rigid body velocity is rigid.
Challenge	The linear and angular momentum of the combined system is conserved.
Background	In [Carlson et al., 2004] these conditions are enforced sequentially.
Background	While for many cases this produces results that look very good, under some situations artifacts can be created because enforcing one of the conditions in general will break a previously enforced one.
Background	Examples of such artifacts might be fluid leaking through solid boundaries or poor performance in piston-like situations.
Approach	Our implementation differs from [Carlson et al., 2004] in a couple of ways, but most significantly we enforce these conditions simultaneously within the mass conservation step.
Approach	In general, the mass conservation step solves for pressures that accelerate the velocity field to be divergence free.
Background	In previous works, including those with two-way coupling, the mass conservation step treats faces to behave as fluid or explicitly prescribes their velocities.
Background	For fluid faces, the pressure accelerates the velocity proportional to the gradient of the pressure while for prescribed faces, the pressure does not effect the fluid.
Background	For a more complete discussion of fluid/prescribed-velocity mass conservation see [Fedkiw et al., 2001].
Approach	We extend mass conservation to include a dynamic, rigid body.
Approach	To do so, we solve for acceleration of the fluid and the rigid body, ignoring pressure for both.
Approach	We then solve for a pressure term that satisfies boundary and incompressibility constraints to find the final accelerations.
Approach	The rigid body accelerations can be computed by creating a matrix R that is multiplied by a vector of the pressures that surround a rigid body.
Approach	R can be formed by a series of matrix multiplications:
Approach	The rightmost matrix finds the net force-torque couple acting on a rigid body by summing up the contribution due to pressure forces acting on rigid body mesh faces.
Approach	M is a diagonal matrix with the mass of the rigid body on the diagonals and I is the inertia matrix.
Approach	The leftmost matrix in the multiplication returns the acceleration of the fluid-rigid faces in the direction of the face normal due to the linear and angular acceleration of the rigid body.
Approach	By construction, accelerations generated by this matrix behave rigidly.
Approach	Computing pressure accelerations of both the fluid and fluid-rigid faces can be expressed as a matrix A multiplied by a vector of all the pressures.
Approach	A row of A that corresponds to a face with fluid on both sides contains the same entries as the standard gradient matrix multiplied by −1/ρ.
Approach	With A built, mass conservation including two way coupling proceeds much in the same way as in the all-fluid case, with A replacing the role of the discrete gradient matrix.
Approach	For a given vector of pressures, p, the intermediate velocity field, z ∗ , is accelerated to the end-of-step velocity, z, by z = z ∗ + ∆tAp.
Approach	For the fluid faces, z ∗ is found by applying all terms of Equation (1) except the pressure term.
Approach	For the fluid-rigid faces, z ∗ is found using a rigid body simulator without pressure forces applied.
Approach	This linear system can be solved efficiently using PCG since the the matrix DA, which replaces the discrete Laplacian from the all fluid case, is also a positive-definite symmetric matrix.
Approach	Using the same machinery, we can also interact with constrained rigid bodies.
Approach	This simply requires finding an R matrix that correctly computes face accelerations due to pressure.
Approach	For example, one could easily alter R such that the body was constrained to just rotate about the origin by replacing b i in Equation (6) with b i = (r i × n i ) T and using only the I −1 block for the center matrix.
Outcome	This idea could be extended further to include even articulated bodies.
Approach	We implemented the method described above in matlab 1 and C, making use of Pyramid [Jonathan Shewchuck, personal communication] for Delaunay triangulation and pixie 2 for all renderings.
Outcome	Typical simulation times for meshes with 100,000 tetrahedra were about 1 minute per frame.
Outcome	Refinement of the simulation mesh near the paddle ensures good conformance to its curved surfaces that produce interesting vortex effects in the smoke.
Approach	The blue valves on either side of the bulb prevent backflow.
Approach	The motion of these valves is not scripted.
Approach	Instead, they are modeled as rigid bodies constrained to rotate about an axis and their motion is caused by two-way interaction with the fluid.
Outcome	On the left is a lighter bunny which is tossed about by the force of the cannons and also affects the motion of the smoke.
Outcome	On the right is a heavier bunny that drops quickly to the ground.
Outcome	Although quality of the mesh elements does not suffer at this level of refinement, the proportion of time spent meshing increases to 39.3%.
Outcome	The motion of the smoke at the higher resolution is more lively and exhibits more fine-scale detail.
FutureWork	A vorticity enhancement method, such as those in [Fedkiw et al., 2001] and [Selle et al., 2005] could be used to further enhance the fluid motion but we do not find such enhancement necessary and so have not implemented it.
Outcome	We have presented a system for performing fluid animation using unstructured tetrahedral domains that can change arbitrarily at each time step.
Outcome	Although our current implementation models completely fluid-filled domains, we believe it would be well-suited for use with surface tracking techniques for liquid simulation.

Challenge	This paper presents a method to retarget the motion of a character to another in real-time.
Approach	The technique is based on inverse rate control, which computes the changes in joint angles corresponding to the changes in end-effector position.
Approach	While tracking the multiple end-effector trajectories of the original subject or character, our on-line motion retargetting also minimizes the joint angle differences by exploiting the kinematic redundancies of the animated model.
Outcome	This method can generalize a captured motion for another anthropometry to perform slightly different motion, while preserving the original motion characteristics.
Outcome	Because the above is done in on-line, a real-time performance can be mapped to other characters.
Outcome	Moreover, if the method is used interactively during motion capture session, the feedback of retargetted motion on the screen provides more chances to get satisfactory results.
Outcome	As a by-product, our algorithm can be used to reduce measurement errors in restoring captured motion.
Outcome	The data enhancement improves the accuracy in both joint angles and end-effector positions.
Outcome	Experiments prove that our retargetting algorithm preserves the high frequency details of the original motion quite accurately.
Challenge	The dream of animating complex living creatures with pure computation (such as inverse kinematics, or dynamic control) proved impractical.
Challenge	Even though creatures are not free from physics, their motion is not a direct consequence of physics.
Background	Dynamic control can provide solutions based on simplified assumptions about human motion.
Challenge	However, the result tends to look quite mechanical.
Background	If a high quality character animation has to be produced during a short period of time, motion capture might be a most reasonable choice these days.
Challenge	The captured data itself is for a specific person in performing specific motion.
Challenge	Whenever the data needs to be reused, it has to be retar- getted to account for the differences in the anthropometry and motion.
Challenge	Therefore motion retargetting is emerging as an important technique in recent character animation.
Challenge	If the original motion characteristics are severely lost during motion retargetting, the technique loses its merit over the above pure computation approaches.
Challenge	The problem we try to solve in this paper can be summarized as: ( 1 ) finding in real-time the motion retargetted to a new character that has different anthropometric proportions, and ( 2 ) at the same time, preserving the features of the original motion during the retargetting.
Challenge	( 3 ) As a by-product, it is possible to use the above retargetting algorithm for enhancing motion capture data so that the errors in joint angles and end-effector positions are reduced.
Approach	On-line motion retargetting presented in this paper is based on inverse rate control [17] (or resolved motion rate control), which is a way to implement inverse kinematics based on Jacobian.
Approach	It computes the changes in joint angles corresponding to the changes in end-effector position.
Approach	While tracking the multiple end-effector trajectories of the original subject or character, our on-line motion retargetting imitates the joint motion of the original character by exploiting the kinematic redundancies of the animated model.
Approach	Moreover, jerky motion is prevented since the next configuration is dependent on the previous configuration in inverse rate control.
Outcome	As will be shown in later experiments, the high frequency details of the original motion, which carries important characteristics of the motion, are also well preserved by our algorithm.
Approach	The input is a stream of joint angle vectors   src of the measured subject in the source motion and another stream of the reference (or desired) end-effector positions x 1 of the animated character at discrete time ticks.
Approach	The output is a stream of joint angle vectors   des of the animated character during the destination motion at corresponding time ticks.
Approach	It explains why it is called on-line.
Approach	If the retargetting can be done in on-line, real-time performance can be mapped to another character, or the feedback of the retargetted animation can facilitate motion capture session so that satisfactory results can be obtained with fewer trials.
Approach	Since the memory required for on-line retargetting does not increase with time, our algorithm can handle an infinitely long sequence of motion.
Approach	For example, when there is a bat-swing motion, we can obtain different swing motions aiming at different hit positions by specifying x 1 t appropriately.
Outcome	As a by-product, our OMR algorithm can be used to reduce measurement errors in restoring captured motion.
Approach	For this data enhancement, we use captured position data for x 1 t even though it can be calculated from   src t by forward kinematic positioning, to recover from possible measurement errors in joint angles.
Approach	If the positioning of the pose is done from joint angles alone, the errors can accumulate as the forward kinematic positioning propagates toward the end-effector.
Approach	The end-effector position data x 1 t can be utilized to limit the above error accumulation within a certain range.
Background	Several techniques have been proposed for reusing or altering existing motions.
Background	Witkin et al’s motion warping [19] and Bruderlin et al’s motion displacement mapping [4] discuss motion editing technique based on direct manipulation of data curves.
Background	Bruderlin et al [4] and Unuma et al [11] utilized signal processing techniques for motion editing.
Background	Wiley et al [18] proposed the interpolation synthesis algorithm that chooses and combines most relevant motions from the database to produce animation with a specific positional goals.
Background	Though some of the techniques above can be used for motion retargetting problem with user’s extra efforts, they don’t specifically address the motion retargetting problem.
Background	In [3], Boulic and Thalmann presented the combined direct and inverse kinematic control technique for motion editing.
Background	The concept called coach-trainee metaphor is very similar to the motion retargetting problem formulation.
Background	The fundamental idea is to consider the joint motion of coach as a reference input to trainee motion for the secondary task exploiting the null space of the Jacobian when solving inverse kinematics.
Background	The inverse kinematic constraint is given by half-space such as plane, cylinder, or sphere.
Background	Although their approach shares the technique of utilizing the redundancy in inverse kinematic control with ours, the problem they solved is not the motion retargetting but is rather a motion correction technique since the end-effector constraint specified by half-spaces is not general to solve the motion retargetting problem.
Background	A method which is devoted to the motion retargetting problem was proposed by Gleicher [6].
Background	He used the spacetime constraint method that minimizes an objective function g x subject to the constrains of the form f x = c .
Background	The constraints can represent the ranges of parameters, or various kinds of spatial-temporal relationship among the body segments and the environment.
Background	The objective function is the time integral of the signal displacement between the source and destination motion.
Background	The global method as above can correlate frames back and forth within the whole duration and thus generally produces more smooth results compared to the local method such as our OMR technique.
Background	But the look-ahead property of the global method is effective when the constraints are imposed only at sparse key frames.
Approach	Our OMR takes continuous trajectories of constraints as input, so that it produces globally coherent motion in spite of local computation.
Approach	The global coherence is also achieved from the effort to exploit the redundancy of the system in resembling the original motion.
Approach	The local coherence of the motion comes from the fact that the adjacent frames are inter-related by the inverse rate control.
Outcome	Therefore, without significant degradation of quality, our algorithm provides much faster and interactive way of motion retargetting.
Background	Bindiganavale and Badler [2] presented a method to abstract and edit motion capture data.
Background	Their algorithm detects significant events and abstracts constraints from the motion, and imposes those constraints to other character.
Background	The constraints abstracted from the motion is solved by inverse kinematics at significant frames and then those frames are interpolated.
Background	Although the constraint abstraction is an improvement compared to the other techniques, the interpolation technique might fail to preserve the high frequency details if the key frames are sparsely spaced.
Approach	m can be 12 or 18 if we want to impose multiple end-effector constraints.
Approach	J 1 is called Jacobian and is an m n matrix that linearly relates the end effector velocity and joint angle velocity at the moment.
Approach	Given the end effector velocity, we can get joint angle velocity by inverting the Jacobian.
Approach	However, most articulated figures have kinematic redundancy and thus the inverse of Jacobian is not unique.
Approach	Some criteria can be specified to pick one that best fits for our purpose.
Approach	One of popular criteria is called the minimal norm solution         ( 4 )        where J 1 + = J 1 T J 1 J 1 T , 1 is the pseudo inverse 1 of J 1 .
Approach	Another way to utilize the redundancy of the system is to set y to the gradient , r g of a criterion function g   in Equation 5.
Approach	Then integration of Equation 5 tries to reduce the value of g   while the end-effector is made to track the given trajectory [9].
Background	Balestrino et al [1], Tsai and Orin [15], and Sciavicco and Siciliano [14, 13] proposed the closed-loop inverse kinematics (CLIK) scheme based on Jacobian pseudoinverse.
Approach	CLIK leads to zero steady state error which means that the error is exponentially convergent to zero for a fixed target position.
Approach	It can be easily shown that as the smallest eigen value of K 1 becomes large, the convergence rate increases accordingly since the error dynamics is governed by the relation e 1 + K 1 e 1 =0 .
Approach	In a continuous time formulation such as Equation 7, a large value of K 1 is desirable.
Approach	However, as will be shown below an arbitrarily large K 1 doesn’t guarantee convergence in implementing the discrete version of Equation 7.
Approach	Therefore, e 1 n should be estimated.
Approach	Below we show that any estimation based on the old values (at n , 1 , n , 2 , : : : ) requires K 1 to be I for the best tracking performance.
Approach	Suppose that we estimated e 1 n simply with e 1 n , 1 .
Approach	) But in practice, we found that instability rarely occurs at a usual sampling rate ( 30 60 Hz) in dealing with human motion.
Approach	x des 2 is the actual result of the secondary task that tries to realize the given goal x 2 .
Approach	It is proven that e 2 is ultimately bounded within a certain range and the tracking error for the primary task is not affected by the second term of Equation 15 [13].
Approach	But again, arbitrarily large K 2 is not allowed in discrete implementation.
Approach	We found that the estimation rule described below gives satisfactory results.
Background	The serial chain is not suitable for modeling creatures since underlying articulated structures contain branches.
Approach	An illustrative example is taken from human upper body, and is shown in Figure 2 .
Approach	The model consists of spine and two arms.
Approach	The waist is the root of the kinematic tree structure, and the two arms are branching at the top of the spine.
Approach	If both hands have their own goals to reach, and if inverse kinematics is solved for these cases separately, then the spine angles will differ in the solutions.
Background	In [22], Zhao and Balder solved this problem by a weighted sum of independently obtained gradients, each of which directs its corresponding end-effector to a goal position.
Background	However, the effects of different weights are not easily predictable.
Background	Depending on the weight assignment, their algorithm can fail to find an inverse kinematic solution even if all the constraints can be actually met.
Approach	Intrinsically, the problem of finding inverse kinematic solution of multiple constraints doesn’t require any weight or priority assignment: if all the end-effector constraints can be met, then it should be possible without considering weights or assigning priorities to each end-effector constraint.
Approach	Compared with Zhao and Badler’s algorithm, Jacobian based inverse rate control gives a quite simple and intuitive solution to the problem.
Approach	The only thing we have to do in order to incorporate multiple end-effector constraints is concatenating the end-effector vectors and composing the Jacobian appropriately.
Approach	Of course, the Jacobian will have many zeroes where the joint angle and the end-effector have no relation such as left elbow joint and right hand.
Approach	In inverse rate control, the above conflict of the spine angles is resolved during the computation of the pseudo inverse of the Jacobian.
Approach	In general, we can formulate the motion retargetting problem with the following task set, and can solve for   des .
Approach	Since joint angle trajectories contain important characteristics of a motion, and since the end-effector movements are already tracked by the primary task, an obvious and useful choice for the secondary task might be to imitate the joint motion of the source character.
Approach	Reasonable choices for K 1 and K 2 are I ’s in discrete implementation as stated before.
Approach	But K 1 , K 2 can be adjusted based on the dexterity measure to get consistent motion near the kinematic singularities.
Approach	A popular dexterity measure is min = max , where min and max are the minimum and maximum, respectively, among the singular values of the Jacobian.
Approach	In this case, smaller K 1 and K 2 should be used if the dexterity measure turns out to be small.
Approach	The adaptive scheme can be also useful if we apply the OMR algorithm to motion transition.
Approach	Smaller gain (e.g. K 1 = K 2 = 0 : 1 I ) will produce sluggish tracking, but produces smooth motion.
Approach	Therefore, if the animated model switches to another motion and there exists a large discrepancy at the motion boundary, smooth transition can be obtained by adjusting the gain matrix K 1 and K 2 appropriately.
Approach	Therefore another provision for enforcing stability might be to clamp the value that goes into the box of J 1 + in Figure 4 whenever it is over a certain threshold.
Approach	The provision might be effective when there is an excessively large acceleration, or when the model is fully stretched and almost no manipulative redundancy is left in the system.
Approach	(In dealing with the human motion, however, the above provision was almost never needed.
Approach	When we capture a motion, we often measure the joint angles and use forward kinematics to reconstruct the motion.
Challenge	But the method can introduce large end-effector position errors since the joint angle error near the base is amplified when it comes to the end-effector, and joint angle errors are accumulated as the forward kinematic positioning propagates toward the end-effector.
Background	Choi et al’s interpolation/regression method [5], applies inverse kinematics at sparse keyframes and the resulting joint angles are interpolated with cubic spline curves.
Background	The interpolation is combined with least square fitting so that the characteristics of the original joint angle data is preserved in the resulting motion.
Approach	The OMR algorithm described in the previous section can be used to reduce measurement errors in restoring the captured motion.
Outcome	The new method is an improvement over the above interpolation/regression method in three aspects: ( 1 ) inverse kinematics is done at every frame, which promises much closer end-effector tracking, ( 2 ) the joint angle imitation is done by exploiting redundant degrees of freedom rather than depending on the least square fit, and ( 3 ) the high frequency component of the original motion is preserved much better in the new method.
Approach	For the enhancement, we measure both joint angle and end-effector trajectories during the motion capture session.
Approach	The measured trajectories are supplied to our motion retargetting algorithm: the end-effector trajectories are supplied for x 1 , and the joint angle trajectories are supplied for   src .
Approach	Of course, the destination character has to be same with the source character, if pure data enhancement needs to be done.
Outcome	In general, our algorithm also reduces the errors in joint angle measurements.
Outcome	While the joint angle errors can accumulate in forward kinematic reconstruction, once it is processed by our OMR, the total amount of accumulated error is limited by the amount of end-effector position error.
Outcome	Moreover, the joint angle error due to the end-effector position error is distributed among all the joints.
Outcome	Therefore unless the amount of end-effector position error is excessively larger than that of joint angle errors, our OMR produces more accurate result than the unprocessed data.
Approach	Note that the above does not mean the retargetting and data enhancement should be done separately.
Approach	If a different destination character is used, the two things are actually achieved at the same time.
Outcome	This is especially useful when a real-time performance is retargetted.
Approach	In the first experiment, we show a retargetting example in which our OMR is applied to retarget a walking motion, to demonstrate that our OMR based on inverse rate control is not inferior in the quality to the retargetting based on spacetime constraints.
Approach	Major error analysis of the algorithm is given in this example.
Approach	In the second experiment, we show the retargetting of bat-swing motion.
Approach	In this experiment, the source motion (refer to the video clip #1) is a curved path walking motion which was procedurally generated by Ko’s locomotion algorithm [8].
Approach	The walker took 13 steps and produced a total of 390 frames.
Approach	The kinematic structure of the characters used for walking motion is shown in Figure 5 .
Approach	Since the lower body motion is far more important than the upper body motion in walking example, we retargetted only the lower body motion.
Approach	As shown in Figure 5 the lower body consists of pelvis, upper leg, lower leg, foot, and toes, and they are connected at the hip, knee, ankle, and ball joints.
Approach	The total degree of freedom of the lower body is 8 3 + 6 = 30 .
Approach	(All the joints were modeled by 3-DOF joints, and the base has extra 6 DOFs.
Approach	) The destination character was about 60% scaled down from the source character with non-uniform proportions.
Approach	In the retargetting, the secondary task was set to   src =   des .
Approach	To specify the primary task, we set the toe-tip of the stance leg as the base and the toe-tip of the swing leg as the end-effector.
Approach	The source character’s toe-tip trajectory was used for x 1 without any modification.
Approach	Therefore the destination character had to take relatively bigger steps considering his body size.
Approach	At the boundaries of steps the base and end-effector were switched.
Approach	It implies that there can be discontinuities at the boundary if the tracking error is large.
Outcome	The retargetted motion with the above task set is shown in the video clip #2.
Outcome	The tracking error of the swing foot was negligible and thus the produced motion was smooth at the step boundaries.
Outcome	But the pelvis motion showed non-uniform speed along the direction of progression (anterior-posterior), which wasn’t observable in the source motion.
Approach	So we constrained the transverse plane motion of the pelvis.
Approach	i.e. the pelvis was designated as another end-effector, and the x; z component of the source character’s pelvis movement was tracked in the destination motion.
Approach	(Note that the pelvis motion along y -axis should be adapted to account for the height difference).
Outcome	After adding the constraint, we could obtain a satisfactory result as shown in the video clips #3 and #4.
Outcome	Even with the extra constraints, the end-effector trajectories of the source and the destination made an accurate match.
Outcome	The comparison is shown in Figure 7 .
Outcome	The dotted curves for the source motion are not visible because they overlap exactly with the solid curves, the end-effector trajectories of the destination motion.
Approach	To show the tracking error microscopically, the area indicated with a small box near the 150th frame in Figure 7 was magnified in Figure 8 .
Outcome	The trajectories in the figure show that the tracking error is kept small where the velocity is nearly constant, but the error increases when the velocity makes sudden changes.
Outcome	The maximum error (1.0464 cm) occurred at the 128th frame where the y -coordinate (height) of the toe-tip reached its peak acceleration and this error was reduced to a negligible level at around the 135th frame as the acceleration decreased.
Approach	The step boundary was taken from low-acceleration points so that the base to end-effector switch makes a smooth transition.
Outcome	The joint angle trajectories of the left leg during the original and retargetted motion are plotted in Figure 10 .
Outcome	Only the angles around the sideways direction (medial-lateral) axes are presented in the graphs.
Outcome	The comparison shows that the amplitude of the hip angle is increased in the destination motion to cover the given step length with the relatively smaller body.
Outcome	Other than that the original joint angle pattern was quite well preserved.
Outcome	3 At the end of every step, the ball joint of the source character showed an abrupt change from a large negative value to zero.
Outcome	It corresponds to the toe-off moment when the toes take off the ground.
Outcome	After the retargetting, the sharp corner of the trajectory was well preserved.
Outcome	In general, our OMR preserves the high frequency content of the motion quite well, since inverse rate control is directed by Jacobian values.
Outcome	Big mountains or valleys are never missed.
Outcome	To recover tiny fluctuations as well, however, a high sampling rate is needed to avoid aliasing.
Outcome	If the sharp corners are undesirable, they can be prevented by adjusting the gain matrix K 2 or clamping some of the control input as stated in Section 4.
Outcome	The adjustment of K 2 does not affect the end-effector tracking performance.
Approach	In this experiment, actual performance of a bat swing motion was processed by our OMR to produce the destination motion of three different characters shown in Figure 9 .
Approach	The anthropometry of Character B is about the average.
Approach	Character A has a longer torso but shorter limbs than average, and Character C has a shorter torso but longer limbs.
Approach	Their kinematic structures are same as Figure 5 except that the torso is segmented to 5 parts and the feet are excluded.
Approach	To set the primary task, the base and end-effectors should be specified as before.
Approach	In this experiment, the pelvis was chosen as the base and two hands were chosen as the endeffectors.
Approach	Three 6-DOF sensors were used to capture those positions and orientations.
Approach	The end-effector motion was directly supplied for x 1 t without any modification.
Approach	Since all the torso segments can not be measured due to the limited number of sensors, we measured only the topmost segment.
Approach	Therefore, the five joints from the waist to the top-most torso segment had to be generated from the orientation difference between the pelvis and the top-most torso segment.
Approach	For this, the measured orientation of the topmost torso segment was added to the primary task, and zero angles for the unmeasured joints were added to the secondary task, expecting the primary task can be met with minimal joint angles along the torso.
Approach	The other sensors were used to measure the joint angles.
Approach	(Because we had only 13 sensors available, we had to give up capturing the foot motion.
Approach	) In this experiment only the upper body motion was adapted by OMR.
Approach	The lower body motion was reconstructed by applying the measured joint angles directly.
Outcome	The retargetting of the source motion to Characters A, B, C are shown in the video clips #5 ̃6, #7 ̃8, and #9 ̃10, respectively.
Outcome	The small green boxes in the video indicate the position of the end-effectors and base.
Outcome	In the video we can observe that end-effector positions are accurately tracked.
Outcome	Since the body dimensions of Character B and the real performer are similar, the retargetted motion does not contain any noticeable difference from the source motion.
Outcome	In the case of Character A, however, we can see the waist is bent to lower the hit position, and the torso is shifted forward to account for the shorter arms.
Outcome	In the case of Character C, the torso is bent backward and makes a bigger twist to account for the longer arms and shorter torso.
Outcome	Snap shots were taken during the retargetted motions to clearly demonstrate the above adaptation for the anthropometric differences and shown in Figure 11 .
Approach	Since we had no privilege to install our program to the platform equipped with a motion capture system, we had to emulate the real-time motion capture.
Approach	That is, the motion data captured at 30Hz was fed to the OMR system with the same sampling rate using a timer.
Outcome	At this sampling rate, not a single frame was lost even with the visualization included.
Approach	We used an Intergraph GX1 system (dual P-III 550, wildcat 4000) for the experiments.
Outcome	The code was not fully optimized and so the performance can be potentially improved further.
Outcome	The slower rate of the bat swing motion is due to the bigger size of the Jacobian matrix compared to the walking motion (8 30 vs. 9 42).
Outcome	As shown in the table, the OMR is fast enough to process motion capture data collected at a usual sampling rate (30 90Hz) in real-time for the models of reasonable complexity.
Outcome	This paper presented the on-line motion retargetting technique based on inverse rate control.
Outcome	The method is an improvement over the off-line retargetting based on spacetime constraints since real-time performances can be retargetted without degradation of retargetting quality.
Outcome	The OMR technique greatly helps to get more satisfactory results in motion capturing with fewer trials by giving the real-time feedback to the performer.
Outcome	Furthermore, the captured data are enhanced in both end-effector positions and joint angles by going through our OMR filter.
Outcome	One minor unsolved problem is that there is no easy way to guarantee full-proof stability of the system due to the non-linearity.
Outcome	We observed that at a very low sampling rate, and if the model goes near the kinematic singularity and thus very little manipulative redundancy is left, then the system can became unstable.
Outcome	However, experiments proved that the system never become unstable at 30Hz or higher sampling rate.
Outcome	If the source motion is available only at a low sampling rate, two remedies are recommended: ( 1 ) by interpolating the source motion curves, first produce more samples, and then use them as the input to the OMR filter, or ( 2 ) scale down the end-effector trajectory to avoid the singular configuration, or use both of ( 1 ) and ( 2 ).
Outcome	The above remedies are for an excessively bad situation.
Outcome	Our on-line motion retargetting produces satisfactory results in retargetting most human or creature motion.
Outcome	If the technique is well utilized, it can be very useful to people in character animation and game industries.

Outcome	This paper presents a system for rapid editing of highly dynamic motion capture data.
Outcome	At the heart of this system is an optimization algorithm that can transform the captured motion so that it satisfies high-level user constraints while enforcing that the linear and angular momentum of the motion remain physically plausible.
Outcome	Unlike most previous approaches to motion editing, our algorithm does not require pose specification or model reduction, and the user only need specify high-level changes to the input motion.
Approach	To preserve the dynamic behavior of the input motion, we introduce a spline-based parameterization that matches the linear and angular momentum patterns of the motion capture data.
Approach	Because our algorithm enables rapid convergence by presenting a good initial state of the optimization, the user can efficiently generate a large number of realistic motions from a single input motion.
Approach	The algorithm can then populate the dynamic space of motions by simple interpolation, effectively parameterizing the space of realistic motions.
Outcome	We show how this framework can be used to produce an effective interface for rapid creation of dynamic animations, as well as to drive the dynamic motion of a character in real-time.
Challenge	Despite great advances in recent years, creating effective tools for synthesis of realistic human motion remains an open problem in computer animation.
Challenge	This is particularly true for synthesis of highly dynamic character motion such as running, leaping, jumping and other athletic and acrobatic maneuvers that frequently occur in feature special effects and video games.
Challenge	Synthesizing such motions can be challenging because any physical inaccuracies in these motions are particularly noticeable.
Background	Both spacetime optimization and controller synthesis approaches have been proposed for direct synthesis of dynamic character motion.
Background	Although these methods do satisfy physical laws, they tend to appear overly smooth and at times robotic.
Challenge	Furthermore, these methods do not provide interactive control, often requiring considerable offline processing time before the animation sequence is generated.
Challenge	In addition, it is difficult to achieve a graceful degradation of realism for the purpose of greater control.
Background	In contrast to direct synthesis, methods based on adaptation of motion capture data produce highly realistic motion, especially in the neighborhood of captured motion samples.
Background	They also run at interactive speeds, as they employ data interpolation techniques.
Background	Unfortunately, these methods require a large number of motion samples.
Background	If the animator wants to interactively control a specific parameter of the animation such as the landing foot position in a particular acrobatic stunt, the need for a large dataset is particularly pronounced: the interpolation techniques would require an already existing family of motion sequences where the only difference in motion is the landing foot position.
Challenge	Gathering such a datataset is not only laborious, but it also requires that the captured family of motions is similar in all other respects (e.g. other landing points, initial and final state, overall style) — an aspect that is quite hard to reproduce by real actors.
Challenge	In fact, the process of generating such parameterized motions is the most challenging aspect of data acquisition for video game production [Buc].
Challenge	In addition, the animators often wish to create non-realistic motions that defy the laws of physics, a space where motion capture simply fails to provide any samples.
Approach	We take the approach to acquiring similar motions is to adapt a single motion sequence several times to synthesize a family of motions that preserve physics constraints.
Approach	Motions created in this manner can satisfy an animator’s exact specifications with a minimum of deviation from the initial motion sequence.
Challenge	Ideally, we would like to use a minimal source of motion data, perhaps a single captured movement, to create a wide range of additional motions.
Background	Recently a number of dynamic motion adaptation methods have been proposed [PW99, ZH99, TSK02, SP04, SHP04], and the work presented in this paper falls into this category.
Challenge	In this paper, we describe the momentum-based motion editing technique.
Outcome	In contrast to the existing methods, our proposed framework is particularly robust to large-scale motion modifications.
Outcome	For example, we can adapt a forward leaping movement, to a collection of leaping movement in different directions including a backward leap, or a 360 ◦ leaping spin.
Outcome	Using our motion editing framework, we show how a family of dynamic movements can be synthesized based on the animator’s needs for interactive control.
Approach	Because our family of motions samples the space widely, satisfies exact constraints, and otherwise deviates minimally from the original source sequence, we can use simple interpolation techniques to allow real-time exploration of this synthetic motion space.
Outcome	We describe a number of real-time animation tools that can be constructed using these synthetic motion families, such as interactive displacement of constraints (e.g. varying foot landing position), as well as inverse control examples such as the determination of the natural volleyball spike that would hit the ball arriving at a specific position in space.
Outcome	In addition, we describe how the same synthetic sampling/interpolation approach can be used to develop realtime controllers for leaping character motion, all synthesized from a single motion-captured leap.
Background	Recent research in computer animation focused on techniques for remapping existing data to given specifications of a new scenario.
Approach	In this paper, we build on the research in both physicsand interpolation-based motion editing methods.
Background	Optimal trajectory methods introduced by Witkin and Kass [WK88] provide a powerful framework for enforcing dynamic constraints while searching for the most favorable motion judged by the objective function.
Background	The dependency on the initial point has been somewhat alleviated by starting out with the captured motion sequence.
Background	Popović and Witkin in 1999 developed a first method that transforms motion capture data while preserving physical properties [PW99].
Background	They found solutions by performing optimizations on the reduced character model.
Background	More recently, editing motion capture data based on spacetime optimization has become a popular strategy for producing realistic character animations [RGBC96, SP04, SHP04].
Background	These methods provide control for modifying data while retaining physically plausible properties of captured motion by restricting the optimization space with additional kinematic constraints (e.g. [RGBC96]), or by solving within the PCA-reduced space of motions [SHP04].
Background	It has recently been shown that relying on simplifications of dynamic constraints is not necessary if proper scaling and estimation of joint angles, torques, and Lagrange multipliers are provided [SP04].
Approach	Our work uses a similar spacetime optimization framework.
Approach	In contrast to other approaches, we formulate significantly simpler momentum constraints on a complex character model, without solving for muscle forces explicitly, similar to [LP02].
Approach	Since we do not compute internal torques for joints, scaling and convergence issues are less critical in our optimization framework.
Approach	Our physics-based motion editing approach is based on the momentum constraints introduced by Liu and Popović [LP02].
Background	In that work, momentum constraints were used for synthesis of highly dynamic motion from simple animations that did not contain sufficient information to synthesize the full motion.
Background	As a result, transition poses had to be introduced to further restrict the optimization space.
Background	There are two main advantages of momentum constraints over the full dynamics constraints.
Approach	First, since dynamic constraints are reduced to only global momentum patterns, we are solving for a much smaller set of unknowns, and over a much “better behaved” set of constraints.
Approach	This allows us to find solutions quickly.
Approach	Also, in our experience, these constraints do not suffer from many local minima, thus enabling us to find solutions significantly further away from the original motion.
Background	The second advantage of momentum constraints is that they encode more about the natural motion than just physical correctness.
Background	For example in natural motion, passive elements such as tendons and ligaments store and release energy during ballistic motion.
Background	To model this with a full dynamic system, one would have to include a complex muscle model.
Background	Momentum constraints effectively record the aggregate effect of the natural torque usage and energy storage/release in a specific momentum pattern.
Background	This additional information embedded within the momentum constraints ensures that adapted motion is not just physically correct, but that it also constrains the motion within the momentum exchange patterns observed in nature.
Approach	In contrast to the original paper that introduced momentum constraints, our method applies momentum constraints directly on the motion capture data.
Approach	Our algorithm does not require any additional pose constraints at the transition points between flight and ground phases.
Approach	Furthermore, we introduce a novel spline-based representation for the momentum patterns that can be used to intrinsically enforce the similarity between the resultant motion and the input motion.
Background	Instead of formulating a physics-based optimization, dynamic filtering is an efficient alternative for motion editing of smaller amplitude.
Background	Per-frame based frameworks largely reduce the computation time, providing an interactive editing interface to the user [TSK02, SKG03].
Background	Unfortunately, the per-frame approach means that animators can modify the spatial position of constraints, but not their position in time.
Background	Tak et al. applied Kalman filter to estimate an optimal pose for the current frame subject to the given constraints.
Background	The result of the estimation is then rectified by least-square-fit to ensure a physically sound motion [TSK02].
Background	Shin et al. approximated the adjustment made to the original motion capture data by correcting the momentum of the character during flight and using the balance constraints on the ground [SKG03].
Approach	In general, these methods are geared toward the local modification compared to the overall motion, such as improving the balance, whereas our approach is able to handle global changes of the motion such as transforming a forward jump to a 360 ◦ backward spin jump.
Background	Another branch of dynamic filtering employs dynamic tracking [ZH99, PR01].
Background	These methods combine motion capture data and dynamic simulation to retain human-like details from the data while presenting interaction with the environment.
Background	These methods produce motions that do not deviate significantly from the input motion, relying on the existence of captured motion that is similar to what the user intends to do.
Background	Straightforward interpolation of joint angles usually fails to preserve physical realism from the original data.
Background	However, many methods have shown that small modification of the motion can be easily done by linear interpolation of joint angles [BW95, WP95, WH97].
Background	Combining interpolation with kinematics constraints, Gleicher adapted original motion to a new character while maintaining environmental constraints such as foot contacts on the floor [Gle98].
Background	A more sophisticated interpolation was presented using radial basis functions to blend motion sequences with various inverse-kinematic goals [RSC01] or different style [RCB98].
Background	Unfortunately, data acquisition and post-processing for these methods present a significant challenge since motion sequences need to be carefully crafted so that they contain the same content yet different in style.
Approach	Our approach only requires one single motion capture sequence as the seed.
Approach	This seed is used to generate a family of motion sequences that parameterize the dynamic space.
Background	Lee and Shin presented a multi-level B-spline representation by which they transform existing motion to satisfy desired constraints adaptively through direct manipulation [LS99].
Background	Using B-spline representation, the motion edits can be limited to user-specified frequency bands, providing a more effective optimization framework.
Approach	Our work adapts the idea of using spline-based representation to constrain the search of the optimization.
Approach	We model the momentum curves by a B-spline representation which are fitted to the original motion so that the search space in the optimization is limited to solutions that have similar dynamic behavior of the original motion.
Approach	Our system is based on an optimization algorithm that can transform the captured motion to satisfy high-level user constraints while preserving physical realism.
Approach	As input, the system takes a single motion capture sequence and the userspecified modification.
Approach	We describe the algorithm in three separate components: Motion pre-fitting, optimization, and interpolation (see Figure 1 ).
Approach	The pre-fitting optimizes a set of coefficients used to model momentum curves so that they are constrained to the similar shapes of the original motion.
Approach	The system then formulates a spacetime optimization that solves for a new motion, where both high-level physical constraints and the user specification are met.
Approach	With a family of such optimized motions that parameterize certain dynamic space, we can apply a simple linear interpolation to generate arbitrary new motion within the dynamic space in real-time.
Approach	Our algorithm adapts the momentum-based constraints [LP02] for the task of motion editing.
Approach	Instead of filling in missing data, motion editing must solve the converse problem of preserving the original data while still satisfying animator-imposed constraints.
Approach	There is no need for keyframing of any kind because the motion already starts in a good initial state.
Approach	Any underlying physical model employed by the system must be flexible enough to precisely describe the initial state of the motion and, at the same time, rigid enough to maintain a semblance of the original motion throughout the editing process.
Approach	At the heart of our algorithm is a set of full-body angular and linear momentum curves.
Approach	These curves constrain the edited motion to the realm of physical realism without the need to simulate expensive dynamical properties such as joint torques and contact forces.
Approach	The momentum curves are parameterized by a set of coefficients that are pre-solved to closely match the input motion.
Approach	The advantage of this approach is twofold.
Approach	First, a good initial state of the momentum coefficients results in rapid convergence of the optimization.
Approach	Second, the coefficients that control the shape of the curves can be fixed throughout the editing process, effectively performing a biased search for similar motions in the momentum space.
Approach	After the motion is captured using an optical system and processed to fit the character’s skeletal structure, we employ the constraint detection technique described in [LP02] to partition the motion into ground-contact and flight stages.
Approach	Since the the animator may at times wish to produce physically impossible jumps that are not constrained to the earth’s gravity, and because the sampling rate varies for each input motion sequence, we also need to determine the time interval between two animation frames.
Approach	Gravity and time step are directly related because we can equivalently choose to find the right gravitational constant that makes the motion realistic for a given unit time step.
Approach	During free-fall stages, the linear momentum is only affected by gravity and the angular momentum remains constant.
Approach	When the body is in contact with external forces, the momentum curves can no longer be represented by a simple set of linear equations.
Approach	Instead, we represent the momentum curves with a 3rd-order non-uniform B-splines for their flexibility and convenient knot based parameterization.
Approach	In our spline representation, the first and last knots have duplicity 4 to ensure interpolation of the end points (see [FvDFH92]).
Approach	A defining characteristic of motion is the shape and magnitude of its momentum curve (see Figure 2 ).
Approach	In the case of our spline representation, the control points determine the magnitude of the curve and the spacing of the knots influence the shape.
Approach	We note that this formulation can capture a greater variability of momentum patterns than the previously used hardwired patterns [LP02].
Approach	This is especially important when dealing with motion capture data due to wide range of different maneuvers possible in the real world.
Approach	In other words, we perform a least-squares regression over the momentum curve in the ground stage, while maintaining C 1 continuity through the transitions to the flight stages.
Approach	There are few exceptions to the problem described above.
Approach	When there is no adjacent flight stage, we remove the constraint corresponding to v i from the statement of the problem.
Approach	Also, the constraint corresponding to v 0 is entirely removed when pre-fitting the vertical linear momentum curve since the transition from a free-fall stage to a ground stage is typically dominated by impulsive forces, which are not C 1 continuous in the vertical momentum component.
Approach	As in [LP02] we model motion as an optimal dynamic process with a set of realistic constraints.
Approach	In general terms, our condition for optimality is that the output motion be both as smooth, and as similar, to the original motion as possible.
Approach	Constraints on the solution ensure that the character’s limb do not bend unnaturally, that the character’s feet do not pass through the ground, and that the character’s full-body momentum curve follows the path of the pre-fit momentum splines.
Approach	The degrees of freedom to be optimized are contained in Q G, where Q is the set of joint angles through time describing the motion and G is the set of the control points controlling the momentum splines.
Approach	In the initial state of the optimization, Q is a good initial guess at the target motion formed by linearly interpolating the original motion between user specified translations and orientations, and G contains the pre-fit momentum coefficients.
Approach	In addition to the constraints and objectives used in [LP02], we also introduce a similarity objective and a pseudo balance objective as described in the following sections.
Approach	The similarity objective is intended to keep the optimized motion as similar to the original as possible.
Approach	We formulate this objective as the squared distance between the original vector of DOFs, Q init , and the solution vector, Q. Each joint DOF is scaled by its natural bound.
Approach	The energy function we wish to minimize is then, E s (Q) = (Q init − Q) 2
Approach	Since we do not model the specific human preference to stay out of extreme leaning movements that in real life can often cause foot slipping on the ground, there are some instances when the resulting motion would leave the character unnaturally leaning without a means of support.
Approach	To pull the optimized solution away from these unstable regions, we include a pseudo balance objective.
Approach	The objective we use attempts to minimize the squared distance between the COM, C(t) of model in the first time-step, t 0 , and last timestep, t f , of the initial and final ground stages of the motion.
Approach	For interior ground stages, we instead minimize the distance between the COM of the model in the middle frame of the stage, C(t m ), and the COM of the linearly interpolated input motion, C orig (t m ), in the same frame.
Approach	To summarize, the unknowns of our system, Q and G, are the character DOFs and the control points for the momentum splines.
Approach	Note that spline knots are omitted to maintain the similar momentum pattern of the original motion.
Approach	The optimization enforces two types of constraints: environment constraints, K e , such as feet positions on the ground, and momentum constraints, K m .
Approach	The following spacetime formulation finds the unknowns Q and G that minimize the objective function while satisfying all the constraints: min Q,G E s (Q) + E b (Q) subject to K K e m (Q) (Q, G) = = 0 0
Approach	Our system provides several high level motion specification tools so that the animator never has to think of editing in terms of constrained optimization.
Approach	First, motions are automatically partitioned into alternating flight and ground stages.
Approach	Alternatively, the user can manually adjust the partitioning to make corrections.
Approach	Next, the user manipulates ground stages with the mouse to translate their position and turns a dial to change the orientations as desired.
Approach	The system treats these specifications as offsets from the original state of a ground stage.
Approach	In other words, given the original translation, q T , and original orientation, θ, of the ground stage, the user specifies offsets ∆q T and ∆θ.
Approach	The new translation and rotation of the ground stage is then altered to be q T + ∆q T and θ + ∆θ, respectively.
Approach	To form a good initial guess at the solution for the frames of the flight stages, the system linearly interpolates the offsets of the adjacent ground stages over each time step of the flight stage.
Outcome	The resulting motion is a crude approximation of the final result, but provides a good initial state for the spacetime optimization.
Approach	The animator can also change the height of the trajectory in a flight stage by interactively shaping a visualization of the trajectory.
Approach	This is particularly useful when creating non-realistic motion that defies gravity, as will be explained below.
Approach	Once the user is satisfied with the edits, the optimization process takes between 1 to 5 minutes per motion.
Approach	Alternatively, several motions can be generated together in a batch mode.
Approach	The technique constructs an output motion in real-time by performing a simple weighted average over the DOFs values from a set of sample motions.
Approach	A family of motions can be populated from the input motion by systematically varying the position and orientation of one or more ground stages and then performing a sequence of similar optimization.
Approach	We provide a user interface for the three most useful types of motion families(see Figure 3 ).
Approach	The first type varies the translation of a ground stage along a line, the second type varies the translation of the ground stage along a 2 dimensional grid, and the third type varies both the translation and orientation of the ground stage along a semi-circle such that the orientation of the character is consistently aligned along the normal vector of the arc.
Approach	The size of the sample space as well as the density at which it is sampled can both be adjusted as necessary.
Approach	Other types of motion families can be easily added.
Approach	Once a motion family is populated, we are able to generate arbitrary intermediary motions by blending the nearest 2 n samples, where n is the number of dimensions in the parameterized space.
Approach	We chose to use a simple linear blending method for several reasons.
Approach	First and foremost, the algorithm is very fast and well suited to any application where the output motion must be generated “on the fly”.
Approach	Since motion families are produced offline, they can be as densely populated as necessary to increase the accuracy of the interpolation.
Approach	Second, since the members of a motion family are produced by the same optimization setup, varying only in specific dimensions (e.g. landing positions, height, orientation, etc), it is often the case that they blend very well and need not be sampled very densely at all.
Outcome	In our results section, 9 samples is the most we ever required to adequately sample the dynamic space of a motion.
Approach	Although foot glide is among the most troublesome artifacts for most motion blending techniques, we find that it is imperceptible for both the line and grid motion families.
Approach	However, when the global orientation and the translation of the motion are interpolated simultaneously, as is the case in the circle motion family, a very miniscule amount of foot glide becomes perceptible.
Approach	A simple fix is to apply a per-frame inverse kinematic (IK) solver to slightly adjust the lower body to satisfy the positional constraints on each foot.
Approach	Solving IK on the lower body not only has the effect of planting the foot firmly on the ground without changing the overall looks of the motion, but is also light-weight enough to converge in real-time, as the motion is being displayed.
Background	In many applications the most important aspect to control is the position and time at which the character makes contact with an object in the environment.
Approach	Consider the example of a soccer header motion, where it is required that the character’s head always makes contact with the soccer ball at the correct moment in time.
Approach	Starting from a single input motion we can generate an arbitrary header by creating a grid motion family that varies the translation of the landing stage.
Approach	The joint configuration at each time-step in the output motion is then defined as a vector function q(x, y,t) of the landing position, (x, y), and the time-step, t.
Approach	If we denote the position of the character’s head by the function h(q), the problem of finding the motion that constrains the characters head to ball position p c at time t c , is reduced to that of finding values (x, y) such that p c = h(q(x, y,t c )).
Approach	This is, in turn, analogous to minimizing the energy function E(x, y) = (p c − h(q(x, y,t c ))) 2 , which can be solved efficiently by a simple gradient descent method.
Approach	The gradients are computed using finite differences.
Approach	One caveat is that q is actually a piecewise function that performs a bi-linear interpolation of the 4 nearest sample motions.
Approach	When one sample motion is replaced by another in the set of 4, q ceases to be C 1 continuous, causing convergence problems with the gradient descent method.
Approach	A simple solution is to replace the linear blending functions f (x) = x and g(x) = (x − 1) with smooth in/out functions such as f (x) = sin 2 (x) and g(x) = cos 2 (x), thereby maintaining C 1 continuity through the transitions.
Outcome	One advantage of our motion generation algorithm is that it provides for a wide range of physically plausible animations in real-time.
Approach	To demonstrate the full benefit of this approach, we have created a video game interface where the user controls the trajectory of a jumping character with a multi-directional control pad (see Figure 6 ).
Approach	We start with a motion capture sequence of a character making two consecutive jumps.
Approach	The interesting aspect of this motion is that the character must exhibit foresight in the motion of the first jump, so that the correct contact forces can be generate in the intermediate ground stage, to create the necessary momentum for the second jump.
Approach	Our approach inherits the same key benefit from spacetime, but allow us generate motions in realtime.
Approach	In this demonstration we wish to control the horizontal translation vectors of the first and second jumps, d 1 and d 2 , respectively.
Approach	First we generate a motion family by varying both the first and last ground stages along a 3x3 grid.
Approach	The entire motion family then consists of 81 optimal motions resulting from permuting the 9 possible starting positions with 9 possible ending positions.
Approach	This is necessary in order to sample the entire range of possible ground stage transitions between the two jumps.
Approach	We are then able to populate the space between sampled motions by linearly interpolating the nearest neighbor optimal solutions.
Approach	In this case, we have 4 dimensions in our sample space corresponding to the values of d 1 and d 2 , making for a total of 2 4 (or 16) nearest neighbor motions.
Approach	Therefore, we can express the output motion as vector function q(d 1 , d 2 ), whenever d 1 and d 2 are within the bounds of the sample space.
Approach	To make our demonstration even more interesting, we chain our jumping motion end to end, such that it continuously loops upon itself.
Approach	This is done by blending the second flight stage of the first motion, q a (d a1 , d a2 ), into the first flight stage of the second motion,q b (d b1 , d b2 ).
Approach	In order to make the blending work, we simply require that d a2 = d b1 .
Approach	In order words, we require the length and direction of the blended jumps be the same.
Outcome	The end result is an interactive jumping simulation where the user controls the direction that the character jumps and then sees the motion carried out in a physically plausible manner.
Approach	Due to the foresight discussed earlier, the character must always have prior knowledge of the next two directions it will jump.
Outcome	This causes some lag time between when the user specifies a direction and when that motion will occur, but this is only natural given the deterministic nature of the ballistic motion.
Approach	The motion sequences in our demonstration were captured at 120 frames per second using an optical motion capture system.
Approach	The character is composed of 18 rigid links and 43 degrees of freedom.
Approach	S0(3) rotations are expressed in exponential map representation.
Approach	The mass distribution of the model is an appropriately scaled version of the population average as obtained from [dL96].
Approach	We used SNOPT [GSM96], a nonlinearly-constrained optimization package, for solving spacetime optimization, as well as for pre-fitting the momentum curves.
Approach	Most edits shown in the accompanying video clips were done in less than 1 minute.
Outcome	The optimization process for each motion took on the order of 2 to 4 minutes to fully converge on a 2Ghz Pentium 4 machine (see Table 1 ).
Outcome	Our system provides a set of UI tools to help the user rapidly specify modifications to existing motions.
Approach	In a hopping example, the animator interactively manipulates the position, height, and orientation of each ground stage.
Approach	The character must cover a longer distance, reach a greater height and assume a new orientation in the modified hopping motion, so she must lower her center of mass, lean farther to the right, and pivot slightly in preparation for the take-off.
Outcome	Despite these changes, the resultant motion remains stylistically similar to the original.
Approach	To show that our system is capable of making drastic changes from the original motion, we edited the same hopping motion to exhibit a 360 ◦ spin followed by a 180 ◦ spin in the opposite direction(see Figure 5 ).
Approach	In order to demonstrate real-time motion interpolation we modified a motion with two consecutive leaps.
Approach	We let the user control the landing and take-off positions along an evenly spaced grid to generate a set of parameterized motions.
Approach	Since the interpolation can be performed in real-time, we are able to generate a jumping motion with arbitrary takeoff and landing positions within the parameterized space in an interactive fashion.
Approach	Another example shows a soccer header motion observed to miss its target.
Approach	First, we correct the motion by increasing the height of the jump to meet the ball at the point of contact.
Approach	Next, we use our editing algorithm to generate a motion family parameterized over the space of the landing position of the motion.
Approach	By interpolating between the optimal motions, we are able to generate arbitrary intermediary motions where the character contacts the ball at any location within the sampled space, in real-time.
Approach	A more intuitive way to edit motion capture data with arbitrary positional constraints is to use our real-time inverse control mechanism.
Approach	In the volleyball slam example, the user interactively specifies the position of the character’s hand in mid-flight.
Outcome	Our system immediately determined the correct linear interpolation of 4 nearest neighbor samples to meet the positional constraint on the hand.
Outcome	The brightness of the sample motions on the floor indicates the weights associated with each sample.
Approach	We used 9 sampled motions which are all edits of the same input sequence.
Outcome	The demonstration shows various slam motions being generated in real-time by using the trajectory of the volleyball to guide the character’s motion.
Outcome	Our system can also be used to create a class of nonrealistic motions that allow the character to exhibit superhuman strength and to defy the laws of physics.
Approach	Consider an example where we wish to edit a jumping motion to reach a higher mid-point in the same time span as the the original motion.
Approach	The first observation to make is that this is physically impossible without altering the gravitational constant, which dictates the maximum rate at which the character returns to the ground from the height of the jump.
Approach	In our system it is easy to alter the gravitational constant in one or more ground stages.
Approach	Still, the character must gain the momentum required to achieve the specified height on takeoff and, subsequently, absorb the same amount of momentum on landing.
Approach	This requires a super-human muscle strength, but since we do not directly model muscle forces, and we place no limits on their magnitude, our system can easily handle these imaginary circumstances.
Background	From the animators perspective, editing non-realistic motion is the same as editing any other motion.
Approach	To increase the height of a flight stage, the animator simply manipulates a visualization of the trajectory of the motion in the flight stage to the required height, and then specifies whether the system should change gravity or, alternatively, the total time in the flight stage.
Approach	If the animator chooses to leave the gravity unaltered, the system increases the length of the time-step in each frame of the flight stage and then continues the editing process as normal.
Outcome	In one example, we edited a forward jump into a 2-meter-long backward jump (see Figure 7 ).
Approach	This work builds on the research in both physics-based motion synthesis and interpolation-based motion editing approaches.
Outcome	In this paper we suggest that using physics-based adaptation to create motion samples for the purpose of data interpolation is perhaps a "sweetspot" between these two approaches.
Outcome	Once the dataset is created, this paradigm allows animators to interactively edit the realistic dynamic motion.
Outcome	The primary contribution of this work is a new momentum-based method for adaptation of ballistic character movement.
Outcome	In contrast to previous dynamic-based adaptation methods, our framework can produce an wide range of motions that are significantly different from the original motion.
Outcome	Our method does not require model reduction, or a reduced motion space.
Outcome	Because we do not solve for the generalized forces for each joint angle, our method is also significantly faster than other physics-based transformation methods.
Outcome	This speed allows us to create a large number of motions within a reasonable time.
Outcome	Once the family of parameterized motion samples has been generated, we describe an interactive framework where the animator can explore the space of realistic motions.
Outcome	We also show how the same framework can be adapted for inverse control.
Outcome	Finally, we show how real-time data-driven controllers for realistic human motion can be constructed from a single motion capture sequence.
Outcome	Naturally, our framework does not handle all realistic character motions.
Outcome	It specifically applies to highly-dynamic motions with ballistic stages.
Outcome	We suspect that momentumbased approach would not be well suited for less energetic motions such as walking.
Outcome	Furthermore, the number of samples required is exponentially proportional to the number of dimensions, thus the current framework is hindered by the offline computation of a large dataset.
Outcome	There are several ways to facilitate the computation by taking advantage of the fact that we are solving a sequence of very similar problems.
FutureWork	A more intelligent sampling strategy is essential for generalizing our approach to a multi-dimensional dynamic space.
Outcome	Because our model does not account for realistic muscle strength, and friction during ground contact there are some extreme cases which do not produce realistic motion.
FutureWork	Adding heuristics such as balance during contact can to a large extent eliminate these problems.

Challenge	While such vertex-parallel computation is often done on the GPU vertex processors, further parallelism can potentially be obtained by using the fragment processors.
Challenge	In this paper, we develop a parallel deformation method using the GPU fragment processors.
Approach	Joint weights for each vertex are automatically calculated from sample poses, thereby reducing manual effort and enhancing the quality of WPSD as well as SSD (Skeletal Subspace Deformation).
Outcome	We show sufficient speed-up of SSD, PSD (Pose Space Deformation) and WPSD to make them suitable for real-time applications.
Background	Skinning is an important part of realistic articulated body animation and is an important topic of computer graphics and animation.
Background	Generally, skinning can be categorized into algorithmic, physically-based, and example-based methods.
Challenge	Although widely used, simple algorithmic skinning schemes cannot capture the complexity and subtlety of real skin deformation, and revised approaches will be required to increase character animation realism.
Background	Physically-based skinning is based on the biomechanics of skin deformation arising from the motions of muscles and tendons.
Challenge	Although this approach can generate physically accurate simulations of each layer, it is not at present suitable for real time applications such as gaming due to the large computation required.
Background	Example-based methods capture some of the complexity of real skin deformation by interpolating scanned or sculpted examples of the desired skin shape in various poses.
Background	Al-      though this requires gathering a sufficient number of samples and some pre-calculation, example-based methods can potentially be used in real-time applications due to their relatively simple real-time computation.
Background	Weighted pose space deformation (WPSD) is an example based skinning method that generates high quality skinning with a limited number of sample poses [KM04].
Background	Although it can generate an accurate skinning, it requires more computation than the original pose space deformation (PSD) [LCF00], since joint distances are computed independently for each vertex.
Background	As such, this method has not been suitable for real-time applications.
Background	Furthermore, both WPSD and SSD require joint weights for each vertex, and accurate joint weights are required to achieve good results.
Challenge	However, the weights are usually manually generated by artists, which requires effort and great skill in the case of a complex skeletal system such as the human hand.
Challenge	In this paper, we present a parallel WPSD algorithm (including automatic determination of joint weights) suitable for SIMD architectures such as current GPUs.
Approach	This can enhance the skinning quality not only of SSD but also WPSD, since both methods require accurate joint weight values.
Approach	The deformation required in WPSD and SSD is independent for each vertex and this per-vertex computation can be parallelized in a SIMD architecture.
Background	The GPU is a general SIMD architecture having one-sided (unidirectional) communication to texture memory.
Approach	We demonstrate our parallel WPSD method using GPU fragment processors.
Approach	In our experiments, we can speed up SSD, PSD, as well as WPSD to around 20 times faster than on the CPU (from 1.2FPS to 25FPS speed-up of WPSD on a detailed model having 22836 triangles with 11574 vertices) using a modern graphics card, thus making WPSD a feasible real-time skinning solution for various applications including games, virtual reality, and other real-time simulations.
Background	Many commercial software packages generate skin deformation arising from joint movement using a method known as (linear blend) skinning, Skeletal Subspace Deformation (SSD), enveloping, etc., based in part on work published by Thalmann et al. [MTLT88].
Background	SSD is based on the weighted blending of affine transformations of each joint and used in many real-time applications due to its simple and fast computation.
Background	However, it also exhibits some well known artifacts such as skin that collapses around the joints at increasing bend angles, and a variety of solutions for these problems have been published [Web00, WP02, MTG03, KZ05].
Background	Recently, example-based methods [LCF00, SRC01, ACP02, KJP02, KM04] have permitted more complex skinning effects such as muscle bulges and major wrinkles, while also addressing the artifacts of simple algorithmic schemes.
Background	In these methods, a number of provided (scanned or sculpted) samples of the desired skin shape are simply interpolated based on the creature’s pose (and possibly additional abstract control “dimensions”).
Background	These example-based methods can also be considered as a non-parametric approach to skin deformation.
Background	In common with non-parametric sampling methods in texture synthesis (and more generally in statistical regression), the amount of memory for these methods grows with the number of training samples, but arbitrary distributions can be approximated.
Background	Some of the most impressive example-based results to date are those of Kurihara and Miyata’s hand model derived from medical images [KM04].
Background	Since acquiring 3D medical images is relatively expensive, they developed weighted pose space deformation (WPSD) to generate proper skinning from a limited number of pose samples.
Background	They modify the distance between poses using the joint weights of each vertex to provide a more appropriate distance measure for skinning.
Background	Although the joint weights for each vertex are important data for SSD and WPSD calculations, they have traditionally been manually generated by skilled artists.
Background	Least-squares based vertex weight estimation was shown in the skinning methods [WP02, MTG03].
Background	James et al. describe mesh based skinning including estimation of bone parameters and vertex weights for each bone [JT05].
Background	In their paper, the vertex weights of each joint are calculated by NNLS (non-negative least squares) and we derive a similar approach to calculate weights for SSD and WPSD.
Background	In recent years, since the performance of GPUs has been improving more rapidly than that of CPUs, and GPUs have many processing units serving as a SIMD parallel architecture, many algorithms have been accelerated by GPU programming [LHK ∗ 04, PF05, GPG].
Background	Deformation and skinning algorithms can also be enhanced by GPUs and several papers have profited from this [JP02, KJP02, BK05, JT05].
Background	However, in previous research, since vertex information cannot be accessed in the fragment program, GPU-based vertex deformation is usually performed by vertex programs.
Challenge	In this paper, we develop a parallel WPSD method using the fragment processors to gain greater parallelism and performance.
Background	Person-specific data modeling and its deformation is also an interesting topic in realistic articulated body simulation.
Background	Rhee et al. described human hand modeling from surface anatomy of the person [RNL06].
Background	Anguelov et al. developed shape completion and animation of people, derived from the set of range scan data and example based deformation in pose and shape space [ASK ∗ 05].
Background	Physically inspired skinning should be also recognized as another important area of articulated body animation.
Background	However, we entrust the review of the subject to the recent related papers [AHS03, CBC ∗ 05, PCLS05, SNF05].
Approach	In skeletal subspace deformation the displacement D(p a ) is omitted and the target surface is calculated by SSD as a blend of affine transforms of v 0 [section 3.1].
Approach	Skinning methods related to PSD use the displacement of an arbitrary pose D(p a ), calculated by interpolation in pose space [section 3.2].
Approach	SSD [MTLT88] is based on the weighted blending of an affine transformation of each joint by equation 2.
Approach	The weight w j can be assigned by the artist to control deformation and usually ∑ n j=1 joint (w j ) = 1.0.
Background	This simple algorithm is used in many commercial graphics packages and real-time rendering applications but shows several limitations, because the deformation of this method is restricted to the subspace of the affine transformation of the joints [LCF00].
Approach	If we have a sufficient set of examples to describe the movement of an articulated object, we can interpolate displacement in “pose space” [LCF00].
Approach	Each sample pose consists of sample skin geometry and the related joint skeleton, and a vector containing the joint angles represents the pose.
Approach	Note that the inverse here is of the weighted sum of affine transforms.
Approach	After defining the displacement of each pose, the displacement at an arbitrary pose can be calculated by RBF (Radial Basis Function) [LCF00] or normalized radial basis function [KM04] interpolation of the example poses’ displacements.
Approach	The weight r k (p a ) is calculated using normalized RBFs and is used in equation 4 to calculate the displacement d a of a vertex in an arbitrary pose p a :
Background	WPSD is developed by Kurihara et al. [KM04] to deform their example-based human hand model derived from medical images.
Approach	In equation 7, since the γ k is the difference of n joint dimensional joint vectors of related poses, every vertex in the pose p k has same distance γ k resulting in the same weight r k (p a ) in every vertex of the pose p k .
Approach	Furthermore, because each element of the joint vector equally contributes to the distance calculation, two vectors having a same value but different order generate same pose distance.
Approach	For example, three different joint vectors p 1 = (θ, 0, 0), p 2 = (0, θ, 0), p 3 = (0, 0, θ) have same distance between them and it can cause unexpected results in PSD.
Background	In WPSD [KM04], Kurihara et al. modify the distance definition between poses using joint weight of each vertex i to give proper weight to each element of a joint vector,
Approach	From this definition, a more accurate pose distance is obtained and it generates better skinning in arbitrary poses, especially when the poses are far from the examples.
Background	The joint weights of each vertex are important to generate accurate skinning in SSD (equation 2) as well as in WPSD (equation 8).
Background	In many applications, the weights are manually generated by skilled artists and it is hard to generate accurate values when a number of joints are involved in deforming a region.
Approach	In this paper, we automatically calculate the joint weights of each vertex from the sample poses to enhance the accuracy of the weight value.
Outcome	This results in better skinning and reduces the elaborate manual work required to create weight maps.
Approach	>From equation 11, we can calculate w from the given value of v and A to reduce the error of this equation.
Approach	We use the non-negative least square (NNLS) method to solve this problem and it determines positive weight values minimizing error in equation 10.
Approach	The calculated weight vector w is normalized to satisfy ∑ n j=1 joint w j = 1.0.
Approach	In order to avoid a singular matrix A, the number of poses should be greater or equal to the number of overall DOF (Degree Of Freedom) of the joint vector (each joint has 3 DOF), and the sample poses should be sufficiently different.
Approach	James et al. used a similar approach to estimate vertex weights in each joint [JT05] and we demonstrate their efforts in our skinning method.
Background	Skinning deformations vary across vertices.
Background	In SSD and WPSD, this per-vertex computation is independent for each vertex and can be parallelized by a SIMD parallel architecture.
Outcome	We developed a parallel skinning algorithm for SSD and WPSD that is suitable to GPUs having a SIMD architecture with one-side communication to texture memory.
Approach	The computation cost of the SSD skinning algorithm is O(n vertex × n joint ) from equations 1, 2, PSD is O(n vertex × n joint × n pose ) from equations 1, 2, 4, and WPSD is O(n vertex × n joint × n pose × n pose × n pose ) from equations 1, 2, 4, 5, 6.
Approach	Where, computation cost of original PSD is defined by equation 1, 2, 4, since r i is same in all vertices and d i can be pre-calculated.
Approach	The number of joints n joint and poses n pose can be reduced to the smaller numbers using the method developed by Kry et al. [KJP02], as will be discussed in section 5.2.1 with efforts to reduce texture memory space.
Background	In previous research, the Eigenskin method based on PSD was developed using GPU vertex programming [KJP02].
Approach	The vertex program uses a relatively small number of slow processing units compared with the fragment processors, and the per-vertex computation cost of the original PSD is O(n joint × n pose ).
Outcome	Therefore WPSD, having higher pervertex computation cost O(n joint × n pose × n pose × n pose ), can clearly benefit from parallel computation on fragment processors.
Approach	We developed parallel skinning using the GPU fragment processors and demonstrate our method using three rendering passes.
Approach	In order to minimize real-time computation, we separate possible pre-calculation steps and save the results into texture memory using texture maps.
Approach	Because the value in the texture memory is not changed in the successive deformation, it can be pre-computed and stored in the read-only texture memory.
Approach	In the first and second pass, per-vertex deformation is calculated in the fragment program and the results are stored in texture maps using the FBO (Frame Buffer Object) extension [Gre05].
Approach	These texture maps are bound to the geometry of the rest pose with their texture coordinates.
Approach	In the third pass, each vertex in the rest pose is changed by the deformed vertex stored in the output texture generated in the first and second passes using vertex texture fetch.
Background	The fragment processors cannot access vertex information.
Approach	Instead, we can use texture memory to send data to the fragment program.
Approach	Information needed in the fragment program is packed into texture maps and stored into texture memory.
Approach	Geometry information from the rest pose is stored into two RGB texture maps, a vertex texture T v and normal texture T n ; each has size n vertex × 3.
Approach	These textures represent parameter v 0 in equation 2 and each 3D element (x, y, z) is stored into the (r, g, b) value of a texel [ Figure 2 ].
Background	In general, the distribution of skinning effects in an articulated body is local to several joints [MMT97,KJP02], even in a region as complicated as a hand.
Background	For example, deformations arising from the PIP (Proximal Interphalangeal) joint of index finger do not propagate to the other fingers, and deformation on the middle phalanx of index finger is only affected by the movement of PIP and DIP(Distal phalanx) joints.
Approach	From this observation, we can reduce joint weight storage from the actual number of joint n joint to a smaller number of “principal joints” n  ̃ joint selected by sorting on the weight value.
Approach	We threshold n  ̃ joint at four in our tests with an additional four elements to hold the related joint index.
Approach	As a result, we can save the joint weights of entire geometry in two RGBA textures T w1 , T w2 each with size n vertex × 4(rgba) and store the entire information required for SSD [equation 2] in four textures T v , T n , T w1 , and T w2 .
Approach	The displacement values calculated by equation 3 can be stored in n pose displacement textures; n pose is the number of sample poses.
Approach	In case of complex joint structures and a large DOF model, we need many sample poses to calculate accurate joint weights and PSD deformation.
Approach	However, since the joint weights can be pre-calculated, we can reduce the number of sample poses needed in real-time PSD computation.
Background	PCA (Principal Component Analysis) of pose space can yield an orthogonal basis called “ Eigendisplacement ” [KJP02].
Approach	If we reduce the size of pose space from n pose to n  ̃ pose “principal poses” ( n  ̃ pose < n pose ), we can reduce the number of displacement textures.
Approach	In our paper, we set n  ̃ pose as eight in our experiment and save displacements of all poses into a RGB texture T d having size n vertex × 8( n  ̃ pose ) × 3(rgb).
Approach	Therefore, from the two important observations of “principal joints” and “principal poses”, the original computation cost for SSD, PSD, and WPSD discussed in section 5.1 can be reduced using n  ̃ joint and n  ̃ pose rather than n joint and n pose .
Approach	In the original PSD, since the weight r i in equation 4 is the same at every vertex, we do not need to calculate this value in the GPU.
Approach	Since the size of this value is just n  ̃ pose , we can simply pass them to the GPU as parameters without generating a texture map.
Approach	Therefore, we store all the information needed to calculate the original PSD at this point.
Approach	In order to reduce real-time computation, we pre-calculate T j in equation 2 and λ in equation 5 and store them into another one channel texture T x having size n  ̃ pose × ( n  ̃ pose + n  ̃ joint × 3(x, y, z)).
Approach	As a result, we store all the variables required to calculate WPSD, PSD, and SSD in six texture maps: T v , T n , T w1 , T w2 , T d , and T x .
Approach	The values in the texture maps are stored in the texture memory at setup time, since they are not changed during the deformation process.
Background	In current graphic card architectures, data transfer from CPU to GPU is slow compared with memory access within the GPU.
Outcome	Although, we cannot access vertex data in the fragment program, the efficiency of parallel computation on a fragment program is higher, since the fragment processor has more processing units and each of them has more computation power than a vertex processor.
Approach	The fragment processing system is a general SIMD architecture using fragment streams as input data; each fragment is assigned to a fragment processor to calculate its final color value independently and in parallel.
Approach	We developed a parallel WPSD algorithms using the fragment processors to enhance the extent of parallel computation.
Approach	Geometry information like vertex positions and normals are stored in texture maps T v and T n as described in section 5.2.1 and the vertex information is referred in the fragment processors to calculate final color values.
Approach	In order to assign each vertex value stored in a texture map to a fragment, we bind the geometry texture T v or T n to a quad and render it using an orthographic camera having the same width and height as the quad.
Approach	Furthermore, since the viewport is set to the same resolution as the textures, each fragment is exactly matched with each texel holding the vertex information, and we can access each vertex using the texture coordinates of the fragment; vertex weights and displacements stored in the texture maps can also be accessed by similar methods.
Background	A similar idea was developed in [PBMH02] to calculate ray tracing in a fragment program and is used in GPGPU (General Purpose computation on GPUs) applications [GPG, LHK ∗ 04, PF05].
Background	The FBO (Frame Buffer Object) extension [Gre05] supports rendering into an attached texture.
Background	This saves memory and time, since there is no copy operation from frame buffer to texture buffer.
Approach	We implemented our WPSD algorithm using the fragment program with the FBO extension to store the result directly into texture maps accessed by vertex program in the next pass.
Approach	We implemented GPU deformation using three rendering passes, and the basic architecture is described in figure 3 .
Approach	In the first pass, we parallelize per-vertex deformation using GPU fragment processors.
Approach	The data required to calculate this deformation is stored in the textures as described in section 5.2.1 and the deformation for each vertex is calculated in a fragment processor.
Approach	In a given arbitrary pose defined by a joint vector, SSD is computed by equation 2 using texture maps T v , T w1 , T w2 and T x ; refer to the texture map notation in section 5.2.1.
Approach	PSD is computed by equation 4 using T d , T x , after calculating r k (p a ) by equation 6.
Approach	In the second pass we calculate and store normal deformation with a similar method as in the first pass, and the results are stored in the texture map T n ′ .
Approach	In the third pass, using a vertex program, each vertex of the rest pose is transformed to the final deformed position using the information from the texture maps computed in the previous two passes.
Approach	In order to access related texture information in each vertex, we created texture coordinates of each texel in pre-processing and used them in the vertex program.
Approach	Specifically, the two texture maps, T v ′ and T n ′ that are generated in the first and second passes are accessed in the vertex program using the texture coordinate of the current vertex.
Approach	Alternatively, multiple render targets (MRTs) can combine the first and second pass, and vertex buffer objects (VBOs) could be used to render the deformed results back to the vertex array [OPE, GPG, LHK ∗ 04].
Approach	We tested our methods using upper arm models consisting of four joints (collar, shoulder, elbow, and wrist).
Approach	Each has three DOF and the wrist is the end joint having no DOF.
Approach	Three different resolution meshes are used to test the performance of GPU parallel computation: the high-resolution model has 91460 triangles with 46036 vertices, the midresolution model has 22836 triangles with 11574 vertices, and the low-resolution model has 5762 triangles with 2972 vertices [ Figure 4 ].
Approach	Note that these models are considerably more detailed than those used in current games, so the reported frame rates would be much higher if typical gameresolution models were used.
Outcome	On the other hand, with the expected growth of GPU processing power, models such as these will be in wide use in a few years, and algorithms such as WPSD will be required to produce realistic deformations at this level of resolution.
Approach	Eight sample poses were created by Poser [Cur] and the joints weights and displacements of each sample were derived from these models [ Figure 5 ].
Approach	Our parallel algorithm is based on three pass GPU computation.
Approach	The fragment program for the 1st and 2nd pass, and the vertex program for the 3rd pass are implemented in the Cg language [FK03].
Approach	For accuracy the GPU computation is performed by 32bit floating point operations with 32bit floating point texture maps.
Approach	Note that the maximum required memory space for the highest resolution model is just 6.8 Mbytes; the size of the output texture T v ′ and T n ′ is the same as the size of T v and T n .
Outcome	The results of GPU-based deformation for SSD, PSD, and WPSD are shown in Figure 1 and 6, and the experiment is performed in a GeForce 6800 Ultra GPU and a 3.4Ghz Pentium 4 CPU.
Outcome	The timing results of each algorithm on the CPU and GPU are summarized in table 1 .
Outcome	On average, our GPU-based deformation shows around 20 times speed-up compared with CPU-based deformation.
Outcome	GPU-based WPSD has roughly the same speed as CPUbased SSD.
Outcome	Therefore, real-time applications using SSD can substitute WPSD running on the GPU without loosing their real-time performance.
Outcome	Since our algorithm shows speed-up for SSD and PSD as well as WPSD, applications can choose the most appropriate skinning method according to the required deformation and detail.
Outcome	In this paper, we present a parallel skinning algorithm suitable for SIMD architectures such as GPUs.
Approach	The joint weights of each vertex are automatically computed by NNLS and used in the skinning computation for SSD and WPSD.
Approach	Independent per-vertex deformation is parallelized on the GPU using three rendering passes.
Approach	In the first and second passes, per-vertex deformation is calculated by the fragment processors and the results are stored in texture maps using FBO.
Approach	In the third pass, using vertex processors, each vertex of the rest pose is changed by the deformed vertex stored in the textures generated by the first and second passes.
Outcome	Articulated body skinning using SSD, PSD, and WPSD are efficiently parallelized by our GPU-based method, and on a detailed model, we obtain around 20 times speed-up compared with CPU-based computation.
Outcome	Principal component compression of the examples and careful analysis of joint distributions can reduce the domain of computation [KJP02] and other algorithms based on the SSD, PSD, and shape interpolation may be parallelized on GPU using our approach.

Challenge	An ambitious goal in the area of physics-based computer animation is the creation of virtual actors that autonomously synthesize realistic human motions and possess a broad repertoire of lifelike motor skills.
Challenge	To this end, the control of dynamic, anthropomorphic figures subject to gravity and contact forces remains a difficult open problem.
Challenge	We propose a framework for composing controllers in order to enhance the motor abilities of such figures.
Outcome	A key contribution of our composition framework is an explicit model of the “pre-conditions” under which motor controllers are expected to function properly.
Approach	We demonstrate controller composition with pre-conditions determined not only manually, but also automatically based on Support Vector Machine (SVM) learning theory.
Approach	We evaluate our composition framework using a family of controllers capable of synthesizing basic actions such as balance, protective stepping when balance is disturbed, protective arm reactions when falling, and multiple ways of standing up after a fall.
Outcome	We furthermore demonstrate these basic controllers working in conjunction with more dynamic motor skills within a prototype virtual stuntperson.
Outcome	Our composition framework promises to enable the community of physics-based animation practitioners to easily exchange motor controllers and integrate them into dynamic characters.
Challenge	Despite the considerable history of progress in animating virtual humans [ 3 , 7 ], physics-based animated characters with a large repertoire of motor skills have so far been elusive.
Background	This may seem surprising in view of the recent successes in implementing a slew of specialist controllers capable of realistically synthesizing the complex dynamics of running, diving, and various gymnastic maneuvers [ 16 ].
Background	While a divide-and-conquer strategy is clearly prudent in coping with the enormous variety of controlled motions that humans and other animals may perform, little effort has been directed at how the resulting control solutions may be integrated to yield composite controllers with significantly broader functionalities.
Challenge	For example, if researcher A creates a walking controller for a dynamic character while researcher B creates a running controller for the same articulated model, it would be beneficial if they could share their controllers (perhaps through an e-mail exchange) and easily create a composite controller enabling the character to both walk and run.
Challenge	This is a difficult problem, but its resolution would help pave the way towards controller libraries for dynamic animation which communities of practitioners could utilize and to which they could contribute.
Challenge	In this paper, we propose a simple yet effective framework for composing specialist controllers into more general and capable control systems for dynamic characters.
Approach	In our framework, individual controllers are black boxes encapsulating control knowledge that is possibly gleaned from the biomechanics literature, derived from the robotics control literature, or developed specifically for animation control.
Approach	Individual controllers must be able to determine two things: (1) a controller should be able to determine whether or not it can take the dynamic character from its current state to some desired goal state, and (2) an active controller should be able to determine whether it is operating nominally, whether it has succeeded, or whether it has failed.
Approach	Any controller that can answer these queries may be added to a pool of controllers managed by a supervisor controller whose goal is to resolve more complex control tasks.
Outcome	An important technical contribution within our controller composition framework is an explicit model of pre-conditions.
Approach	Preconditions characterize those regions of the dynamic figure’s state space within which an individual controller is able to successfully carry out its mission.
Approach	Initially, we demonstrate the successful composition of controllers based on manually determined pre-conditions.
Approach	We then proceed to investigate the question of whether pre-conditions can be determined automatically.
Approach	We devise a promising solution which employs Support Vector Machine (SVM) learning theory.
Approach	Our novel application of this technique learns appropriate pre-conditions through the repeated sampling of individual controller behavior in operation.
Approach	As a testbed of our techniques, we are developing a physicallysimulated animated character capable of a large repertoire of motor skills.
Outcome	An obvious application of such a character is the creation of a virtual stuntperson: the dynamic nature of typical stunts makes them dangerous to perform, but also makes them an attractive candidate for the use of physics-based animation.
Challenge	The open challenge here lies in developing appropriate control strategies for specific actions and ways of integrating them into a coherent whole.
Challenge	In this paper, we demonstrate families of composable controllers for articulated skeletons whose physical parameters reflect anthropometric data consistent with a fully-fleshed adult male.
Approach	One family of controllers is for a 37 degree-of-freedom (DOF) 3D articulated skeleton, while a second family of controllers has been developed for a comparable 16 DOF 2D articulated skeleton.
Approach	While the 3D skeleton illustrates the ultimate promise of the technique, the easier control associated with the 2D skeleton allows for more rapid prototyping of larger families of controllers and more careful analysis of their operation.
Challenge	As has been recognized in the robotics literature, the control of broad skilled repertoires of motion remains very much an open problem even for 2D articulated figures.
Outcome	The upright balancing dynamic figure is pushed backwards by an external force; its arms react protectively to cushion the impact with the ground; the figure comes to rest in a supine position; it rolls over to a prone position, pushes itself up on all fours, and rises to its feet; finally it balances upright once again.
Outcome	A subsequent disturbance will elicit similar though by no means identical autonomous behavior, because the initial conditions and external forces will usually not be exactly the same.
Outcome	Control sequences of such intricacy for fully dynamic articulated figures are unprecedented in the physics-based animation literature.
Challenge	The simulation and animation of human characters is a challenging problem in many respects.
Challenge	Comprehensive solutions must aspire to distill and integrate knowledge from biomechanics, robotics, control, and animation.
Challenge	Models for human motion must also meet a particularly high standard, given our familiarity with what the results should look like.
Challenge	Not surprisingly, a divide-and-conquer strategy is evident in most approaches, focusing efforts on reproducing particular motions in order to yield a tractable problem and to allow for comparative analysis.
Background	The biomechanics literature is a useful source of predictive models for specific motions, typically based on experimental data supplemented by careful analysis.
Background	These models target applications such as medical diagnosis, the understanding and treatment of motor control problems, the analysis of accidents and disabilities, and high-performance athletics.
Background	Computer simulation is becoming an increasingly useful tool in this domain as the motion models evolve to become more complex and comprehensive [ 26 , 27 , 29 ].
Background	Given the challenge of achieving high-fidelity motion models for individual motions, there have been fewer efforts towards integrated solutions applicable to multiple motions.
Background	Reference [ 26 ] is one such example.
Background	Robotics research has made remarkable progress in the successful design of a variety of legged robots [ 28 ] and, more recently, bipedal robots with anthropomorphic aspirations [ 23 ].
Background	Despite their limited motion repertoires and rather deliberate movements, these robotic systems are truly engineering marvels.
Background	The work in [ 1 ] provides a good summary of behavioral architectures explored in the context of robotics.
Background	A 3 DOF ball-juggling robot is described in [ 6 ] which uses a theory of behavior composition, although the practicality of extending the method to high-DOF dynamic models of human motions is unclear.
Background	Computer animation is to a large extent unencumbered by the exacting fidelity requirements of biomechanical models and the mechanical limitations of robotic systems.
Background	This has spawned a great variety of kinematic and dynamic models for character motion [ 3 , 4 , 7 ].
Background	While motion capture solutions based on blending and warping techniques may give satisfactory results for such tasks in the short term, controller based approaches reveal more about the physics, planning, and control of such motions and they therefore serve as a basis for more general solutions.
Background	Dynamically simulated characters were first proposed over 15 years ago [ 2 , 34 ] and since then have progressed in sophistication in a variety of directions.
Background	Controllers have been successfully designed for specific human motions such as walking, running, vaulting, cycling, etc. [ 16 , 22 , 35 ].
Challenge	Dynamically simulated articulated characters equipped with an integrated, wide-ranging repertoire of motor skills currently remain an unachieved goal.
Background	Some positive steps in this direction are evident, however.
Background	Examples include an integrated repertoire of motor controllers for biomechanically animated fish [ 30 ], a methodology for controller design and integration applicable to simple figures [ 32 ], a demonstration of successful integration for selected diving and gymnastic motions [ 35 ], and adapting a controller designed for one character to work on another character [ 17 ].
Background	The work of Wooten [ 35 ] is the most relevant as an example of a sequence of successive transitions between several controllers for human motions such as leaping, tumbling, landing, and balancing.
Background	Transitions are realized by including the end state of some controllers in the starting states of other controllers.
Background	This currently remains ambitious work in progress.
Challenge	Our work is aimed at creating dynamic human characters with broadly integrated action repertoires.
Approach	Unlike previous work focusing on specific athletic movements, our methodology is to begin with a core set of simple actions, including balancing, small steps, falling reactions, recovery from falls, standing up from a chair, and others.
Approach	In the present paper, we do not cover in any appreciable detail the design of individual controllers to effect such basic actions.
Outcome	1 Rather, our contribution here is a framework for composing individual controllers, however they may be designed, into more capable control systems for dynamic characters.
Outcome	Our pre-condition learning algorithm adds to the growing body of learning algorithms that have been successfully applied in the context of computer animation in recent years [ 14 , 15 ].
Approach	In our controller composition framework, we consider individual controllers as black boxes which are managed by a simple supervisor controller.
Approach	When no controller is active, the supervisor polls the pool of controllers, querying each whether it can handle the transition of the dynamic character from its current state to the desired goal state.
Approach	Individual controllers return an integer confidence/suitability score when queried in order to bid on becoming the active controller.
Approach	In our implementation, controllers that can perform a sensible action given the current state of the character return an integer in the range 1⁄2 1⁄21⁄4 , while those that can handle the current state as well as guarantee a transition to the desired state, return an integer in the range 1⁄21⁄4 3⁄41⁄4 .
Approach	Lastly, a value of 1⁄4 means that a controller is unsuited for the current state.
Approach	The controller that returns the highest score becomes active.
Approach	While this scoring scheme potentially allows for a nuanced evaluation of the controller suitability in terms of criteria such as probability of success or energy used, our current controllers resort to a simpler scheme.
Approach	This consists of a binary success/failure evaluation multiplied by a weighting factor assigned to each controller that serves to establish a relative preference ordering.
Approach	It does not appreciably burden the controller design task.
Approach	Each controller can be as primitive or as sophisticated as its designer wishes.
Approach	A controller within the pool of available controllers can be as simple as a constant force, or as complex as a structured hierarchy of multiple levels of control abstraction.
Approach	For example, as more controllers are added to the system, we may wish to group all the walking and running controllers together into a cluster that can be treated as one encapsulated controller.
Approach	Regardless of the encapsulation, our composition method requires controllers to define pre-conditions, post-conditions and expected performance.
Approach	Pre-conditions are a set of conditions over the state of the character and the environment.
Approach	If these conditions are met then the controller can operate and possibly enable the character to satisfy the post-conditions.
Approach	Assuming that the pre-conditions were met, the post-conditions define a range of states for the final state of the character after the execution of the controller.
Approach	In other words the controller realizes a mapping between a domain of input states to a range of output states for the character.
Approach	Because of unexpected changes in the environment, this mapping may not always succeed, which motivates the notion of expected performance.
Approach	The controller should be able to evaluate its performance in order to detect failure at any point during its operation.
Approach	To do this, the controller must at all times have knowledge of the current and expected state of the character or the environment.
Approach	Defining the pre-conditions, post-conditions, and expected performance for complex characters, motions, and environments is not a straightforward task.
Approach	However, we believe that the effort required to generate these specifications is a fair and necessary price to pay to achieve the benefits of composability.
Approach	Controllers that adhere to these specifications can form a pool of available controllers managed by the supervising controller.
Approach	The position and velocity of the center of mass are denoted as and respectively.
Approach	The base of support of a figure (often called the support polygon) is denoted as Ë .
Approach	It is represented by a polygon that surrounds the foot or feet that are in contact with the ground at any given time.
Approach	In general, pre-conditions are relationships and constraints involving several different parameters.
Approach	Most of our controllers can operate within a small region of the state space which we denote Ê  ́ Õ μ .
Approach	These include the contact points between the character and the ground, as well as the normal of the ground and the amount of friction at the contact points.
Approach	Usually, this is indicated by the relative position and velocity between the figure’s center of mass and the base of support.
Approach	Typically, if the projection of along the gravity vector does not intersect the base of support Ë , the figure is considered to be unbalanced.
Approach	We denote the balance conditions as  ́ Ë μ .
Approach	Successful operation of a controller brings the character from an initial state, as defined by the pre-conditions, to a desired state or a desired region Ê  ́ Õ Ó μ in the state space.
Approach	In general, however, the post-conditions are different from the pre-conditions.
Approach	For example, while a pre-condition for a falling controller requires that the center of mass be moving, the postconditions require that the center of mass be at rest.
Approach	Our framework permits the automatic selection of the appropriate controller based on the information provided by the controllers themselves.
Approach	Only the individual controllers can detect whether they are operating normally or whether failure is imminent.
Approach	Failure in our case means that the controller cannot meet its post-conditions Ç .
Approach	The controller may fail because of a sudden change in the environment or because of badly designed pre-conditions.
Approach	The sooner a controller can detect failure the sooner another more appropriate controller can take over.
Approach	This is important for making a character behave naturally.
Approach	For example, the character should not attempt to continue a walking gait if it has lost its balance and it is falling.
Approach	In our implementation, the expected performance consists of expressions similar to those of the pre-conditions È .
Approach	In particular if the controller successfully completes its task in the time interval Ø 1⁄2 , Ø 3⁄4 , then  ́ Ø 1⁄2 μ 3⁄4È and  ́ Ø 3⁄4 μ 3⁄4Ç .
Approach	Transitions between controllers are not explicitly modeled as they would be in a finite state machine.
Approach	They occur implicitly in response to the evolution of the motion over time, as the system state traverses the “regions-of-competency” of the various controllers.
Approach	Nevertheless, given that most controllers are designed for specific situations, typical patterns of controller activation occur.
Approach	For example, the controllers and transitions used in achieving the motion shown in Fig. 1 is given by balance fall default rollover prone-tostanding balance.
Approach	For example, the prone-to-standing fall transition can occur if the figure is given a sufficiently strong push while rising.
Approach	Most of the transitions which are not shown but are still practically feasible are of this nature, dealing with falling behaviors.
Approach	Note that the fall controller always responds to the specific direction of the current fall.
Approach	Any transition involves one controller being deactivated and another being activated.
Approach	A controller can become deactivated (and thereby elicit a transition) for one of three reasons.
Approach	First, it may relinquish control by declaring success upon reaching its postcondition, as is the case for a standup controller which has successfully returned the character to a standing position.
Approach	Second, user intervention may elicit a transition.
Approach	The controllers designed for sitting or balanced standing will retain control until intervention by a user (or by a higher level planner) forces a desired transition.
Approach	Thus, when the 2D character is balanced a user-driven process must choose among the next plausible actions, namely one of sit, walk, or dive (see Fig. 4 ).
Approach	Third, a controller may detect failure, as will be the case for unpredictable events such as a push or an unforeseen obstacle causing the character to trip.
Approach	The transitions in Figs. 3 and 4 are labelled according to the type of controller deactivations which typically elicit the given transition patterns.
Approach	We note that our framework is designed to work in interactive settings.
Approach	As such, controllers typically start with slightly different initial conditions each time they are invoked, the user can interact with the character at any time, and generally there are no guarantees that the controller will reach the same end state each time it operates.
Approach	As a result, the transition graph is dynamic in structure.
Challenge	For controllers associated with complex dynamic characters, determining the exact region of the state space and the general conditions that determine success or failure of the controller is in general a non-trivial matter.
Approach	The manual approach allows designers to incorporate their knowledge within controllers, whereas the automatic approach is based on machine learning techniques.
Background	For certain cases, suitable pre-conditions for specific controllers may be found in the biomechanics literature [ 8 , 25 ].
Background	For example Pai and Patton [ 25 ] present a comprehensive study of balance in the sagittal plane and identify the conditions under which a human can compensate for postural disturbances and maintain balance without stepping.
Background	For certain other cases, the pre-conditions are trivially defined by the desired motion itself.
Background	Certain controllers function as intermediate stages between other controllers.
Challenge	In any case, the designer of a controller presumably understands the way the controller operates, and thus is able to provide high level conditions on its success or failure.
Challenge	For example, the designer of a walking controller knows if the controller can operate when the walking surface has minimal friction properties.
Challenge	Also, human motion is shaped by notions such as comfort, and only the designer can take this into account.
Challenge	For example, if a person is pushed while standing he/she might take a protective step because it may be more comfortable to do so instead of maintaining an inverted pendulum balancing strategy.
Background	Similarly, the way people react to slipping and imbalance and the protective behaviors they employ are largely age dependent.
Approach	In this section, we introduce an automatic, machine learning approach to determining pre-conditions, which is based on systematically sampling the performance of controllers.
Approach	Our method uses a machine learning algorithm attributed to Vapnik [ 33 ] known as Support Vector Machines (SVMs), which has recently attracted much attention, since in most cases the performance of SVMs matches or exceeds that of competing methods.
Background	SVMs are a method for fitting functions to sets of labeled training data.
Background	The functions can be general regression functions or they can be classification functions.
Approach	In our application, we use simple classification functions with binary outputs which encode the success or failure of a controller.
Background	Burges [ 5 ] provides an excellent tutorial on SVMs.
Approach	Mathematically, we are given Ð observations, each consisting of an dimensional vector Ü 3⁄4 1⁄2 Ð and the associated “truth” Ý 3⁄4   1⁄2 1⁄2 provided by a trusted source.
Approach	Here, Ý 1⁄2 labels a positive example—in our application, the observed success of a controller applied when the dynamic figure is in state Ü — while Ý   1⁄2 labels a negative example—the failure of the controller applied to state Ü .
Approach	The set of observations Ü Ý is called the training set.
Approach	The SVM is a machine whose task is to learn the mapping Ü Ý from a training set.
Approach	The SVM is defined by functional mappings of the form Ü  ́ Ü « μ , where « are parameters.
Approach	A particular choice of « generates a “trained” SVM.
Approach	In a trained SVM, the sign of the decision function  ́ Ü μ represents the class assigned to a test data point Ü .
Approach	In our application, a properly trained SVM predicts if a controller will succeed (  ́ Ü μ 1⁄4 ) or fail (  ́ Ü μ 1⁄4 ) on a given state Ü of the dynamic character.
Approach	How does one train an SVM?
Approach	In the simplest case of a linear SVM with separable training data, there exists a decision boundary separating positive from negative examples which takes the form of a “separating hyperplane” in .
Approach	The SVM training algorithm computes the separating hyperplane with the largest margin · ·   , where · (   ) is the shortest distance from the separating hyperplane to the closest positive (negative) example.
Approach	SVM training requires the solution of a quadratic programming optimization problem involving a Lagrange multiplier « for every datapoint in the training set.
Approach	Those datapoints in the solution with corresponding « 1⁄4 are called support vectors.
Approach	The support vectors are critical elements of the training set.
Approach	They lie closest to the separating hyperplane.
Approach	If other observations in the training set are moved (subject to certain restrictions) or removed and SVM training is repeated, the same separating hyperplane will result.
Approach	To use a trained SVM, we simply determine on which side of the decision boundary a given test data point Ü lies and assign the corresponding class label to that point.
Approach	The linear SVM is easily generalized to nonseparable training data.
Approach	Furthermore, it is straightforward to generalize the theory to encompass nonlinear SVMs for which the decision boundaries are no longer hyperplanes (i.e., the decision function are no longer linear functions of the data).
Approach	The trick, in principle, is to map the data to some higher (possibly infinite) dimensional space in which the linear theory can be applied.
Approach	This is easily done by introducing kernel functions Ã  ́ Ü Ü μ , such as the polynomial kernel Ã (RBF)  ́ Ü Ý kernel μ  ́ Ü Ã ¡  ́ Ü Ý · Ý μ 1⁄2μ Ô , ÜÔ ́ or the   Ü Gaussian   Ý 3⁄4 3⁄4 or 3⁄4 μ radial .
Approach	To apply the SVM technique to the problem of determining controller pre-conditions, we train a nonlinear SVM classifier to predict the success or failure of a controller for an arbitrary starting state.
Approach	Thus, the trained SVM demarcates the boundary of regions in the figure’s state space wherein the controller can successfully do its job.
Approach	Training sets comprising examples Ü Ý are generated by repeatedly starting the dynamic figure at a stochasticallygenerated initial state Ü , numerically simulating the dynamics of the figure under the influence of the controller in question, and setting Ý ·1⁄2 if the controller succeeds or Ý   1⁄2 if it fails.
Approach	The distribution of the stochastically-generated initial states is of some importance.
Approach	The sample points should ideally be located close to the boundaries which demarcate the acceptable precondition region of state-space.
Approach	However, these boundaries are in fact the unknowns we wish to determine and thus we must resort to a more uniform sampling strategy.
Approach	Unfortunately, the high dimensionality of the state-space precludes regular sampling.
Approach	A shortduration simulation (typically 0.3s) is then carried out from this initial state while a randomized perturbation process is executed.
Approach	This currently consists of applying an external force of random (but bounded) magnitude and random direction to the center-of-mass of the pelvis.
Approach	Simultaneously, the character’s joints are perturbed in a stochastic fashion by setting randomized offset target angles for the joints and using the character’s PD joint controllers to drive the joints towards these perturbed positions.
Approach	While the perturbation strategy is admittedly ad-hoc, we have found it to be effective in sampling the pre-condition space, as is validated by the online use of the learned pre-condition models.
Approach	We employ T. Joachims’ SVM Ð Ø software which is available on the WWW [ 21 ].
Approach	The software can accommodate large training sets comprising tens of thousands of observations and it efficiently handles many thousands of support vectors.
Approach	It includes standard kernel functions and permits the definition of new ones.
Approach	It incorporates a fast training algorithm which proceeds by solving a sequence of optimization problems lower-bounding the solution using a form of local search.
Approach	It includes two efficient estimation methods for error rate and precision/recall.
Approach	The SVM training phase can take hours in our application, but this is done off-line.
Approach	For example, on a 733 MHz PIII computer, the SVM training time for a training set of 8,013 observations is 2,789 seconds using the polynomial kernel, 2,109 seconds using the linear kernel, and 211 seconds using the radial kernel.
Approach	For a training set of 11,020 observations, the training time is 8,676 seconds using the polynomial kernel, 3,593 seconds using the linear kernel, and 486 seconds using the radial kernel.
Approach	Once trained, the SVM classifier can provide answers on-line in milliseconds.
Approach	We compared the performance of the SVM algorithm to that of a nearest neighbor (NN) classifier [ 9 ].
Outcome	Given a training set, the nearest neighbor classifier returns for an arbitrary state Ü the same succeed/fail label as the label for that observation in the training set that is closest to Ü .
Outcome	NN classifiers should perform particularly well in cases where the feasible area in the state space is highly fragmented and localized.
Outcome	Note that the NN method requires zero training time, but that it provides an answer in Ç  ́ Ò μ time where Ò is size of the training set.
Approach	To compute accuracy rates, we trained the SVM and NN pre-condition learning algorithms using randomly sampled observations collected from each of the controllers.
Approach	Then we generated test sets of novel observations and compared their true success/fail status against that predicted by the trained NN and SVM pre-conditions to obtain the accuracy percentages listed in the rightmost two columns of the table.
Outcome	The results show that the SVM algorithm consistently outperforms the NN classifier.
Approach	For the results shown in the table, the SVM algorithm employed polynomial kernel functions.
Outcome	We ran a similar set of experiments using Gaussian RBF kernel functions, but the accuracies were consistently lower than those obtained with polynomial kernel functions.
Approach	Our control composition framework is implemented within DANCE , a portable, extensible object-oriented modeling and animation system [ 24 ].
Background	2 DANCE provides a platform that researchers can use to implement animation and control techniques with minimal design and implementation overhead.
Background	The core of the system supports four base classes, Systems, Simulators, Actuators and Geometries which are loadable as plug-ins in accordance with simple APIs.
Background	Articulated objects are a System subclass that support skeleton hierarchies.
Background	They have kinematic properties and, usually, fully dynamic physical properties as well.
Approach	Our virtual actors, which will be described shortly, are dynamic articulated objects implemented as Systems within DANCE .
Approach	An actuator is a generic concept that includes anything that can exert forces or, in general, interact in any way with systems or other actuators.
Approach	For example, gravity, the ground, the collision mechanism, the supervisor controller and individual controllers are implemented as actuators.
Background	DANCE places no restrictions on the complexity of the controllers.
Background	Simulators compute the equations of motion of all the dynamic characters and other systems in DANCE .
Background	DANCE offers built in support for SD/FAST, a commercial system which produces optimized simulation code [ 18 ].
Approach	Our simulators are automatically produced by SD/FAST from description files.
Approach	They use Kane’s method for computing articulated dynamics and a fourth order explicit Runge-Kutta time integrator for numerically simulating the motions.
Approach	Actuators and simulators are implemented as DANCE plug-ins.
Approach	This allows the user to dynamically load controllers and simulators at runtime.
Approach	In addition, researchers can exchange, simulators, and controllers in the form of dynamically linked pieces of code.
Approach	Object collisions (including self collisions) are handled by the Collision actuator.
Approach	This actuator works on pairs of objects.
Approach	The DANCE API allows it to work with objects that have different simulators.
Approach	Collision detection is based on a library that uses oriented bounding boxes [ 13 ].
Approach	Collision resolution uses a penalty method that corrects geometry interpenetration using spring-and-damper forces.
Approach	As with all penalty methods, it can make the system stiff, but it has performed well in our experiments to date.
Approach	The red arrows indicate the joint positions and axes of rotational degrees of freedom (DOFs) which are also presented in the table.
Approach	The 3D skeleton model has 37 DOFs, six of which correspond to the global translation and rotation parameters.
Approach	The dynamic properties of both models, such as mass and moments of inertia, are taken from the biomechanics literature and correspond to a fullyfleshed adult male.
Approach	The models are equipped with natural limits both on the motion of the joints and the strength of their muscles.
Approach	However, DANCE has no built in muscle model and does not enforce the limits automatically.
Approach	Users can implement the model they prefer and include code to enforce the limits of the model.
Approach	Our plug-in control scheme uses rotational spring-and-damper forces for control and enforces the limits on the joints with exponential springs.
Approach	Most of the controllers for our virtual stuntperson are based on pose control, which has often been used both for articulated objects [ 31 ] and soft objects [ 11 ].
Background	Pose control is based on cyclic or acyclic finite state machines with time transitions between the states.
Background	Each state of the controller can be static or depend on feedback parameters.
Approach	For some of our controllers, we use continuous control, in the sense that the control parameters are tightly coupled with some of the feedback sensors.
Approach	The balance controllers are an example of this.
Approach	We designed several controllers based in part on experimental studies of how humans detect loss of balance [ 25 ] and analysis of protective and falling behaviors [ 8 ].
Approach	The resulting parameterized controllers have been enhanced with appropriate pre-conditions, post-conditions, and expected performance and have been integrated using an arbitration-based supervising controller.
Approach	Each controller has full access to the internal data structures of DANCE including all the information associated with any character or object in the system.
Approach	This allows the controllers to define arbitrary sensors that keep track of necessary information such as state parameters for feedback loops and the state of the environment.
Approach	For efficiency, the supervisor controller calculates a number of common sensor values that are available to all the controllers.
Approach	Many controller transitions in the control framework happen autonomously, such as taking a protective step in response to losing balance.
Approach	However, other actions are initiated in a voluntary fashion.
Approach	For example, a standing character can do any of (1) remain standing using the balance controller, (2) sit-down, (3) walk, and (4) dive.
Approach	Currently, the user directs these voluntary motions by interactively entering command strings to the supervisor controller which, in turn, directly increases the suitability score of the designated controller and forces the arbitration process to be invoked to select a new active controller.
Approach	The control of voluntary motions could equivalently be delegated to a high-level planner, although this kind of planning is beyond the scope of our work at present.
Outcome	At the heart of our prototype system is a composite controller that is capable of handling a large number of everyday tasks, such as walking, balancing, bending, falling, and sitting.
Approach	In addition, we present brief descriptions of the controllers involved in producing several stunt actions.
Outcome	While the given controller descriptions are for the 3D character, the equivalent 2D controllers are very similar.
Approach	We began our implementation with the simple tasks of standing, recovering balance when pushed, and falling.
Approach	An autonomous human agent should be able to balance, standing naturally in place.
Approach	Should loss of balance occur, the character ought to react naturally either with a restoring motion or with a protective falling behavior depending on which action is appropriate in each case.
Outcome	Affording a dynamic articulated figure with natural reactions to loss of balance or impending falls is an essential step towards believable autonomous characters.
Approach	A balance controller is responsible for maintaining a natural standing posture.
Approach	This controller is based on an inverted pendulum model [ 12 ], using the ankles to control the body sway.
Outcome	Despite the fact that the body of the character is not as rigid as the inverted pendulum hypothesis suggests, the approximation works well in practice.
Approach	An animated character should attempt to maintain balance in response to external disturbances by shifting its weight, taking a step or bending at the waist.
Approach	If the character cannot maintain balance, it must then resort to a falling behavior.
Background	The manner in which people fall depends on a number of factors such as their physique, their age and their training.
Background	For example, the work in [ 19 ] shows that, during a fall, the elderly are more likely to impact their hip first as compared to younger adults falling under the same conditions.
Approach	Our fall controller is designed with the average adult in mind.
Approach	Its main action is thus to absorb the shock of the impact using mostly the hands.
Approach	The pre-conditions of the fall controller are defined in accordance with those of the balance controller.
Approach	Situations that are beyond the capabilities of the latter should be handled by the fall controller.
Outcome	Our implementation of the fall controller can handle falls in any direction, responding in different ways to falls in different directions.
Background	Sitting down in a chair and rising from a chair are common everyday tasks.
Approach	We have implemented a controller that can do both depending on the instructions of the animator.
Approach	Apart from the command string supplied by the user, the pre-conditions are either a balanced upright posture or a balanced sitting posture.
Approach	The postconditions are similarly defined.
Background	Getting up off the ground is a surprisingly difficult motion to simulate.
Background	It involves rapid changes of the contact points and significant shifting of the figure’s weight.
Background	In addition, the frictional properties of the ground model can influence the motion.
Approach	The pre-conditions for this controller are straightforward.
Approach	The character must be lying with its back flat on the ground, within some tolerance.
Approach	The post-conditions are that the character should be on its feet with its center of mass within the support polygon.
Approach	Then it would be up to another controller to take over and bring the character from a crouching position to a standing one.
Background	When lying on their back, some people may choose to roll-over to a prone position before attempting to stand.
Approach	We have implemented a roll-over controller that can emulate this action.
Approach	The pre-conditions of the roll-over controller require a supine posture, and no movement of the center of mass.
Approach	The postconditions of the roll controller are fairly simple and they include any prone position for which the character is extended and fairly straight; i.e., no crossing of legs or arms, etc.
Approach	When lying face-down, the pre-conditions can be fairly relaxed.
Approach	Our controller assumes that is has the time to change the state of the character to one from which it knows how to rise.
Approach	As long as the figure is not lying on its arms and the ground is relatively flat it will attempt to get up.
Approach	The post-conditions are chosen such that they satisfy the pre-conditions of the balance controller.
Approach	Apart from everyday actions, we want our dynamic character to be able to do a variety of other voluntary actions dictated by the animator.
Approach	Such actions can potentially include vigorous and/or physically dangerous actions.
Challenge	It is our hope that if a large number of researchers contribute controllers the character can eventually be used as a virtual stuntperson.
Background	The kip is an athletic motion often seen in martial arts films and is depicted in Fig. 9 .
Approach	The controller is based on a pose controller whose pre-conditions include a variation of supine positions.
Approach	As before, the first part of the controller makes sure that the character assumes a position suitable for performing the kip.
Approach	The larger part of the motion is ballistic, which focuses the control mainly at the kick off and landing phases.
Approach	The last part of the controller applies continuous control to bring the stuntman to an erect position from which the balance controller can take over.
Approach	The character can be instructed to lunge forward and upward at a takeoff angle controlled by the user.
Approach	When the hands contact the ground a front-roll is attempted.
Approach	The pre-conditions of this controller are defined be an upright position and little movement of the center of mass.
Approach	We have also experimented with a multiple character scenario, with one character tackling another, Fig. 11 .
Approach	While the timing of the tackle is scripted, it illustrates the capability of the system to cope with a pair of interacting characters, each equipped with its own supervisory controller.
Outcome	We have produced two relatively long animation sequences that demonstrate the potential of the our framework.
Approach	The sequence for the 3D skeleton model presented in Fig. 1 involves controllers whose pre-conditions are provided analytically by the designer.
Approach	Such conditions tend to define square regions within the space defined by the parameters involved.
Outcome	Despite their simple form, such pre-conditions can generally work well as is demonstrated by the intricacy of the animation produced.
FutureWork	We expect to investigate the application of SVM-learned pre-conditions to the 3D model in the future.
Approach	A second animation sequence with the 2D terminator model (see Fig. 12 ) makes use of a set of controllers having a mix of analytic and learned pre-conditions.
Approach	The sequence of controllers that generated the animation was: balance sit lean-forward rise balance walk step-to-stand balance dive default kneel kneel to stand balance step-forward step-tostand balance step-back step-to-stand balance fall default.
Approach	The analytical pre-conditions prune large parts of the state space and the svm-classifier provides a more accurate success/failure prediction within the remaining region.
Outcome	During the animation sequence, the svm-classifier correctly refined the analytical answer in several cases.
Approach	Most of the computational burden in our approach lies in the numerical simulation of the equations of motion.
Outcome	The computations associated with the controllers and our composition framework are negligible in comparison.
Outcome	In general, the 2D model simulates in real time, while the 3D model runs between 5 and 9 times slower than real time on a 733 MHz Pentium III system.
Challenge	The challenges of physics-based controller design plus the technical obstacles that researchers face when attempting to share their algorithms has hindered progress in the important area of physicsbased character animation.
Outcome	This paper has presented a methodology for ameliorating the problem with a framework which facilitates the exchange and composition of controllers.
Outcome	Our framework has been implemented within a freely available system for modeling and animating articulated characters.
Outcome	To our knowledge, our system is the first to demonstrate a dynamic anthoropomorphic character with controlled reactions to disturbances or falls in any direction, as well as the ability to pick itself up off the ground in several ways, among other controlled motions.
Outcome	We hope that our system will foster collective efforts among numerous practitioners that will eventually result in complex composite controllers capable of synthesizing a full spectrum of human-like motor behaviors.
FutureWork	Given the enormous challenge of building controllers capable of large repertoires of dynamic human-like motion, it is inevitable that the work presented in this paper is incomplete in many ways.
FutureWork	Published control methods for 3D walking, running, and stair climbing make obvious candidates for integration into our system.
FutureWork	Coping with variable terrain and dynamic environments are dimensions of added complexity that should provide work for years to come.
FutureWork	Automatic parameterization of controllers to variations in character dimensions and mass is a necessary step for having solutions adaptable to a variety of characters.
FutureWork	Deriving controllers from motion-capture data is an exciting but difficult prospect, although some progress is already being made in this area.
FutureWork	Other methods of “teaching” skills to a dynamic character also warrant investigation.
FutureWork	Finally, intelligently integrating controllers which affect only subsets of DOFs needs to be addressed in order to allow for the parallel execution of controllers.
Approach	The articulated body must be in a balanced upright position, the velocity and acceleration of the center of mass should not exceed certain threshold values as explained in [ 25 ], and both feet must maintain contact with the ground at all times.
Approach	The controller can tolerate small perturbations of the posture and the velocity/acceleration of the center of mass by stiffening the ankle joints.
Approach	For larger accelerations of the center of mass, the controller actively actuates the ankle joint to reduce the acceleration of the center of mass.
Approach	The post-conditions are similar to the pre-conditions.

Approach	We use a bar-network (bar-net) as a deforming mechanism.
Outcome	This technique can be used similarly to a conventional skinning tool, but can also make a skin surface behave in a physically plausible manner due to the inherent physical properties of the network.
Background	A bar-net is a structure commonly used in structural engineering.
Background	Its shape depends on the structural and material properties and the forces acting upon it.
Challenge	Computing the rest shape of an arbitrary bar-net is a time-consuming non-linear problem.
Approach	In order to speed up the computation and also for such a bar-net to be used intuitively to help computer animation, we have defined a set of properties that a desirable bar-net should satisfy.
Approach	This allows a bar-net shape finding problem to be solved using linear equations.
Approach	We adopt a two-layer structure for the representation of a skin surface, including a coarse mesh and a fine mesh.
Approach	To deform a skin surface, we couple a bar-net to its coarse mesh, which in turn deforms the fine mesh when the coupled bar-net is deformed.
Approach	The fine surface mesh can be of different forms, including Nurbs, subdivision surfaces and polygons.
Background	Skin deformation resulting from the movement of characters, such as humans and animals, is one of the most interesting and challenging topics in computer animation.
Challenge	The modelling and deformation of such characters are inevitably complicated and timeconsuming, because of their structural complexity.
Challenge	While realism is important, other factors such as intuitiveness, ease of interaction and computational cost are also of great importance in animation production.
Challenge	Often a compromise among these factors has to be reached.
Background	There are two general categories of methods in animation practice: simulation and authoring.
Background	Simulation 1–3 refers to the use of a mathematical model to automatically recreate the physical reality on computers.
Background	A simulation method lets the animator easily create certain effect which could be otherwise almost an impossible mission with manual manipulations.
Background	However, the disadvantage is also obvious.
Background	Although the simulation techniques provide some parameters for the animator to control the animation, the connection between these parameters and the result is often implicit.
Background	It is usually difficult for an artist to understand the exact physical meaning of these parameters and connect them to the final outcome.
Background	Second, such methods are often computationally expensive.
Background	The authoring methods refer to those that the animator can use to manipulate the modelling or deformation directly.
Background	The animator is able to see the result immediately and has a full control over the deformed shape of the character in question.
Background	Both types of methods were around for a long time.
Background	However, animators turn to favour those tools that they feel they have a control and can evaluate the results directly.
Outcome	In this paper we present a new skinning technique for the deformation of computer-animated characters.
Outcome	A key advantage is that it combines the strengths of both prevalent categories discussed above.
Outcome	This technique is based on a physically inspired deformation model from structural engineering, known as the bar-networks (bar-nets) and can therefore deform realistically based on the physical properties leading to physically plausible outcomes.
Outcome	Meanwhile, instead of letting the mechanical model taking its full course, the animator is able to operate it as a physically based authoring tool in the same way as other conventional deformation tools.
Outcome	We call this technique the bar-net driven deformations.
Background	In animation practice, either for the film industry or games design, it is quite often for each character model to have two layers of mesh, a rough mesh (low resolution) and a fine mesh (high resolution).
Background	The high-resolution (high-res) mesh may take various forms, such as Nurbs, subdivision surfaces and polygon meshes.
Background	The detailed skin shapes including skin deformations, wrinkle, squama and feather are created on this layer.
Background	Because the mesh is very dense involving fine detail, it is inefficient to animate directly on this layer.
Background	The low-resolution (low-res) mesh thus works as an efficient intermediate layer for the modelling and deformation of the fine skin layer.
Approach	Our technique adopts this two-layered strategy.
Approach	The animator creates almost all skin deformation effects on the low-res layer.
Approach	In order to take advantage of the physics, we couple an aforementioned mechanical bar-net with the low-res mesh in areas where deformations are expected to occur.
Approach	This low-res layer gets deformed physically and in turn deforms the high-res mesh of the character’s skin model.
Approach	In order for a mechanical network to be useful in skin deformation for animated characters, we devise a set of properties for the network to satisfy.
Approach	These properties allow the behaviours of skin and anatomic tissues, e.g. muscle groups, to be mimicked intuitively in computer animation and to be computed rapidly.
Background	What needs pointing out is although there is a similarity between a bar-net and a mass–spring model, they have substantially different behaviours.
Background	A mass–spring model will not be able to satisfy the properties we define here.
Background	Most of the techniques on character deformation can be roughly categorized into two groups: authoring and simulation methods, although the boundary between them does not always seem clear.
Background	The technique of Free Form Deformations (FFDs) first introduced by Sederberg and Parry 4 remains popular and has been adopted by many animation software packages due to its simplicity and modelling speed.
Background	FFDs were later extended by several other researchers.
Background	5–8 All these techniques are purely geometric in nature and make no attempt to simulate the physical properties or behaviours of a character.
Background	Based on the FFDs, two very popular deformation tools were developed in Maya, the Lattice and Wrap deformers.
Background	An intuitive attempt to deform a character was involving a skeleton into skin deformation.
Background	This approach has a long history and it treats the skin as a shell that moves by an explicit function of the skeleton.
Background	Vertices of the skin are deformed by a weighted combination of the joint transformations of the character’s skeleton.
Background	9–13 Collectively, such methods are known as the smooth skinning.
Background	They are easy to understand and intuitive to use.
Background	A tedious part is the proper assignment of the weights.
Background	In production, the weights are painted by the animator, and thus the animator has full control over the outcomes.
Background	The smooth skinning approach suffers from some notorious drawbacks, called the candy wrapper effect or collapsing elbow effect, due to its lack of consideration of volume preservation for the soft tissues.
Background	The example-based methods were developed as an alternative in order to overcome this kind of problems 14–17 and have had some success.
Background	With this method, which is called Blend Shape in animation production, the animator can control the exact appearance of the character.
Background	In facial animation, for example, the animator often needs to dictate how a facial model deforms to achieve different expressions.
Background	On the downside, however, a large number of models have to be made in the pose space and stored for shape interpolation.
Background	This is an expensive process.
Background	The drive for realism in computer graphics has lead to some new modelling and deformation techniques.
Background	A group of techniques that have gained increasing popularity in the computer animation of characters are those based on characters anatomy.
Background	These models attempt to mimic their real life counterparts by reproducing their anatomical structures.
Background	These anatomy-based skinning methods differ on the complexity of the models and their behaviours of the underlying anatomical structures.
Background	Some use simple muscle shapes, such as abstract muscle operators, 18 meatballs, 19 some employ detailed models.
Background	20–23 The obvious advantage of this group of methods is its ability in achieving detailed visual quality during animation.
Background	One of the difficulties of these techniques, 24–28 however, is that they are indirect to use, as one has to model the anatomical structures before its appearance arrives.
Background	Achieving a particular look of the skin requires the determination of the shape, number and the layout of the muscles underneath.
Background	Until the skin mesh envelops the underlying structure, it is very hard to anticipate how the character looks like from the outside.
Background	To retain the advantage of the anatomy-based technique without losing intuitiveness, recent research has looked into the issue of estimating the muscles from the skin shape.
Background	29–30 This new technique has had a degree of success.
Background	The current limitations are that they could only use simple muscle shapes, which are sufficient in obtaining detailed deformations.
Background	Anatomy based multi-layered models have significantly improved the realism of the modelling of complex living creatures.
Challenge	Character animation based on the deformation of underlying anatomical structures, such as muscles or fat, is a very complicated process.
Challenge	Issues like mechanical forces, material properties and collision among anatomic structures all need to be properly addressed.
Challenge	The computational cost is inevitably excessive.
Challenge	Such computational costs place severe restrictions on many applications.
Challenge	Our bar-net driven skinning method endeavours to take advantage of the anatomy-based approach, the smooth skinning approach and the physically based approach.
Approach	It follows the current animation workflow, except that a bar-net is coupled with the low-res skin layer.
Approach	When the low-res mesh (or a part of it) is coupled with a bar-net, the couple mesh is called the control mesh in this paper.
Approach	Bar-nets deform according to both the external forces it is subject to and the stiffness properties of the network.
Approach	By controlling these two factors, the animator can easily create the various skin deformation effects including muscle bulge, wrinkles and creases easily.
Approach	The control mesh is bound to the character skeleton in the same way as the traditional smooth skinning method.
Approach	The bar-nets work like a deformer (a term used in many animation packages, e.g. Maya) to change the shape of the skin surface.
Approach	It is compatible with all the other deformation tools incorporated in current animation software.
Approach	They can accumulatively deform the skin shape in a certain order which can be easily changed by the animator on the fly.
Approach	The fine mesh, either in the form of Nurbs, subdivision surfaces or polygons, is deformed by the control mesh using the wrapping deformation method 8 which is available with many commercial animation packages.
Approach	Further detailed deformations including wrinkle can also be added on either by manipulating the skin surface directly or by coupling a bar-net with the fine mesh using the same mechanism.
Approach	A bar-net connects n s points, P i , in three-dimensional space with straight-line segments, called bars.
Approach	These points on the net are known as nodes.
Approach	The nodes can be either fixed or free.
Approach	Fixed nodes will not have their positions changed regardless of whether they are subjected to external forces.
Approach	Free nodes can be moved to balance the acting forces on the net.
Approach	Each bar connects two nodes.
Approach	These bars can be stretched and squashed resulting from the positioning of the end nodes, but they cannot be bent.
Approach	The network described above is in fact a graph with links connecting pairs of nodes.
Approach	A matrix C s , called the branch–node matrix can be formed, which represents in a tabular form the graph of the network.
Approach	Assuming that there are n free nodes and n f fixed nodes, the branch–node matrix can be further subdivided into two sub-matrices, C and C f , by grouping the free-node columns and fixed-node columns of the original matrix, respectively.
Approach	These matrices are used in computing the rest shape of a bar-net.
Approach	A deformable part of the low-res mesh of a character can be considered as a bar-net.
Approach	This analogy establishes a natural link between a mechanical bar-net and a surface patch.
Approach	If a bar-net is coupled with a surface, the surface can be made to behave like a piece of elastic material.
Approach	Thus many numerical methods developed in structural and mechanical engineering for the manipulation of structures and networks can be applied to control the deformation of the surfaces.
Approach	Deforming the bar-net deforms the coupled surface, hence the name bar-net driven deformation.
Approach	Bar-nets can have any arbitrary topology.
Approach	They are not restricted to a quadrilateral topology unlike most curved surface patches.
Background	Quadrilateral patches are the easiest to control and there have been many algorithms developed to implement them.
Background	But methods for controlling the deformation of a non-quadrilateral surface patch analytically remain an interesting research topic.
Background	Such a problem could be resolved by coupling a general mechanical bar-net with the control points of a surface patch of the same topology.
Approach	The principle idea of the proposed bar-net driven deformation technique is to regard the deformable area of an animated creature as a network, which deforms under an acting force.
Approach	The final shape of the surface represents the rest shape of the network and is the result of the balance of all external and internal forces.
Approach	One does not need to worry about the shape of the network itself.
Approach	This because we use a bar-net only as a control mechanism.
Approach	Changing the stiffness with other parameters unchanged has an influence on the whole network.
Approach	This is in line with the physical property of human tissues and therefore makes physical sense.
Approach	The x, y, z components of the external loads applied to the free nodes (non-fixed nodes) have independent influences on the deformation.
Approach	The x component of the displacement is only determined by the x component of the applied force, and similarly for the y and z components.
Approach	So when the animator wishes to finetune the effects on the x, y or z direction separately, the surface will deform as expected.
Approach	The deformation of the free nodes satisfies the superposition principle.
Approach	In other words, if one free node is subject to the influence of a number of forces simultaneously, the general deformation applied to the node is the same as the sum of all the deformations generated by applying these loads independently.
Approach	The benefit from this property is that several muscles, bones, fat tissues can affect the skin deformation simultaneously through summing up of their individual forces.
Background	Network form finding is always a numerically complicated problem in mechanical and structural engineering.
Background	Various numerical methods exist.
Background	As far as most mechanical networks are concerned, the relationship between the equilibrium state and the acting forces is non-linear.
Background	Shape change cannot be trivially related to the magnitude and direction of the external forces.
Background	Often numerical algorithms are deployed to determine the rest form of a network, which is inevitably time-consuming and not very useful for animation production.
Approach	In our case, the effect of stiffness of a network can be approximated by the quantity of force-length ratios of all the bars.
Background	Some researchers call this quantity the force density.
Approach	Using this stiffness parameter, we found the force density method 31 satisfies the above-defined properties.
Approach	Using a bar-net together with the force density form finding method, the prevailing advantages of this technique are its speed of computation and intuitiveness in shape control.
Approach	Coupling bar-nets with a skin surface makes it ‘mechanically deformable’.
Background	Skin deformation can happen around the joints where its surface bends and also in places where the underlying anatomic structure, such as muscles, pushes and pulling the skin surface.
Approach	Using above defined bar-net properties, deformations are achieved by applying virtual forces to the appropriate free nodes of the control mesh.
Approach	We use the force density parameter (equivalent to stiffness) and external forces to control the deformation.
Approach	Multiple bars can be grouped together to simulate the effect of muscle groups.
Approach	The user can manipulate the force on each node to tune the deformation interactively.
Approach	One can also change the force densities to make the network firmer or softer.
Approach	Forces are applied to only eight nodes of the bar-net.
Approach	In this example, the force densities are kept unchanged.
Approach	The gradually changed forces on the control mesh are bound to the elbow rotation angle, which produce both the bulge effect and compensate for the volume loss that the traditional smooth skinning method suffers.
Background	Deformed muscles always change the shape of the skin surface.
Approach	Using the model of a human arm, we illustrate how to generate the muscle effect with bar-nets.
Background	There are approximately 50 muscles in a human upper limb, most of which are large and complex.
Background	Muscles usually act in groups, some muscles act to move the joint, some to support the movement by avoiding unwanted secondary movements.
Background	The combination of these actions causes the muscles, hence the arm, to deform.
Approach	In the animated arm model, we are only concerned with the muscles producing major influence on the skin.
Approach	The deformation of the forearm is complex but relatively unnoticeable.
Approach	Therefore, in this case, only the deformations caused by biceps brachii and triceps brachii are generated.
Approach	When the arm flexes, the biceps brachii contracts and bulges.
Approach	At the same time, the triceps brachii relaxes to allow this action.
Approach	The opposite occurs during the extension of the forearm.
Approach	The biceps brachii and triceps brachii are positioned on opposite sides of the upper arm.
Approach	Accordingly only the nodes lying around the central line of the two muscles are set to free, all other nodes are fixed.
Approach	All the bars in the network are initially assigned a uniform force density.
Approach	Because of the tendon of the biceps brachii, flexing the arm deforms the biceps brachii in all three directions (x, y and z): it is shortened along the arm due to its contraction and it bulges in the other two directions to maintain its volume.
Approach	To simulate the force of the muscles, we apply some simple loads to the midpoints of the network as shown in Figure 3(b) .
Approach	These loads deform the surface to form a natural muscle bulge, as shown in Figure 3(c) .
Approach	This example demonstrates that the animator can easily shape the characters using the virtual forces as user-handles.
Approach	There are 2891 vertices and 2816 faces in the subdivision model.
Approach	While in the bar-net, there are 12 free nodes which are the only necessary resources involved in the form finding and it involves little computation cost.
Approach	Local deformations can be similarly achieved by changing the force densities.
Approach	For example, reducing the force density of the network increases the size of the bulge effect as shown in Figure 3(d) .
Background	The human shoulder is a typical area where notorious skin deformities occur using a traditional skinning method.
Outcome	Most computer-animated characters are complex both geometrically and topologically.
Outcome	The use of quadrilateral meshes to model the geometry of such characters is frequently inadequate.
Background	Computer-animated characters come in different shapes, e.g. in a form of a human, an animal or a completely imaginary figure.
Background	Branches, holes, non-manifolds and irregularities are possible geometric features of their body forms.
Outcome	Satisfying our designed properties, the network is capable of handling any connectivity (topology).
Outcome	In practice, an animated character can be initially modelled by sketching its basic shape roughly.
Outcome	This rough model is coupled with a bar-net to act as the control mesh of the character.
Outcome	The fine skin surface can be represented in various surface forms.
Outcome	Our implementation includes three major surface modelling forms: Nurbs, subdivision surfaces and polygons.
Outcome	Once the control mesh is deformed, it can deform the fine surface model using the Wrap deformer available in many animation packages.
Outcome	Bar-net driven skinning is applicable also to the modelling of wrinkle, where the bar-net is bound to the fine mesh rather than the rough mesh in order to obtain a detailed look.
Background	Character deformation in computer animation has attracted a great deal of research effort over the last two decades.
Background	The earlier models, despite being cheap, had difficulties in creating realistic character deformations.
Background	With the quest for realism, more physically based and CPU intensive computation models have emerged, notably the multi-layered anatomy-based approach.
Challenge	However, in addition to the computational cost, it is undesirable to require the animator to model many muscles before the skin shape is developed.
Outcome	In this paper, we propose a physically motivated deformation authoring technique, called the bar-net driven skinning.
Outcome	Its main strength lies in the combination of speed, intuitiveness and good realism.
Outcome	Our technique can achieve similar results to those of the anatomy-based techniques, but in an interactive manner.
Outcome	Bar-nets reach their rest shape when the acted forces equilibrate.
Outcome	Changing the forces and/or stiffness leads to a change of their shape.
Outcome	Coupling a part of surface mesh with bar-nets allows the surface deformation to be controlled by manipulating the networks and can take advantage of the physical behaviour inherent to the network.
Approach	In order to allow deformations to be produced quickly and intuitively, we have devised a set of properties that an ‘ideal’ bar-net should satisfy, which make intuitive shape control and fast computation possible.
Outcome	To deform the skin surface of a character, we couple a bar-net with a low-res mesh, called the control mesh, which links with the skin surface.
Outcome	This makes the skin mechanically deformable and achieves realistic deformation outcomes.
Outcome	We provide two types of user-handles associated with a bar-net, the virtual forces applied to the free nodes of a network and the force density values.
Outcome	They can be used individually as an interactive modelling tool or collectively to mimic the muscle forces from a muscle group.
Outcome	We have implemented this technique into prototype program in a form of a plug-in for the Autodesk Maya software ( Figure 7 ).
Outcome	It provides the animator with a new deformer which can be used both as a modelling and an animation tool.
Outcome	The animator can interactively change the fix–free status of each node, define and manipulate the forces on each free node, tune the force densities for selected bars.
Outcome	On the downside, the tools developed so far are still relatively primitive.
Outcome	The user needs to understand the basic principles of the bar-net properties before the technique can be used efficiently.
FutureWork	To remedy this problem we are currently designing higher-level tools with an interactive user interface, which will hide this complexity from the user.
Approach	The equilibrium shape of the net structure is reached when all the forces applied at each node sum up to zero.
Approach	p x ; p y ; p z are the external load vectors.
Approach	It is clear from Equation ( 4 ) that any state of equilibrium of a general network structure can be obtained by the solution of one system of linear equations, which is computationally inexpensive.

