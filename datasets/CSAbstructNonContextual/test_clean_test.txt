objective	In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions.

method	To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy.

background	The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function.

result	We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.

background	This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient.

method	Next, the details of MAC layer protocols and multiplexing schemes needed to efficiently support this new physical layer are discussed.

method	In order to understand the improved user experience, we provide highlights of new QoS, QoE, and SON features associated with the 5G evolution.

method	Subsequently, we make an in-depth survey of underlying novel mm-wave physical layer technologies, encompassing new channel model estimation, directional antenna design, beamforming algorithms, and massive MIMO technologies.

objective	5G wireless systems, with improved data rates, capacity, latency, and QoS are expected to be the panacea of most of the current cellular networks' problems.

method	For alleviating the increased network energy consumption and operating expenditure, we make a detail review on energy awareness and cost efficiency.

method	We also look into the killer applications, considered as the major driving force behind 5G.

method	In this survey, we make an exhaustive review of wireless evolution toward 5G networks.

background	Ever increasing proliferation of smart devices, introduction of new emerging multimedia applications, together with an exponential rise in wireless data (multimedia) demand and usage is already creating a significant burden on existing cellular networks.

method	We first discuss the new architectural changes associated with the radio access network (RAN) design, including air interfaces, smart antennas, cloud and heterogeneous RAN.

background	The vision of next generation 5G wireless communications lies in providing very high data rates (typically of Gbps order), extremely low latency, manifold increase in base station capacity, and significant improvement in users' perceived quality of service (QoS), compared to current 4G LTE networks.

result	This provides valuable insight into why some mobile applications are more successful than others.

objective	In this vein, this paper presents a comparative analysis of six Iranian android games in an app store “Iran Apps” which were top level games in terms of download and rate during a 3-month study.

background	While recent literature has partly advanced our understanding of mobile application business models, less attention has been given to identify the most successful business model especially in the game area.

result	The results reveal that freemium and In-app purchase models were more successful, while advertising model can be efficiently used as a complementary model.

other	© 2015 Elsevier Ltd.

result	In reference to narcissism, there was a positive relationship between using Instagram to be cool and for surveillance.

result	Another interesting finding shows that there is a positive relationship between high levels of social activity (traveling, going to sporting events, visiting friends, etc.) and being motivated to use Instagram as a means of documentation.

result	Theoretical contributions of this study relate to our understanding of uses and gratifications theory.

result	The next significant finding was a positive relationship between those who scored high in interpersonal interaction and using Instagram for coolness, creative purposes, and surveillance.

background	Instagram is the fastest growing social network site globally.

other	”

result	This study uncovers new motives for social media use not identified in previous literature.

objective	This study investigates motives for its use, and its relationship to contextual age and narcissism.

objective	A survey of 239 college students revealed that the main reasons for Instagram use are “Surveillance/Knowledge about others,” “Documentation,” “Coolness,” and “Creativity.

background	This study examines factors influencing preference for e-books as well as reported use of e-book content.

background	Despite the ability to easily access supplemental content through e-books via hyperlinks and other features, students were more likely to use special features in print books than in e-books.

background	Previous research has demonstrated that the experience of reading e-books is not equivalent to reading textbooks.

other	All rights reserved.

background	No significant correlations existed between the number of e-books previously used and overall preference of e-books: Participants who had previously used an e-book still preferred print texts for learning.

background	Although the present student cohort is the most technologically savvy to ever enter universities, students do not prefer e-books over textbooks regardless of their gender, computer use or comfort with computers.

other	2010 Elsevier Ltd.

background	Digital market has never been so unstable due to more and more demanding users and new disruptive competitors.

method	Through a Systematic Literature Review, we found that digital transformation is more than just a technological shift.

method	According to this study, these transformations have had an impact on the business models, the operational processes and the end-users experience.

method	Considering the richness of this topic, we had proposed a research agenda of digital transformation in a managerial perspective.

objective	CEOs from most of industries investigate digitalization opportunities.

background	Highly inspired from natural computing in the brain and recent advances in neurosciences, they derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike firing.

result	Finally, Section 5 discusses application domains, implementation issues and proposes several simulation frameworks.

result	The computational power of SNNs is addressed in Section 3 and the problem of learning in networks of spiking neurons is tackled in Section 4, with insights into the tracks currently explored for solving it.

background	1 Professor at Universit de Lyon Laboratoire de Recherche en Informatique INRIA CNRS bat.

method	Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation.

background	This chapter relates the history of the “spiking neuron” in Section 1 and summarizes the most currently-in-use models of neurons and synaptic plasticity in Section 2.

objective	Today, the main challenge is to discover efficient learning rules that might take advantage of the specific features of SNNs while keeping the nice properties (general-purpose, easy-to-use, available simulators, etc.

background	Spiking Neuron Networks (SNNs) are often referred to as the 3rd generation of neural networks.

background	SNNs overcome the computational power of neural networks made of threshold or sigmoidal units.

other	) of traditional connectionist models.

result	Using ITS4 we found new remotelyexploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software.

method	This method is eficient enough to offer real-time feedback to developers during coding while producing few false negatives.

background	We describe ITS4, a tool for statically scanning security-critical C source code for vulnerabilities.

method	Unlike other techniques, our method is also simple enough to scan C + + code despite the complexities inherent in the language.

other	The ITS4 source distribution is available at h t tp : //www.rstcorp.

background	Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and eficiency.

other	com/its4.

method	Eleven propositions are offered suggesting that organizational trust, pooled interdependence, organizational community, organizational culture, technology adoption, technology utility, technology usability, socialization, sensitivity to privacy, and predisposition to trust influence an individual’s level of trust in the HRIS technology (technology trust) and ultimately the success of an HRIS implementation process.

method	Specifically, organizational, technological, and user factors are considered and modeled to generate a set of testable propositions that can subsequently be investigated in various organizational settings.

objective	This paper generates 11 propositions exploring the relationship between Human Resource Information Systems (HRIS) and the trust an individual places in the inanimate technology (technology trust) and models the effect of those relationships on HRIS implementation success.

background	Scholars in many disciplines have considered the antecedents and consequences of various forms of trust.

result	A summary of the relationships between the key constructs in the model and recommendations for future research are provided.

method	Furthermore, a classification of mentioned techniques into two main fraud detection approaches, namely, misuses (supervised) and anomaly detection (unsupervised) is presented.

objective	In this paper, after investigating difficultiesof credit card fraud detection, we seek to review the state of the art in credit card fraud detection techniques, datasets and evaluation criteria.

background	Many techniques have been proposed to confront thegrowthin credit card fraud.

method	The advantages and disadvantages of fraud detection methods are enumerated and compared.

background	However, all of these techniques have the same goal of avoiding the credit card fraud; each one has its own drawbacks, advantages and characteristics.

background	Although using credit cards provides enormous benefits when used carefully and responsibly,significant credit and financial damagesmay be causedby fraudulent activities.

background	It becomes an unavoidable part of household, business and global activities.

background	Credit card plays a very important rule in today's economy.

method	Again, a classification of techniques is proposed based on capability to process the numerical and categorical datasets.

method	Different datasets used in literatureare then described and grouped into real and synthesized data and the effective and common attributesare extracted for further usage.

other	2008 Elsevier Ltd.

background	To evaluate the proposed algorithm, we use some real and artificial data sets and compare with the results of other algorithms in terms of the adjusted Rand index.

other	All rights reserved.

background	This paper proposes a new algorithm for K-medoids clustering which runs like the K-means algorithm and tests several methods for selecting initial medoids.

method	Experimental results show that the proposed algorithm takes a significantly reduced time in computation with comparable performance against the partitioning around medoids.

background	The proposed algorithm calculates the distance matrix once and uses it for finding new medoids at every iterative step.

method	The mapped vector is then passed through the generative model to predict the missing content.

method	Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space.

method	We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image.

result	The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80% corruption and large blocky corruption.

result	Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods.

objective	In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN).

background	• Updates even more aggressive than AMAF can be even

result	Our results show that: • Random play-outs provide more information about the goodness of moves made earlier in the play-out.

background	• AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out.

background	We present and explore the effectiveness of several variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go.

method	In this paper, we propose a sentence-level attention model to select the valid instances, which makes full use of the supervision information from knowledge bases.

result	We conduct three experiments on a widely used dataset and the experimental results show that our approach outperforms all the baseline systems significantly.

background	Distant supervision for relation extraction is an efficient method to scale relation extraction to very large corpora which contains thousands of relations.

background	However, the existing approaches have flaws on selecting valid instances and lack of background knowledge about the entities.

result	The background knowledge not only provides more information for predicting relations, but also brings better entity representations for the attention module.

method	And we extract entity descriptions from Freebase and Wikipedia pages to supplement background knowledge for our task.

background	Increasing use of the World Wide Web as a B2C commercial tool raises interest in understanding the key issues in building relationships with customers on the Internet.

result	These findings complement the previous findings on e-commerce and shed light on how to establish a trust relationship on the World Wide Web.

objective	This research identifies a number of key factors related to trust in the B2C context and proposes a framework based on a series of underpinning relationships among these factors.

result	Customer’s trust levels are likely to be influenced by the level of perceived market orientation, site quality, technical trustworthiness, and user’s web experience.

objective	The findings in this research suggest that people are more likely to purchase from the web if they perceive a higher degree of trust in e-commerce and have more experience in using the web.

background	Given the differences between a virtual and a conventional marketplace, antecedents and consequences of trust merit re-examination.

method	People with a higher level of perceived site quality seem to have a higher level of perceived market orientation and trustworthiness towards e-commerce.

method	Positive ‘word of mouth’, money back warranty and partnerships with well-known business partners, rank as the top three effective risk reduction tactics.

method	Furthermore, people with a higher level of trust in e-commerce are more likely to participate in e-commerce.

background	Trust is believed to be the key to these relationships.

background	These inferences are based on the premise that the function of a protein may be discovered by studying its interaction with one or more proteins of known functions.

background	Most of a cell’s functional processes involve interactions among proteins, and a key challenge in proteomics is to better understand these complex interaction graphs at a systems level.

background	In this chapter, we describe recent efforts to predict interactions between proteins and between protein domains.

method	Protein-protein interactions directly contribute to protein functions, and implications about functions can often be made via PPI studies.

background	Because of their importance in development and disease, protein-protein interactions (PPIs) have been the subject of intense research in recent years.

method	We also describe methods that attempt to use protein interaction data to infer protein function.

result	The second part of this chapter reviews recent computational approaches to predict protein functions from PPI networks.

background	In addition, a greater understanding of PPIs can be achieved through the detailed investigation of the protein domain interactions which mediate PPIs.

background	Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.

method	We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.

method	Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.

objective	The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.

result	We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.

objective	In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.

background	Causal ordering was first proposed in the ISIS system developed at Cornell University.

method	The implementation of causal ordering proposed in this paper uses logical clocks of Mat te rn-Fidge (which define a partial order between events in a distr ibuted system) and presents two advantages over the implementation in ISIS: (1) the information added to messages to ensure causal ordering is bounded by the number of sites in the system, and (2) no special protocol is needed to dispose of this added information when it has become useless.

method	The interest of causal ordering in a distr ibuted system is that it is cheaper to realize than total ordering.

background	This paper presents a new algorithm to implement causal ordering.

result	The implementation of ISIS presents however advantages in the case of site failures.

method	We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions.

result	These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.

objective	In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed to scale simple OLTP-style application loads over many servers.

method	Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses.

background	While these affordances allow users to keep up-to-date, they also produce a basis for social comparison and envy on an unprecedented scale.

result	From a provider’s perspective, our findings signal that users frequently perceive Facebook as a stressful environment, which may, in the long-run, endanger platform sustainability.

objective	In study 2, the role of envy feelings is examined as a mediator between intensity of passive following on Facebook and users’ life satisfaction.

background	Even though envy may endanger users’ life satisfaction and lead to platform avoidance, no study exists uncovering this dynamics.

background	The wealth of social information presented on Facebook is astounding.

background	In study 1, we explore the scale, scope, and nature of envy incidents triggered by Facebook.

background	To close this gap, we build on responses of 584 Facebook users collected as part of two independent studies.

method	Confirming full mediation, we demonstrate that passive following exacerbates envy feelings, which decrease life satisfaction.

method	In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance.

result	This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years.

background	Speech separation is the task of separating target speech from background interference.

background	A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data.

objective	Over the past decade, many supervised separation algorithms have been put forward.

method	Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features.

result	Traditionally, speech separation is studied as a signal processing problem.

method	We first introduce the background of speech separation and the formulation of supervised separation.

objective	Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement speech-nonspeech separation, speaker separation multitalker separation, and speech dereverberation, as well as multimicrophone techniques.

method	The important issue of generalization, unique to supervised learning, is discussed.

background	The algorithm employs the orientation between a set of descriptors of muscular activities and a nonlinearly mapped version of them.

result	EMG data collected from nine transradial amputees performing six classes of movements with different force levels is used to validate the proposed features.

background	This paper presents a new feature extraction algorithm for the challenging problem of the classification of myoelectric signals for prostheses control.

objective	The proposed idea can be summarized in the following three steps: 1) extract power spectrum moments from the current analysis window and its nonlinearly scaled version in time-domain through Fourier transform relations, 2) compute the orientation between the two sets of moments, and 3) apply data fusion on the resulting orientation features for the current and previous time windows and use the result as the final feature set.

background	It incorporates information about the Electromyogram (EMG) signal power spectrum characteristics derived from each analysis window while correlating that with the descriptors of previous windows for robust activity recognition.

result	When compared to other well-known EMG feature extraction methods, the proposed features produced an improvement of at least 4%.

result	Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.

method	We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author.

method	We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm.

background	Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite.

method	We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task.

background	It is based on the use of linguistic preferences to provide individuals' opinions, and on the use of fuzzy majority of consensus, represented by means of a linguistic quantifier.

background	This paper presents a consensus model in group decision making under linguistic assessments.

method	The consensus degrees indicate how far a group of individuals is from the maximum consensus, and linguistic distances indicate how far each individual is from current consensus labels over the preferences.

method	Several linguistic consensus degrees and linguistic distances are defined, acting on three levels.

result	This consensus model allows to incorporate more human consistency in decision support systems.

background	Mobile business is a young promising industry created by the emergence of wireless data networks.

background	Similar to other emerging industries, it is characterized by a large number of uncertainties at different levels, in particular concerning technology, business strategy and consumer demand.

objective	We argue that successful business models are likely to be the ones that best address the economic peculiarities underlying this industry, like mobility, network effects and natural monopolies.

objective	This paper focuses on the strategic uncertainties, where a large number of actors are trying a number of strategic approaches to position themselves in the most favourable position in the value system.

method	This paper intends to apply a business model analysis methodology in order to better understand the strategic approaches of these actors.

method	Like segmentation, we use the image structure to guide our sampling process.

method	In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition.

method	Like exhaustive search, we aim to capture all possible object locations.

other	The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).

method	We introduce selective search which combines the strength of both an exhaustive search and segmentation.

result	The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition.

method	Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible.

objective	This paper addresses the problem of generating possible object locations for use in object recognition.

result	Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations.

method	"Distributed data processing" and "distributed processing" are two phrases which illustrate that axiom.

background	When they fail to do that, they lead to confusion and misunderstanding.

objective	This paper is an attempt to reverse that trend.

background	Words have only one purpose in a technical context–the transmission of information.

background	Like many other words in the lexicon of the computer professional, these have become cliches through over-use, losing much of their original meaning in the process.

objective	The basic setup consists of two deep networks playing against each other in a zero-sum game setting.

result	Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.

method	In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs.

method	The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature.

objective	However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible.

result	In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games.

background	Generative Adversarial Networks have emerged as an effective technique for estimating data distributions.

background	It has driven animated characters in Hollywood films, and is a standard feature of commercial animation packages.

background	Blendshapes”, a simple linear model of facial expression, is the prevalent approach to realistic facial animation.

background	The blendshape approach originated in industry, and became a subject of academic research relatively recently.

method	We show that, despite the simplicity of the blendshape approach, there remain open problems associated with this fundamental technique.

method	This course describes the published state of the art in this area, covering both literature from the graphics research community, and developments published in industry forums.

method	We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs.

method	In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking.

method	We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.

objective	This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking.

method	Furthermore, the two most important groups – topics from SM and those from KM study were studied respectively to compare their development in order to show the fusion, the separation and other relationship.

objective	By using CiteSpace, this paper mapped important references that lead trends of SMKM development, authors contributing greatly to this field and hot topics of all the related articles.

other	2014 Elsevier Ltd. All rights reserved.

background	However, previous SMKM studies have not been depicted well by combining work of both researchers in social media study and ones in KM (which supports organizational learning) study.

method	The way that SMKM study developed was analyzed according to the visualization of references and topics.

method	With support of social media, organizations may facilitate the knowledge management process within firms (e.g., knowledge sharing), then to encourage employees to promote collaborative learning behaviors from e-learning to social learning.

background	Social media is bringing great challenges and wonderful opportunities for organizational learning.

method	There is a significant trend in the recent studies is increasing number of publications on social media supported knowledge management (SMKM).

method	Finally, hottest trends and topics in these years and recent future were discussed to provide help for future work.

background	While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios.

background	As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls.

method	VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms.

result	We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.

objective	This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent’s belief of environment dynamics.

method	We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces.

background	Scalable and effective exploration remains a key challenge in reinforcement learning (RL).

method	Analyzing the previous performance of the system (training examples) by means of this technique, knowledge is obtained that can be used to decide which is the most appropriate dispatching rule at each moment in time.

method	The issues with this method are that the performance of these rules depends on the state the system is in at each moment and also that no “ideal” single rule exists for all the possible states that the system may be in.

method	To achieve this goal, a scheduling approach that uses machine learning can be used.

background	A common way of dynamically scheduling jobs in a manufacturing system is by implementing dispatching rules.

method	Therefore, it would be interesting to use the most appropriate dispatching rule for each instance.

result	In this paper, a literature review of the main machine learning based scheduling approaches from the last decade is presented.

result	The experiments performed revealed significantly accurate forecasts.

background	In this procedure, each method being combined is associated to a numerical weight that indicates the contribution of the method in the combined forecast.

result	In order to evaluate this solution, we implemented a prototype that uses a MLP network to combine two widespread methods.

method	In this paper, a machine learning technique uses features of the series at hand to define the adequate weights for a pre-defined number of forecasting methods.

method	We present the use of machine learning techniques to define the weights for the linear combination of forecasts.

background	The linear combination of forecasts is a procedure that has improved the forecasting accuracy for different time series.

background	A WSN is a highly dynamic network because nodes die due to severe environmental conditions and battery power depletion.

background	WSNs invariably operate in an unattended mode and in many scenarios it is impossible to replace sensor motes after deployment, therefore a fundamental objective is to optimize the sensor motes' life time.

background	In recent years, WSNs have received tremendous attention in the research community, with applications in battlefields, industrial process monitoring, home automation, and environmental monitoring, to name but a few.

method	The state-of-the-art in operating systems for WSNs has been examined in terms of the OS Architecture, Programming Model, Scheduling, Memory Management and Protection, Communication Protocols, Resource Sharing, Support for Real-Time Applications, and additional features.

background	This paper presents a survey on the current state-of-the-art in Wireless Sensor Network (WSN) Operating Systems (OSs).

background	Furthermore, a WSN is composed of miniaturized motes equipped with scarce resources e.g., limited memory and computational abilities.

result	These features are surveyed for both real-time and non-real-time WSN operating systems.

objective	The purpose of this survey is to highlight major concerns pertaining to OS design in WSNs and to point out strengths and weaknesses of contemporary OSs for WSNs, keeping in mind the requirements of emerging WSN applications.

background	These characteristics of WSNs impose additional challenges on OS design for WSN, and consequently, OS design for WSN deviates from traditional OS design.

method	For the most popular zippers on the Internet, there is a fast attack that does not require any information about the files in the archive; instead, it gets doubly-encrypted plaintext by exploiting a weakness in the pseudorandom-number generator.

background	The deflate algorithm “zippers” now use to compress the plaintext before encryption makes it difficult to get known plaintext.

background	Biham and Kocher demonstrated that the PKZIP stream cipher was weak and presented an attack requiring thirteen bytes of plaintext.

method	We consider the problem of reducing the amount of known plaintext by finding other ways to filter key guesses.

method	In most cases we can reduce the amount of known plaintext from the archived file to two or three bytes, depending on the zipper used and the number of files in the archive.

background	Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users’ characteristics and profile information.

background	The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited.

method	In this paper we show that taking user context into account improves the detection of cyberbullying.

method	A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end.

background	The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness.

method	The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative.

background	They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture.

result	The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.

objective	They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality.

background	An early suggestion for a color stereo computer display involved a rotating filter wheel held in front of the eyes.

result	The image can easily be constructed by computer from any 3D scene or solid object description.<<ETX>>

background	A new, simple, and symmetric algorithm can be implemented that results in higher levels of detail in solid objects than previously possible with autostereograms.

method	In contrast, this article describes a method for viewing on paper or on an ordinary computer screen without special equipment, although it is limited to the display of 3D monochromatic objects.

background	In a stereoscope, an optical instrument similar to binoculars, each eye views a different picture and thereby receives the specific image that would have arisen naturally.

method	(The image can be colored, say, for artistic reasons, but the method we describe does not allow colors to be allocated in a way that corresponds to an arbitrary coloring of the solid object depicted.)

background	Downloading the book in this website lists can give you more advantages.

background	So, this is not only this ways of knowing in hci.

method	However, this book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts.

background	So many books can be found in this website.

method	This is simple, read the soft file of the book and you get it.

background	It will show you the best book collections and completed collections.

background	In the past few years deep neural networks implemented on GPU clusters have become the state of the art in image classification.

background	They provide excellent classification ability at the cost of a more complex data manipulation process.

method	However, we show the proposed system is capable of operating with zero loss in classification accuracy if the memristors utilized are able to store at least 16 unique values (essentially acting as 4-bit devices).

result	To the best of our knowledge, this is the first paper that presents a memristor based circuit for implementing CNN recognition.

background	This paper presents a simulated memristor crossbar implementation of a deep Convolutional Neural Network (CNN).

method	However once these systems are trained, we show that the analog crossbar circuits in this paper can highly parallelize the recognition phase of a CNN algorithm.

method	One of the drawbacks of using memristors to carry out computations is that the data stored will likely have less precision when compared to typical 32-bit floating point memory.

result	This is also the first paper that provides a circuit for precise memristor based analog convolution.

method	When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model, (2) the student DNN outperforms the original DNN, and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.

method	As the DNN performs a mapping from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers.

method	We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN.

background	We propose the use of dimensionality reduction as a defense against evasion attacks on ML classifiers.

result	We empirically evaluate and demonstrate the feasibility of dimensionality reduction of data as a defense mechanism against evasion attacks using multiple real-world datasets.

result	Our key findings are that the defenses are (i) effective against strategic evasion attacks in the literature, increasing the resources required by an adversary for a successful attack by a factor of about two, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification, and human activity classification.

background	We present and investigate a strategy for incorporating dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase.

objective	In this paper we critically review task analysis models and techniques.

result	These approaches to task analysis are discussed in order to develop a richer picture of human activity, while analyzing their limitations, general weaknesses, and possibilities for improvement.

result	We note that the major approaches have focused on very different levels of analysis, and call for greater integration of these different levels in task analysis theory.

method	We consider their ability to determine the appropriate set of atomic actions in a task, their effect on workers’ motivational needs, their support of users’ cognitive and sociocultural processes, and their effectiveness in supporting interface design.

method	We consider many research directions in this survey, namely, user association, interference management, energy efficiency, spectrum sharing, resource management, scheduling, backhauling, propagation modeling, and the economics of UDN deployment.

method	Also, we present the enabling technologies for network densification in order to understand the state-of-the-art.

background	In UDNs, the access nodes and/or the number of communication links per unit area are densified.

background	Ultradense network (UDN) is one of the leading ideas in this racetrack.

background	The exponential growth and availability of data in all forms is the main booster to the continuing evolution in the communications industry.

background	The popularization of traffic-intensive applications including high definition video, 3-D visualization, augmented reality, wearable devices, and cloud computing defines a new era of mobile communications.

background	The immense amount of traffic generated by today's customers requires a paradigm shift in all aspects of mobile networks.

method	Moreover, we summarize and compare some of the recent achievements and research findings.

method	We discuss the modeling techniques and the performance metrics widely used to model problems in UDN.

objective	In this paper, we provide a survey-style introduction to dense small cell networks.

method	Please note that the goal of our LaneNet is built to detect lane line only, which introduces more difficulties on suppressing the false detections on the similar lane marks on the road like arrows and characters.

result	The high running speed and low computational cost endow our LaneNet the capability of being deployed on vehicle-based systems.

background	Lane detection is to detect lanes on the road and provide the accurate location and shape of each lane.

method	Stage one uses a lane edge proposal network for pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based on lane edge proposals.

method	Despite all the difficulties, our lane detection is shown to be robust to both highway and urban road scenarios method without relying on any assumptions on the lane number or the lane line patterns.

background	It severs as one of the key techniques to enable modern assisted and autonomous driving systems.

method	In this paper, we propose a deep neural network based method, named LaneNet, to break down the lane detection into two stages: lane edge proposal and lane line localization.

method	However, several unique properties of lanes challenge the detection methods.

method	The lack of distinctive features makes lane detection algorithms tend to be confused by other objects with similar local appearance.

method	Moreover, the inconsistent number of lanes on a road as well as diverse lane line patterns, e.g. solid, broken, single, double, merging, and splitting lines further hamper the performance.

background	While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison.

background	Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task.

objective	In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models.

result	I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.

background	Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years.

background	As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field.

background	A number of GAN variants have been proposed and have been utilized in many applications.

result	Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and speed.

objective	This paper presents a deep learning-based approach that can handle general high-dimensional parabolic PDEs.

background	Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as “the curse of dimensionality”.

result	This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.

background	To this end, the PDEs are reformulated as a control theory problem and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function.

background	Magnetotelluric method of Earth structure recognition is shortly presented together with the its most popular measurement method called the remote reference method.

result	The practical examples of Kalman filter application to the real 2D and 3D data illustrate the merits of presented technique.

method	The basic theory of nonstationar, discrete Kalman filter and its implementation to multichannel magnetotelluric data recorded in multi-site experiment are also discussed with details.

background	The results of application of multichannel Kalman filtering to reduction of uncorrelated noise in magnetotelluric recordings are discussed in this article.

background	The mutual orthogonality among the received signals is often assumed but cannot be achieved in practice for all Doppler and delay pairs.

objective	We demonstrate the effect of the increase in the correlation among the received signals from different transmitters on the detection performance.

method	Employing the asymptotic statistical characteristics and the numerical performance of the test, we analyze the sensitivity of the MIMO radar with respect to changes in the cross-correlation levels of the measurements.

method	Based on the expectation maximization algorithm, we propose a method to estimate the target, correlation, and noise parameters.

method	We then use the estimates of these parameters to develop a statistical decision test.

method	We introduce a data model considering the correlation among the data from different transmitter-receiver pairs as unknown parameters.

background	We consider the effect of imperfect separability in the received signals on the detection performance of multi-input multi-output (MIMO) radar with widely separated antennas.

method	We also indicate proof-of-concept through the demonstration of a human-robot cooperative manipulation task performed with a PR2 robot.

background	Finally, we analyze the quality of task-level anticipatory knowledge required to improve prediction performance early in the human motion trajectory.

result	The results indicate a considerable improvement over prior techniques in early prediction, achieving 70% or higher correct classification on average for the first third of the trajectory (<; 500msec).

method	We demonstrate the benefits of this approach through offline statistical analysis of human motion data.

background	Interest in human-robot coexistence, in which humans and robots share a common work volume, is increasing in manufacturing environments.

method	Motion-level anticipatory models are constructed using multiple demonstrations of human reaching motions.

background	Efficient work coordination requires both awareness of the human pose and a plan of action for both human and robot agents in order to compute robot motion trajectories that synchronize naturally with human motion.

objective	In this paper, we present a data-driven approach that synthesizes anticipatory knowledge of both human motions and subsequent action steps in order to predict in real-time the intended target of a human performing a reaching motion.

method	We produce a library of motions from human demonstrations, based on a statistical representation of the degrees of freedom of the human arm, using time series analysis, wherein each time step is encoded as a multivariate Gaussian distribution.

method	The present paper have employed two different textual representations, Word2vec and N-gram, for analyzing the public sentiments in tweets.

objective	In this paper, we have applied sentiment analysis and supervised machine learning principles to the tweets extracted from Twitter and analyze the correlation between stock market movements of a company and sentiments in tweets.

objective	Understanding author's opinion from a piece of text is the objective of sentiment analysis.

background	Previous studies have concluded that the aggregate public mood collected from Twitter may well be correlated with Dow Jones Industrial Average Index (DJIA).

background	Especially, Twitter has attracted a lot of attention from researchers for studying the public sentiments.

background	The thesis of this work is to observe how well the changes in stock prices of a company, the rises and falls, are correlated with the public opinions being expressed in tweets about that company.

method	In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase.

background	Now-a-days social media is perfectly representing the public sentiment and opinion about current events.

background	Stock market prediction on the basis of public sentiments expressed on Twitter has been an intriguing field of research.

background	Predicting stock market movements is a well-known problem of interest.

background	In the past Richard Watson was the accepting senior editor for this paper.

result	Thus, effective development and implementation of KMS requires a foundation in several rich

background	Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era.

result	Consistent with the interest in organizational knowledge and knowledge management (KM), IS researchers have begun promoting a class of information systems, referred to as knowledge management systems (KMS).

other	For more details see http://www.misq.org/misreview/announce.html few years, however, there has been a growing interest in treating knowledge as a significant organizational resource.

method	The objective of KMS is to support creation, transfer, and application of knowledge in organizations.

method	MISQ Review articles survey, conceptualize, and synthesize prior MIS research and set directions for future research.

result	Knowledge and knowledge management are complex and multi-faceted concepts.

background	Even the support vector machine (SVM) has been proposed to provide a good generalization performance, the classification result of the practically implemented SVM is often far from the theoretically expected level because their implementations are based on the approximated algorithms due to the high complexity of time and space.

method	Each individual SVM is trained independently using the randomly chosen training samples via a bootstrap technique.

result	Various simulation results for the IRIS data classification and the hand-written digit recognitionshow that the proposed SVM ensembles with bagging outperforms a single SVM in terms of classification accuracy greatly.

method	To improve the limited classification performance of the real SVM, we propose to use the SVM ensembles with bagging (bootstrap aggregating).

method	Then, they are aggregated into to make a collective decision in several ways such as the majority voting, the LSE(least squares estimation)-based weighting, and the double-layer hierarchical combining.

background	Most pseudorandom number generators (PRNGs) scale poorly to massively parallel high-performance computation because they are designed as sequentially dependent state transformations.

method	We introduce several counter-based PRNGs: some based on cryptographic standards (AES, Threefish) and some completely new (Philox).

result	In addition to essentially unlimited parallel scalability, our PRNGs offer excellent single-chip performance: Philox is faster than the CURAND library on a single NVIDIA GPU.

result	All our PRNGs pass rigorous statistical tests (including TestU01's BigCrush) and produce at least 264 unique parallel streams of random numbers, each with period 2128 or more.

background	We demonstrate that independent, keyed transformations of counters produce a large alternative class of PRNGs with excellent statistical properties (long period, no discernable structure or correlation).

method	These counter-based PRNGs are ideally suited to modern multi-core CPUs, GPUs, clusters, and special-purpose hardware because they vectorize and parallelize well, and require little or no memory for state.

background	In recent years, there has been an explosion of interest in mining time series databases.

method	A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times.

background	One of the most commonly used representations is piecewise linear approximation.

result	We introduce a novel algorithm that we empirically show to be superior to all others in the literature.

background	As with most computer science problems, representation of the data is the key to efficient and effective solutions.

result	We show that all these algorithms have fatal flaws from a data mining perspective.

objective	In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques.

method	This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time series data.

background	Simulations show that GSP can conserve energy.

method	The measurements were then used to build an energy consumption model for GSP

objective	The gossip-based sleep protocol (GSP) (X. Hou et al., 2004) is an example of a protocol that implements routing and some MAC functions in an effort to conserve energy.

background	GSP was implemented on the Mica2 platform and measurements were conducted to determine energy consumption.

background	The energy consumption rate for sensors in a wireless sensor network vary greatly depending on the protocols the sensors use for communication.

background	We expand on this effort by building a prototype system and measuring energy consumption rates.

background	The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level.

background	Deep learning uses multiple layers to represent the abstractions of data to build computational models.

objective	This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications.

background	However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective.

background	Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing.

background	The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain.

background	Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth.

objective	It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.

background	We present a novel clustering algorithm for tagging a face dataset (e. g., a personal photo album).

method	Then, the Rank-Order distance of two faces is calculated using their ranking orders.

method	The Rank-Order distance is motivated by an observation that faces of the same person usually share their top neighbors.

method	Using the new distance, a Rank-Order distance based clustering algorithm is designed to iteratively group all faces into a small number of clusters for effective tagging.

result	The core of the algorithm is a new dissimilarity, called Rank-Order distance, which measures the dissimilarity between two faces using their neighboring information in the dataset.

method	Specifically, for each face, we generate a ranking order list by sorting all other faces in the dataset by absolute distance (e. g., L1 or L2 distance between extracted face recognition features).

result	The proposed algorithm outperforms competitive clustering algorithms in term of both precision/recall and efficiency.

background	Common performance criteria are computational complexity, geometric optimality, global optimality, structural degeneracies, and the number of solutions.

background	The ability to handle minimal sets of correspondences, resulting solution multiplicity, and generalized cameras are further desirable properties.

method	This paper presents the first PnP solution that unifies all the above desirable properties within a single algorithm.

background	A large number of absolute pose algorithms have been presented in the literature.

method	We compare our result to state-of-the-art minimal, non-minimal, central, and non-central PnP algorithms, and demonstrate universal applicability, competitive noise resilience, and superior computational efficiency.

result	Our algorithm is called Unified PnP (UPnP).

background	We introduce a novel approach to automatically recover 3D human pose from a single image.

method	Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly.

background	Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose.

result	Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.

method	For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms.

method	In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems.

other	J. Han was supported in part by the research grant NSERC-A3723 from the Natural Sciences and Engineering Research Council of Canada, the research grant NCE:IRIS/Precarn-HMI5 from the Networks of Centres of Excellence of Canada, and research grants from MPR Teltech Ltd. and Hughes Research Laboratories.

objective	In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently.

method	A classi cation of the available data mining techniques is provided and a comparative study of such techniques is presented.

background	Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities.

result	Index Terms | Data mining, knowledge discovery, association rules, classi cation, data clustering, pattern matching algorithms, data generalization and characterization, data cubes, multiple-dimensional databases.

background	Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues.

background	Researchers in many di erent elds have shown great interest in data mining.

method	We develop a hierarchy of these pseudo-complemented algebras that includes Stone algebras.

method	Both theories are combined to prove Chen and Grätzer’s construction theorem for Stone algebras.

method	Independently of this theory we study filters based on partial orders.

result	The latter involves extensive reasoning about algebraic structures in addition to reasoning in algebraic structures.

background	A range of algebras between lattices and Boolean algebras generalise the notion of a complement.

other	(see Figure 1).

background	We could also assign functional or logical labels such as sentences, titles, captions, author names, and addresses to some of these regions.

result	Automatic analysis of an arbitrary document with complex layout is an extremely difficult task and is beyond the capabilities of the state-of-the-art document structure and layout analysis systems.

result	Many documents, such as newspapers, magazines and brochures, contain very complex layout due to the placement of figures, titles, and captions, complex backgrounds, artistic text formatting, etc.

method	Document images are often generated from physical documents by digitization using scanners or digital cameras.

method	The process of document structure and layout analysis tries to decompose a given document image into its component regions and understand their functional roles and relationships.

objective	We will look into each of these steps in detail in the following sections.

result	A human reader uses a variety of additional cues such as context, conventions and information about language/script, along with a complex reasoning process to decipher the contents of a document.

method	The processing is carried out in multiple steps, such as preprocessing, page decomposition, structure understanding, etc.

background	A document image is composed of a variety of physical entities or regions such as text blocks, lines, words, figures, tables, and background.

background	Due to parasitic capacitance coupling between flash cells that are physically close to each other, flash cell programming can lead to cell-to-cell program interference, which introduces errors into neighboring flash cells.

result	We find that a partially-programmed flash cell (i.e., a cell where the second programming step has not yet been performed) is much more vulnerable to cell-to-cell interference and read disturb than a fully-programmed cell.

background	Modern NAND flash memory chips provide high density by storing two bits of data in each flash cell, called a multi-level cell (MLC).

background	When a flash cell is programmed, a high voltage is applied to the cell.

method	We experimentally characterize the effects of two-step programming using contemporary 1X-nm (i.e., 15–19nm) flash memory chips.

result	In this paper, we demonstrate that two-step programming exposes new reliability and security vulnerabilities.

result	Second, it programs the most significant bit to bring the MLC up to its full voltage state.

background	An MLC partitions the threshold voltage range of a flash cell into four voltage states.

method	First, the flash memory partially programs the least significant bit of the MLC to some intermediate threshold voltage.

method	In order to reduce the impact of cell-to-cell interference on the reliability of MLC NAND flash memory, flash manufacturers adopt a two-step programming method, which programs the MLC in two separate steps.

background	In many practical learning scenarios, there is a small amount of labeled data along with a large pool of unlabeled data.

method	The only requirement our co-training strategy places on each supervised learning algorithm is that its hypothesis partitions the example space into a set of equivalence classes (e.g. for a decision tree each leaf deenes an equivalence class).

background	Many supervised learning algorithms have been developed and extensively studied.

method	We present a new \co-training" strategy for using un-labeled data to improve the performance of standard supervised learning algorithms.

method	Unlike much of the prior work, such as the co-training procedure of Blum and Mitchell (1998), we do not assume there are two redundant views both of which are suucient for classiication.

result	We evaluate our co-training strategy via experiments using data from the UCI repository.

background	Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.

background	Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.

method	To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.

background	There are two benefits of LSGANs over regular GANs.

result	We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.

background	Second, LSGANs perform more stable during the learning process.

result	We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.

background	However, we found that this loss function may lead to the vanishing gradients problem during the learning process.

background	First, LSGANs are able to generate higher quality images than regular GANs.

method	We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence.

result	Using real traces collected from Dianping, a representative LBSN, we demonstrate that DeepScan can achieve excellent prediction performance with an F1-score of 0.964.

method	To provide reliable information and improve the experience for legitimate users, we design and implement DeepScan, a malicious account detection system for LBSNs.

background	Our daily lives have been immersed in widespread location-based social networks (LBSNs).

background	Malicious attackers can easily join and post misleading information, often with the intention of influencing users' decisions in urban computing environments.

result	We also find that the time series features play a critical role in the detection system.

method	DeepScan combines newly introduced time series features and a set of conventional features extracted from user activities, and exploits a supervised machine-learning-based model for detection.

background	As an open platform, LBSNs typically allow all kinds of users to register accounts.

method	Different from existing approaches, DeepScan leverages emerging deep learning technologies to learn users' dynamic behavior.

method	In particular, we introduce the long short-term memory (LSTM) neural network to conduct time series analysis of user activities.

method	Depression and anxiety symptoms were measured using the Patient-Reported Outcomes Measurement Information System (PROMIS).

result	Results: Compared to those who used 0e2 social media platforms, participants who used 7e11 social media platforms had substantially higher odds of having increased levels of both depression (Adjusted Odds Ratio [AOR] 1⁄4 3.0, 95% CI 1⁄4 1.9e4.8) and anxiety symptoms (AOR 1⁄4 3.2, 95% CI 1⁄4 2.0e5.1).

method	We used ordered logistic regression models to assess associations between use of multiple SM platforms and mental health outcomes while controlling for eight covariates, including overall TSSM.

method	Methods: We surveyed a nationally-representative sample of 1787 U.S. young adults ages 19e32.

result	These associations are strong enough that it may be valuable for clinicians to ask individuals with depression and anxiety about multiple platform use and to counsel regarding this potential contributing factor.

background	Introduction: While increased time spent on social media (TSSM) has been associated with depression and anxiety, the independent role of using multiple social media (SM) platforms is unclear.

result	We assessed use of multiple SM platforms with an adapted Pew Internet Research scale.

result	Associations were linear (p < 0.001 for all) and robust to all sensitivity analyses.

other	© 2016 Published by Elsevier Ltd.

result	Conclusions: Use of multiple SM platforms is independently associated with symptoms of depression and anxiety, even when controlling for overall TSSM.

result	The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models.

background	This work presents an approach for computing feature contributions for random forest classification models.

background	Model interpretation is one of the key aspects of the model evaluation process.

objective	It allows for the determination of the influence of each variable on the model prediction for an individual instance.

background	For “black box” models, such as random forest, this information is hidden inside the model structure.

background	The explanation of the relationship between model variables and outputs is easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance.

method	Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology.

background	Costs and benefits are not equally distributed.

method	This approach allows a more flexible and scalable solution for cost benefit sharing and may enable new business models for the Internet of Things.

background	The Internet of Things is one of the most promising technological developments in information technology.

background	It promises huge financial and nonfinancial benefits across supply chains, in product life cycle and customer relationship applications as well as in smart environments.

method	However, these cost benefit sharing approaches are complex, time consuming, and have failed to achieve broad usage.

method	Cost benefit sharing models have been proposed to overcome this problem and to enable new areas of application.

method	On the basis of a beverage supply chain scenario, a prototype installation, based on an open source billing solution and the Electronic Product Code Information Service (EPCIS), is shown as a proof of concept and an introduction to different pricing options.

method	In this chapter, an alternative concept, suggesting flexible pricing and trading of information, is proposed.

background	One of the main reasons for this is the missing profitability for each individual stakeholder.

background	However, the adoption process of the Internet of Things has been slower than expected.

result	The cases show that performance measurement systems of different supply chain actors should be aligned in order to achieve strategic consistency.

method	The paper aims at filling this gap, and proposes an integrated framework for the after-sales network performance measurement, and provides an empirical application to two automotive case companies and their official service network.

other	# 2007 Elsevier B.V. All rights reserved.

result	In particular, the performance of different actors at the process level of the framework concurs in determining the after-sales service overall performance towards the final customer.

background	Nonetheless, little attention was devoted by scientific and managerial literature to this topic.

background	The after-sales activities are nowadays acknowledged as a relevant source of revenue, profit and competitive advantage in most manufacturing industries.

objective	In addition, since many actors are involved along the after-sale service supply chain, an integrated and multi-attribute set of measures needs to be designed consistently at every level of the supply chain.

result	In addition, linkages at other levels (mainly the business and activity ones) may be needed or helpful in ensuring consistency between strategic and operational objectives, inside the organisations and thus for the whole supply chain.

method	Top and middle management, therefore, should focus on the definition of a structured business performance measurement system for the after-sales business.

objective	This paper proposes to use autoencoders with nonlinear dimensionality reduction in the anomaly detection task.

method	Moreover, autoencoders can be useful as nonlinear techniques without complex computation as kernel PCA requires.

method	The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data.

method	This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails.

result	Finaly, the authors examine the learned features in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.

method	Also, autoencoders can increase their accuracy by extending them to denoising autoenconders.

method	The authors apply dimensionality reduction by using an autoencoder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property.

background	In the research field of game AI, it is a good approach to allow the non-player-characters (NPCs) of digital games to become more humanity.

background	It presents well performance in simulation of the thinking ability of human.

background	However, it needs a trial-and-error process to achieve the goal.

background	Reinforcement learning is an unsupervised machine learning method in the area of Artificial Intelligence.

objective	The goal of this paper is to make this game become more interesting from the enhanced interactions with these intelligent NPCs.

objective	In this paper, we try to build a Tank-battle computer game and use the methodology of reinforcement learning for the NPCs (tanks).

background	Tie strength has been operationalized as weights.

method	We illustrate the benefits of this approach by applying one of them to Freeman’s EIES dataset.

background	Ties often have a strength naturally associated with them that differentiate them from each other.

objective	However, these generalizations have solely focused on tie weights, and not on the number of ties, which was the central component of the original measures.

objective	A few network measures have been proposed for weighted networks, including three common measures of node centrality: degree, closeness, and betweenness.

objective	This paper proposes generalizations that combine both these aspects.

result	Extensive experiments on 12 real-world travel datasets demonstrate the effectiveness of PHINE over strong baseline methods.

objective	In this work, we aim to accurately predict passenger satisfaction over their rides and understand the key factors that lead to good/bad experiences.

result	We have deployed PHINE in the DiDi Big Data Center, delivering high-quality predictions for passenger satisfaction on a daily basis.

background	While current ride-sharing services have collected massive travel data, it remains challenging to develop data-driven techniques for modeling and predicting user ride experience.

background	Online taxicab platforms like DiDi and Uber have impacted hundreds of millions of users on their choices of traveling, but how do users feel about the ride-sharing services, and how to improve their experience?

method	Our PHINE framework is novel in that it is composed of spatial-temporal node binding and grouping for addressing the inherent data variation, and pattern preservation based joint training for modeling the interactions among drivers, passengers, locations, and time.

method	Based on in-depth analysis of large-scale travel data from a popular taxicab platform in China, we develop PHINE (Pattern-aware Heterogeneous Information Network Embedding) for data-driven user experience modeling.

background	In some cases, pattern recognition algorithms for processing depth images can be tested using actual sensors observing real-world objects.

background	3D sensors such as LIDARs, stereo cameras, time-of-flight cameras, and the Microsoft Kinect are increasingly found in a wide range of applications, including gaming, personal robotics, and space exploration.

result	The software is written in C++, and is released under the open source BSD license.

method	It permits publishing to a TCP socket at high frame-rates or can save to PCD (point cloud data) files.

method	We present GLIDAR, an OpenGL and GL Shading Language-based sensor simulator, capable of imaging nearly any static three-dimensional model.

background	Computer generation of images is especially useful for Monte Carlo-type analyses or for situations where obtaining real sensor data for preliminary testing is difficult (e.g., space applications).

background	In many situations, however, it is common to test new algorithms using computer-generated synthetic images, as such simulations tend to be faster, more flexible, and less expensive than hardware tests.

method	GLIDAR allows basic object manipulations, or may be connected to a physics simulator for more advanced behaviors.

background	This paper attempts to examine the effects of virtual team dimensions on social identities of its members.

method	Questionnaire-based data have been accomplished from 149 members of 44 teams.

result	The hypothesized relationships among the proposed variables are tested via a structural equation model (SEM).

method	Due to the importance of social identity, an attempt has been made to examine its influence on organizational variables (i.e. job satisfaction, job involvement, job commitment, and organizational citizenship behavior).

result	Results show that the geographically disperse and culturally diverse variables are negatively related to the social identity as against those of temporary and organizational variables which are related positively.

background	Sources like culture, place, and time seem to continuously acquire social identities.

method	A review of the literature shows that the geographically dispersed, culturally diverse as well as temporary dimensions of virtual teams do not match with their stability as members have different ethnic, social, or cultural backgrounds.

result	The leapfrog method is not dissipative, but we show that restarting results in a method with a useful amount of dissipation.

result	We also show that Gragg’s smoothing scheme improves the stability of the method.

other	2008 Elsevier Inc. All rights reserved.

other	‘‘

other	.

method	We prove that restarting in this way results in a method that is not stable.

background	The leapfrog method is popular because of its good stability when solving partial differential equations with oscillatory solutions.

other	”.

method	It has the disadvantage that the solution at odd time steps tends to drift farther and farther from the solution for even time steps, so it is common to stop the integration every twenty time steps or so and reinitialize with the first order forward Euler method . .

method	We further show that if the step size is not too big, perturbations grow so slowly that the computations are stable enough for practical purposes.

method	Players’ appearance models are unsupervised learned from hundreds of samples automatically collected by detection.

method	Thereafter, these models can be utilized for player labeling (Team A, Team B and Referee).

background	Automatic player detection, labeling and tracking in broadcast soccer video are significant while quite challenging tasks.

objective	In this paper, we present a solution to perform automatic multiple player detection, unsupervised labeling and efficient tracking.

result	The testing results on FIFA World Cup 2006 video demonstrate that our method can reach high detection and labeling precision, and reliably tracking in cases of scenes such as multiple player occlusion, moderate camera motion and pose variation.

method	Player tracking is achieved by Markov Chain Monte Carlo (MCMC) data association.

method	Some data driven dynamics are proposed to improve the Markov chain’s efficiency.

background	Players’ position and scale are determined by a boosting based detector.

background	Principal component analysis (PCA) is a popular tool for linear dimensionality reduction and feature extraction.

method	We also give an introduction on how PCA is used in active shape models (ASMs), and discuss how kernel PCA can be applied to improve traditional ASMs.

objective	In this paper, we first review the basic ideas of PCA and kernel PCA.

method	Then we focus on the reconstruction of pre-images for kernel PCA.

method	Then we show some experimental results to compare the performance of kernel PCA and standard PCA for classification problems.

method	We also implement the kernel PCA-based ASMs, and use it to construct human face models.

background	Kernel PCA is the nonlinear form of PCA, which better exploits the complicated spatial structure of high-dimensional features.

method	Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy.

background	Moreover, attacks can be carried out against groups of victims and can degenerate in physical violence.

method	We first propose a variety of hate categories to distinguish the kind of hate.

result	The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.

objective	In this work, we aim at containing and preventing the alarming diffusion of such hate campaigns.

background	Cyberbullism, incitement to self-harm practices, sexual predation are just some of the severe effects of massive online offensives.

method	We test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition.

background	While favouring communications and easing information sharing, Social Network Sites are also used to launch harmful campaigns against specific groups and individuals.

objective	Using Facebook as a benchmark, we consider the textual content of comments appeared on a set of public Italian pages.

method	Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, we design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM).

objective	In this article we examine 55 case studies in order to identify relevant characteristics of mobile marketing campaigns.

result	The proposed scheme allows to unambiguously characterize any given mobile marketing campaign and to identify the respective objectives.

background	Current mobile marketing research mostly covers success factors and acceptance analysis.

result	The outcome of the paper is the derivation of four mobile marketing standard types and an examination of campaign objectives that can be addressed by mobile marketing.

background	Categorization, when addressed, lacks in appropriate foundation and is not linked to objectives at all.

background	Marketing experts consider the mobile device as an extremely promising marketing tool as it supports them to cope with their major challenge: getting time and attention from customers.

objective	This paper presents an open source framework enabling easy integration of GPU and FPGA resources; Our work provides direct data transfer between the two platforms with minimal CPU coordination at high data rate and low latency.

background	Even less research are focusing on direct interaction of the two platforms [1].

method	Notwithstanding the generality of the presented framework, we present in this paper an actual implementation consisting of a single GPU board and a FPGA board connected through a PCIe link.

result	Measures on this implementation demonstrate achieved data rate that are close to the theoretical maximum.

background	Many researches investigate interaction and benefits of coupling them with a general purpose processor (CPU), but very few, and only very recently, integrate the two in the same computational system.

objective	Finally, at the best of our knowledge, this is the first proposition of an open source implementation of a system including an FPGA and a GPU that provides code for both sides.

background	In recent years two main platforms emerged as powerful key players in the domain of parallel computing: GPUs and FPGAs.

result	Among 113 participants and 22,398 extracted Instagram pictures, we found distinct picture features (e.g., hue, brightness, saturation) that are related to personality traits.

objective	In this study we tried to infer personality traits from the way users take pictures and apply filters to them.

background	By applying photo filters, users are able to create a style that they want to express to their audience.

method	To investigate this relationship, we conducted an online survey where we asked participants to fill in a personality questionnaire, and grant us access to their Instagram account through the Instagram API.

result	This allow for new ways to extract personality traits from social media trails, and new ways to facilitate personalized systems.

result	Our findings suggest a relationship between personality traits and the way users want to make their pictures look.

background	Instagram is a popular social networking application, which allows photo-sharing and applying different photo filters to adjust the appearance of a picture.

objective	To scale such a system to a whole building we introduce a space partitioning scheme to reuse fiducial markers throughout the environment.

method	To enable such applications we construct an indoor tracking system that covers a substantial part of a building.

objective	It is based on visual tracking of fiducial markers enhanced with an inertial sensor for fast rotational updates.

result	Finally we demonstrate two location based applications built upon this facility, an indoor navigation aid and a library search applica-

background	We believe that augmented reality is a natural interface to visualize spacial information such as position or direction of locations and objects for location based applications that process and present information based on the user’s position in the real world.

background	In this work we investigate building indoor location based applications for a mobile augmented reality system.

result	Moreover, it is revealed that user-generated content is perceived as more trustworthy when compared to official tourism websites, travel agents and mass media advertising.

method	It is also shown that there is a strong correlation between perceived level of influence from social media and changes made in holiday plans prior to final decisions.

objective	Through an empirical study among holiday travellers, residing in the Former Soviet Union Republics, this paper presents a comprehensive view of role and impact of social media on the whole holiday travel planning process: Before, during and after the trip, providing insights on usage levels, scope of use, level of influence and trust.

result	Findings suggest that social media are predominantly used after holidays for experience sharing.

background	A globally defined model of the quadrotor UAV rigid body dynamics is introduced as a basis for the analysis.

background	This paper provides new results for the tracking control of a quadrotor unmanned aerial vehicle (UAV).

method	A nonlinear tracking controller is developed on the special Euclidean group SE(3) and it is shown to have desirable closed loop properties that are almost global.

result	Several numerical examples, including an example in which the quadrotor recovers from being initially upside down, illustrate the versatility of the controller.

background	The UAV has four input degrees of freedom, namely the magnitudes of the four rotor thrusts, that are used to control the six translational and rotational degrees of freedom, and to achieve asymptotic tracking of four outputs, namely, three position variables for the vehicle center of mass and the direction of one vehicle body-fixed axis.

background	Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso.

method	We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap.

background	We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity.

result	Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.

method	Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure.

objective	Our goal is to recover the common set of relevant inputs for each output cluster.

method	We present an efficient optimization method that can handle a largescale problem.

method	A survey was conducted among Agile professionals, gathering survey data from 109 Agile projects from 25 countries across the world.

result	Limitations of the study are discussed together with interpretations for practitioners.

method	Agile software engineering methods have recently emerged as a new and different way of developing software as compared to the traditional methodologies.

objective	Subsequently, reliability analysis and factor analysis were conducted to consolidate this preliminary list into a final set of 12 possible critical success factors for each of the four project success categories – Quality, Scope, Time, and Cost.

result	The results revealed that only 10 out of 48 hypotheses were supported, identifying three critical success factors for Agile software development projects: (a) Delivery Strategy, (b) Agile Software Engineering Techniques, and (c) Team Capability.

method	Based on existing literature, a preliminary list of potential critical success factors of Agile projects were identified and compiled.

result	However, their success has mostly been anecdotal, and research in this subject is still scant in the academic circles.

background	While software is so important for all facets of the modern world, software development itself is not a perfect process.

result	Multiple regression techniques were used, both at the full regression model and at the optimized regression model via the stepwise screening procedure.

method	This research study was a survey study on the critical success factors of Agile software development projects using quantitative approach.

background	In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans.

method	We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).

objective	We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input.

background	Autonomous vehicles capable of navigating unpredictable real-world environments with little human feedback are a reality today.

background	Autonomous vehicle control imposes very strict requirements on the security of the communication channels used by the vehicle to exchange information as well as the control logic that performs complex driving tasks such as adapting vehicle velocity or changing lanes.

objective	This study presents a first look at the effects of security attacks on the communication channel as well as sensor tampering of a connected vehicle stream equipped to achieve CACC.

result	We also illustrate how different countermeasures, such as downgrading to ACC mode, could potentially be used to improve the security and safety of the connected vehicle streams.

background	Such systems rely heavily on onboard sensors such as cameras, radar/LIDAR, and GPS as well as capabilities such as 3G/4G connectivity and V2V/V2I communication to make real-time maneuvering decisions.

result	Our simulation results show that an insider attack can cause significant instability in the CACC vehicle stream.

result	However, the robot could not move up the wall well and fell down often.

background	This paper presents a wall-climbing robot which adopts passive suction cups as the attaching components.

result	As a result, it is shown that a moment generated by both of the gravity and the attaching force of suction cups turns the robot down from the wall.

result	The experiments showed that the proposed robot could attach and remove suction cups passively.

background	Using only one motor, this robot can not only move on a wall but also attach suction cups to the wall and remove them from the wall.

method	The prototype has been designed, fabricated and tested.

result	In order to solve this problem, the load of each suction cup when attached to a vertical wall is analyzed.

objective	Therefore, the proposed robot can realize the climbing motion on a wall with relatively low energy consumption.

result	Then a new model which improves the falling problems is thus designed.

background	Passive suction cups do not consume additional energy to keep adhesion.

objective	Taking the perspective of the brand community building plus the brand trust and loyalty literatures, our goal is to show how brand communities based on social media influence elements of the customer centric model (i.e., the relationships among focal customer and brand, product, company, and other customers) and brand loyalty.

method	A survey-based empirical study with 441 respondents was conducted.

background	There is an ongoing debate over the activities of brands and companies in social media.

background	Some researchers believe social media provide a unique opportunity for brands to foster their relationships with customers, while others believe the contrary.

result	The implications for marketing practice and future research are discussed.

other	All rights reserved.

other	© 2012 Elsevier Ltd.

method	We find that brand trust has a fully mediating role in converting the effects of enhanced relationships in brand community to brand loyalty.

result	The results of structural equation modeling show that brand communities established on social media have positive effects on customer/product, customer/brand, customer/company and customer/other customers relationships, which in turn have positive effects on brand trust, and trust has positive effects on brand loyalty.

result	Books now will appear in printed and soft file collection.

background	Imagine that you get such certain awesome experience and knowledge by only reading a book.

result	However, many people sometimes have no space to bring the book for them; this is why they can't read the book wherever they want.

other	How can?

method	One of them is this book business process management concepts languages architectures.

other	It is so usual with the printed books.

objective	It seems to be greater when a book can be the best thing to discover.

method	However, unlike existing approaches, we provide declarative semantics more suitable for process mining.

background	Process discovery—discovering a process model from example behavior recorded in an event log—is one of the most challenging tasks in process mining.

method	Causal nets are related to the representations used by several process discovery techniques (e.g., heuristic mining, fuzzy mining, and genetic mining).

objective	Therefore, we advocate a new representation more suitable for process discovery: causal nets.

background	Moreover, discovered process models tend to have deadlocks and livelocks.

background	The primary reason is that conventional modeling languages (e.g., Petri nets, BPMN, EPCs, and ULM ADs) have difficulties representing the observed behavior properly and/or succinctly.

method	To clarify these semantics and to illustrate the non-local nature of this new representation, we relate causal nets to Petri nets.

background	Fifty-four percent of the managers reported hot environmental conditions, 28% a noisy environment, and 26% a lack of resources and facilities.

result	A significant correlation ðp , 0:01Þ was found among productivity indicators and health and organizational attributes.

result	Ninety-four percent of the companies did not carry out ergonomic assessments.

result	Management (88%) acknowledged not having knowledge or access to ergonomics information.

other	q 2003 Elsevier Ltd. All rights reserved.

objective	The main objective of this research was to identify factors that affected worker productivity, occupational health and safety in selected industries in a developing country.

background	Fifty production managers participated in the study.

result	Lack of skills in ergonomics and training, communication and resources are believed to be some of the factors contributing to the poor ergonomic conditions and consequent loss of worker productivity and reduced health and safety in these industries.

method	Managers received worker complaints of fatigue, back pain, upper-body pain, hand and wrist pain and headaches.

background	However, valid statistical analysis used to identify such networks must address sources of noise in order to avoid possible confounds such as spurious correlations based on non-neuronal sources.

background	Resting state functional connectivity reveals intrinsic, spontaneous networks that elucidate the functional architecture of the human brain.

method	We describe the methods implemented in the Conn toolbox for the analysis of fcMRI data, together with examples of use and interscan reliability estimates of all the implemented fcMRI measures.

method	Compared to methods that rely on global signal regression, the CompCor noise reduction method allows for interpretation of anticorrelations as there is no regression of the global signal.

method	We have developed a functional connectivity toolbox Conn ( www.nitrc.org/projects/conn ) that implements the component-based noise correction method (CompCor) strategy for physiological and other noise source reduction, additional removal of movement, and temporal covariates, temporal filtering and windowing of the residual blood oxygen level-dependent (BOLD) contrast signal, first-level estimation of multiple standard functional connectivity magnetic resonance imaging (fcMRI) measures, and second-level random-effect analysis for resting state as well as task-related data.

result	The results indicate that the CompCor method increases the sensitivity and selectivity of fcMRI analysis, and show a high degree of interscan reliability for many fcMRI measures.

method	The toolbox implements fcMRI measures, such as estimation of seed-to-voxel and region of interest (ROI)-to-ROI functional correlations, as well as semipartial correlation and bivariate/multivariate regression analysis for multiple ROI sources, graph theoretical analysis, and novel voxel-to-voxel analysis of functional connectivity.

background	Despite their massivesize, successful deep artificial neural networkscan exhibit a remarkably small differencebetween training and test performance.

objective	Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice.

result	We corroborate these experimental findings with a theoretical construction showing that simpledepth two neural networksalready haveperfect finitesampleexpressivity assoon as thenumber of parameters exceeds thenumber of datapointsas it usually does in practice.

background	Conventional wisdom attributessmall generalization error either to propertiesof themodel family, or to the regularization techniquesused during training.

method	This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise.

result	We interpret our experimental findingsby comparison with traditional models.

method	Specifically, our experimentsestablish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data.

method	The use of visualization and highly interactive visual analytics systems can help to support this analysis process with respect to investigation, comparison, and summarization of malware samples.

background	While there are various automatic approaches and techniques available to detect, identify, or capture malware, the actual analysis of the ever-increasing number of suspicious samples is a time-consuming process for malware analysts.

background	The need to log and analyze activity encompasses networks, individual computers, as well as mobile devices.

objective	We provide a systematic overview and categorization of malware visualization systems from the perspective of visual analytics.

result	Additionally, we identify and evaluate data providers and commercial tools that produce meaningful input data for the reviewed malware visualization systems.

background	Due to the increasing threat from malicious software (malware), monitoring of vulnerable systems is becoming increasingly important.

result	This helps to reveal data types that are currently underrepresented, enabling new research opportunities in the visualization community.

background	Currently, there is no survey available that reviews available visualization systems supporting this important and emerging field.

objective	We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data.

background	Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry.

background	Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision.

method	Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised.

objective	By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.

method	We extract pixel features by either choosing the Hue or the Intensity as the dominant property based on the Saturation value of a pixel.

background	We have analyzed the properties of the HSV (Hue, Saturation and Value) color space with emphasis on the visual perception of the variation in Hue, Saturation and Intensity values of an image pixel.

method	The histogram retains a uniform color transition that enables us to do a window-based smoothing during retrieval.

result	The results have been compared with those generated using the RGB color space.

method	The feature extraction method has been applied for both image segmentation as well as histogram generation applications – two distinct approaches to content based image retrieval (CBIR).

method	Segmentation using this method shows better identification of objects in an image.

objective	The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent.

background	We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes.

result	Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.

method	We talked with participants who have different types of followings to understand their techniques, including targeting different audiences, concealing subjects, and maintaining authenticity.

result	Our model of the networked audience assumes a manyto-many communication through which individuals conceptualize an imagined audience evoked through their tweets.

background	Some techniques of audience management resemble the practices of ‘micro-celebrity’ and personal branding, both strategic self-commodification.

background	Social media technologies collapse multiple audiences into single contexts, making it difficult for people to use the same techniques online that they do to handle multiplicity in face-to-face conversation.

background	This article investigates how content producers navigate ‘imagined audiences’ on Twitter.

method	This paper modeled the micro grid photovoltaic power generation system ,including silicon solar cell, photovoltaic inverters, battery energy storage system, and the micro power distribution system .The use of power system analysis software (DIGSILENT) of actual power system simulation, the simulation results verify the model's correctness.

objective	The accurate modeling of micro-grid access to power system planning and design stage needs is the primary problem to solve.

result	In the power grid fault disturbance, the light intensity of disturbance and the load disturbances, the simulation results show that the optical storage combined with micro network has fast dynamic response characteristics, and its network of grid-connected voltage influenced by the changes of the light and load is little, while more affected by the network fault influence.

other	Keywords—sensor fusion; extended Kalman filter; localization; Robot Operating System

background	Accurate state estimation for a mobile robot often requires the fusion of data from multiple sensors.

result	It can support an unlimited number of inputs from multiple sensor types, and allows users to customize which sensor data fields are fused with the current state estimate.

method	This paper presents a software package, robot_localization, for the Robot Operating System (ROS).

background	Software that performs sensor fusion should therefore support the inclusion of a wide array of heterogeneous sensors.

method	The package currently contains an implementation of an extended Kalman filter (EKF).

result	In this work, we motivate our design decisions, discuss implementation details, and provide results from real-world tests.

result	Experimental results demonstrate its robustness and fast speed compared with the standard ICP algorithm.

method	The SICP algorithm is independent of shape representation and feature extraction; thereby it is general for scaling registration.

background	The ICP algorithm is accurate and fast for registration between two point sets in a same scale, but it doesn't handle the case with different scales.

objective	This paper instead introduces a novel approach named the scaling iterative closest point (SICP) algorithm which integrates a scale matrix with boundaries into the original ICP algorithm for scaling registration.

method	This method uses a simple iterative algorithm with the SVD algorithm and the properties of parabola incorporated to compute the translation, rotation and scale transformations at each iterative step, and its convergence is rapid with only a few iterations.

background	Fractional calculus has recently attracted much attention in the literature.

method	Some numerical examples are also presented.

background	In particular, fractional derivatives are widely discussed and applied in many areas.

objective	In this paper, based on Fourier series and Taylor series technique, we provide some numerical methods for computing and simulating fractional derivatives by using Matlab.

background	However, it is still hard to develop numerical methods for fractional calculus.

method	This paper presents methods to quantify the group differences between patients with schizophrenia and healthy controls, by extracting specialized features and analyzing group differences on a feature manifold.

background	In particular, patients with schizophrenia usually have impaired expressions in the form of "flat" or "inappropriate" affects, which make the quantification of their facial expressions a challenging problem.

result	The features are then embedded into an ISOMAP manifold to quantify the group differences between controls and patients.

result	Experiments show that our results are strongly supported by the human rating results and clinical findings, thus providing a framework that is able to quantify the abnormality in patients with schizophrenia.

method	The features include 2D and 3D geometric features, and the moment invariants combining both 3D geometry and 2D textures.

result	Facial expression recognition experiments on actors demonstrate that our combined features can better characterize facial expressions than either 2D geometric or texture features.

background	Most of current computer-based facial expression analysis methods focus on the recognition of perfectly posed expressions, and hence are incapable of handling the individuals with expression impairments.

method	This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems.

background	However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited.

background	Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data.

result	An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.

background	In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units.

background	Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks.

result	The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-ofthe-art on Office datasets.

result	Overall, the approach can be implemented with little effort using any of the deep-learning packages.

background	Top-performing deep architectures are trained on massive amounts of labeled data.

result	We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer.

method	As the training progresses, the approach promotes the emergence of “deep” features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains.

method	The resulting augmented architecture can be trained using standard backpropagation.

background	In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available.

background	Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).

background	Their computational power is jointly used for collaborative processing of variety of tasks – and this processing is realized in distributed manner.

result	First implementations were based on single UAV, later the potential of multiple UAVs collaborating in a team was noticed.

method	They have been employed to realize multiple tasks such as surveillance or environmental monitoring.

method	In this paper we survey the applications implemented over cooperative teams of UAVs that operate as distributed processing systems.

background	Distributed Processing Systems are the ones that include multiple devices (which could be of many types, such as PC computers, mobile devices etc.)

background	UAV Unmanned Aerial Vehicles (also called drones) gain significant attention over recent years.

result	Many applications were implemented in distributed manner, using multiple collaborative UAVs and the distributed processing systems principles.

background	that have computational and communication capabilities.

method	The case mimics as close as possible the project as it really took place.

background	The problem to be addressed is the replacement of a Hospital Information System in a large regional hospital.

background	In this paper we relate our experiences with a course specifically designed to address this issue.

objective	The objectives of the course are threefold: to train management and communication skills, to integrate and apply knowledge gained at different previous courses, and to learn by experience the difference between a real-world problem and a textbook problem.

background	Real-world case studies are important to complement the academic skills and knowledge acquired by computer science students.

result	We found that the three objectives mutually reinforce each other, which is a decisive factor for the success of the course.

result	Students’ evaluations show that the objectives of the course are met and that it is regarded as very useful.

background	The Internet of Things (IoT) represents a modern approach where boundaries between real and digital domains are progressively eliminated by changing over consistently every physical device to smart object ready to provide valuable services.

method	Estimating risk is a complex operation that requires the consideration of a variety of factors in the access control environment.

other	Existing risk-based access control models are discussed and compared in terms of the risk estimation technique, risk factors, and the evaluation domain.

background	These services provide a vital role in different life domains but at the same time create new challenges particularly in security and privacy.

other	This paper presents a review of different risk estimation techniques.

result	Moreover, the interpretation and estimation of the risk might vary depending on the working domain.

method	One of the essential elements in this model is the risk estimation process.

method	This model performs a risk analysis to estimate the security risk associated with each access request and uses the estimated risk to make the access decision.

background	Authentication and access control models are considered as the essential elements to address these security and privacy challenges.

background	Risk-based access control model is one of the dynamic access control models that provides more flexibility in accessing system resources.

background	Decision Tree Classifiers (DTC's) are used successfully in many diverse areas such as radar signal classification, character recognition, remote sensing, medical diagnosis, expert systems, and speech recognition, to name only a few.

result	After considering potential advantages of DTC's over single stage classifiers, subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed_ Finally, several remarks are made concerning possible future research directions.

objective	This paper presents a survey of current methods for DTC designs and the various existing issues.

background	Perhaps, the most important feature of DTC's is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution which is often easier to interpret.

method	BASIC GA follows all common steps of the genetic algorithms.

result	A range of various optimization problems has been solved to test its capability.

method	To handle all sorts of constraints the static and dynamic penalty f ©

method	It involves real representation schemes for both real and integer variables.

result	It provides an opportunity to the genetic operators to be extended with new schemes.

method	Three biased selection schemes for reproduction; four for recombination and three for mutation are applied in it and a new selection scheme for replacement is approached.

objective	This paper introduces in details a genetic algorithm-called BASIC, which is designed to take advantage of well known genetic schemes so as to be able to deal with numerous optimization problems.

method	BASIC GA can be easy adjusted to the concrete problems by fitting its global and local parameters.

method	The algorithm differs from most visual odometry algorithms in two key respects: (1) it makes no prior assumptions about camera motion, and (2) it operates on dense disparity images computed by a separate stereo algorithm.

method	This algorithm has been tested on many platforms, including wheeled and legged vehicles, and has proven to be fast, accurate and robust.

result	This paper includes a detailed description of the algorithm and experimental evaluation on a variety of platforms and terrain types.

result	Processing time is approximately 20 ms on a 512times384 image.

result	For example, after 4000 frames and 400 m of travel, position errors are typically less than 1 m (0.25% of distance traveled).

objective	This paper describes a visual odometry algorithm for estimating frame-to-frame camera motion from successive stereo image pairs.

method	By doing so, they contribute to the constitution of a digital layer upon the physical mobility infrastructure.

background	However, there is a lack of understanding about digital transformation of primarily physical industries, whose products cannot be completely digitized, e.g., automotive industry.

background	The phenomenon of digital transformation received some attention in previous literature concerning industries such as media, entertainment and publishing.

background	We examined the impact of major digital trends on dominant business models.

method	We conducted a rigorous content analysis of substantial secondary data from industry magazines aiming to generate insights to this phenomenon in the automotive industry.

method	Despite its strong foundation in the physical world, the industry is undergoing important structural changes due to the ongoing digitalization of consumer lives and business.

method	Our findings indicate that trends related to social media, mobile, big data and cloud computing are driving automobile manufactures to extend, revise, terminate, and create business models.

background	Computer laboratory is a facility where students access to the hardware and software necessary to fulfil the requirements in the course.

result	The results show that some modification is required to improve the current layout and fulfil a better environment for teaching and learning process in the facility.

objective	The objective of this paper is to propose new layout designs for the facility and identify the best layout that suits the purpose of the laboratory.

background	Computer laboratories are also used to train and expose students on computer programming, simulation and other subjects.

objective	The design and layout of the laboratory is an important concern to ensure that the facility provides maximum benefits to students.

objective	This paper examine the layout design of Industrial Application Computer Laboratory, in Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia.

method	We implement ThinkAir and evaluate it with a range of benchmarks starting from simple micro-benchmarks to more complex applications.

background	Smartphones have exploded in popularity in recent years, becoming ever more sophisticated and capable.

objective	In this paper we propose ThinkAir, a framework that makes it simple for developers to migrate their smartphone applications to the cloud.

background	As a result, developers worldwide are building increasingly complex applications that require ever increasing amounts of computational power and energy.

objective	Advancing on previous work, it focuses on the elasticity and scalability of the cloud and enhances the power of mobile cloud computing by parallelizing method execution using multiple virtual machine (VM) images.

method	We finally use a memory-hungry image combiner tool to demonstrate that applications can dynamically request VMs with more computational power in order to meet their computational requirements.

method	We then show that a parallelizable application can invoke multiple VMs to execute in the cloud in a seamless and on-demand manner such as to achieve greater reduction on execution time and energy consumption.

objective	ThinkAir exploits the concept of smartphone virtualization in the cloud and provides method-level computation offloading.

method	First, we show that the execution time and energy consumption decrease two orders of magnitude for a N-queens puzzle application and one order of magnitude for a face detection and a virus scan application.

background	Human being is the most intelligent animal in this world.

background	Intuitively, optimization algorithm inspired by human being creative problem solving process should be superior to the optimization algorithms inspired by collective behavior of insects like ants, bee, etc.

method	Two benchmark functions were tested to validate the effectiveness and usefulness of the proposed algorithm.

objective	In this paper, we introduce a novel brain storm optimization algorithm, which was inspired by the human brainstorming process.

objective	Automatic selection of these hard examples can make training more effective and efficient.

objective	We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors.

result	OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use.

objective	Our motivation is the same as it has always been - detection datasets contain an overwhelming number of easy examples and a small number of hard examples.

result	Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.

result	Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset.

result	But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012.

objective	The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune.

method	We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.

background	This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology.

background	We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis.

background	This paper describes a simple, efficient algorithm to locate all occurrences of any of a finite number of keywords in a string of text.

method	Construction of the pattern matching machine takes time proportional to the sum of the lengths of the keywords.

background	The number of state transitions made by the pattern matching machine in processing the text string is independent of the number of keywords.

objective	The algorithm consists of constructing a finite state pattern matching machine from the keywords and then using the pattern matching machine to process the text string in a single pass.

result	The algorithm has been used to improve the speed of a library bibliographic search program by a factor of 5 to 10.

background	Wireless Networks, and the use of mobile devices, are bringing the world a new means of communication and day-to-day business activities.

background	In recent years, there has been an elevated awareness in Information Security and Corporate Governance.

result	This paper will determine the risks involved with new technologies and motivate the importance of understanding these risks before its implementation by emphasising the important role Corporate and IT Governance play.

background	The implementation of these new Wireless devices also brings about new security threats to Information assets.

objective	This paper discusses the need to consider possible risks to ensure business survival and business continuity before the implementation of new technologies, with specific interest to wireless networks and wireless devices.

background	Information Technology is fast becoming essential within business processes and is considered a technical issue yet neglected at board level.

background	Organisations, and their board of directors, have become increasingly aware that securing information is vital for the organisation in both financial terms and corporate identity.

result	This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing.

result	Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions.

objective	To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently.

result	We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

result	However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency.

result	A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle.

method	To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles.

objective	The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree.

method	One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions.

background	Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images.

method	The method presented is robust to common defects in raw scanned data such as outliers and noise often present in extreme environments such as underwater, both for sonar and optical surveys.

background	These scans (often) acquired by underwater robots usually result in an unstructured point cloud, but given the common downward-looking or forward-looking configuration of these sensors with respect to the scene, the problem of recovering a piecewise linear approximation representing the scene is normally solved by approximating these 3D points using a heightmap (2.5D).

method	We present a method devoted to full 3D surface reconstruction that does not assume any specific sensor configuration.

method	Moreover, the proposed method does not need a manual preprocessing step.

background	Nevertheless, this representation is not able to correctly represent complex structures, especially those presenting arbitrary concavities normally exhibited in underwater objects.

method	Finally, and given the unbeatable level of detail that optical methods can provide, we analyze the application of this method on optical datasets related to biology, geology

background	Underwater range scanning techniques are starting to gain interest in underwater exploration, providing new tools to represent the seafloor.

result	This property leads to its wide application to any kind of range scanning technologies and we demonstrate its versatility by using it on synthetic data, controlled laser scans, and multibeam sonar surveys.

method	It is also generic as it does not need any information other than the points themselves to work.

background	Invasive species are one of the main drivers of biodiversity loss.

result	These issues will be explored using examples from a wide range of habitats and site conditions, towards the development of a robust methodology to identify native and non-native species.

background	In the past decade, the development of environmental spectroscopy, both field spectrometers and airborne imaging spectrometers, has allowed progress in identifying individual species from remote sensing data.

objective	However, use of environmental spectroscopy for species identification needs understanding at a more fundamental level, especially the development of generalized methodologies and rules for detection and mapping, which is an area of active research today.

result	The results indicate that with the implementation of standard security features, new, as well as, current risks can be minimized to acceptable levels albeit that the most serious risks, i.e., those derived from the human factor, need more careful consideration, as they are inherently complex to handle.

method	The risks classified as high were either related to the human factor or to the software components of the system.

result	With such a model of security and privacy in design in place, it will contribute to enforcing system security and enhancing user privacy in smart homes, and thus helping to further realize the potential in such IoT environments.

objective	In this context, understanding the risks related to the use and potential misuse of information about homes, partners, and end-users, as well as, forming methods for integrating security-enhancing measures in the design is not straightforward and thus requires substantial investigation.

result	A discussion of the implications of the risk analysis results points to the need for a more general model of security and privacy included in the design phase of smart homes.

background	Enforcing security in Internet of Things environments has been identified as one of the top barriers for realizing the vision of smart, energy-efficient homes and buildings.

method	Out of 32 examined risks, 9 were classified as low and 4 as high, i.e., most of the identified risks were deemed as moderate.

method	A risk analysis applied on a smart home automation system developed in a research project involving leading industrial actors has been conducted.

background	The performance of pattern classifiers depends on the separability of the classes in the feature space - a property related to the quality of the descriptors - and the choice of informative training samples for user labeling - a procedure that usually requires active learning.

objective	This work is devoted to improve the quality of the descriptors when samples are superpixels from remote sensing images.

method	We introduce a new scheme for superpixel description based on Bag of visual Words, which includes information from adjacent superpixels, and validate it by using two remote sensing images and several region descriptors as baselines.

background	In the past, GPUs enabled these breakthroughs because of their greater computational speed.

background	In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices.

result	Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.

result	As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL).

background	Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models.

result	We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated.

objective	Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks.

result	A further discussion of LFW face verification result is given in the end.

method	Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training.

result	An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively.

method	These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition.

objective	This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition.

background	The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning.

background	Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity.

objective	This motivates us to investigate their effectiveness on face recognition.

background	While we describe both software and hardware aspects, the focus of the paper is on software issues such as color calibration and removal of scanner artifacts.

method	With current scanner technology, the resulting camera system is capable of taking black&white, color, or near-infrared photographs with up to 490 million pixels.

result	We believe that the camera system described here has many potential applications in image-based modeling and rendering, cultural heritage projects, and professional digital photography.

background	We describe a system for transforming an off-the-shelf flatbed scanner into a $200 scan backend for large format cameras.

result	Our analysis shows that we achieve actual optical resolutions close to the theoretical maximum, and that color reproduction is comparable to commercial camera systems.

background	Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to structure from motion approaches.

result	This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas.

method	Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation.

result	We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera

method	Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware.

background	The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework.

background	We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene.

other	# 2001 Elsevier Science B.V. All rights reserved.

objective	In this study, we introduce playfulness as a new factor that re ̄ects the user's intrinsic belief in WWW acceptance.

method	Using it as an intrinsic motivation factor, we extend and empirically validate the Technology Acceptance Model (TAM) for the WWW context.

background	Ease of use and usefulness are believed to be fundamental in determining the acceptance and use of various, corporate ITs.

background	These beliefs, however, may not explain the user's behavior toward newly emerging ITs, such as the World-Wide-Web (WWW).

background	Call Centers are important channels of communication within the consumer relationship and a point of integration between suppliers and their customers.

method	However, specifying the capacity of a Call Center is not a trivial task, but one that demands a significant knowledge of mathematics, in particular of analytical models.

result	This paper presents the Erlang B, Erlang C and Simulation models followed by a comparison based on a case study, in order to identify the advantages of using simulation.

background	Correctly sizing the capacity of a given Call Center can bring benefits not only in terms of improved customer service (efficacy), but also in terms of reduced operating costs (efficiency).

result	The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on SCITAIL, especially in comparison to a simple majority class baseline.

result	SCITAIL is the first entailment set that is created solely from natural sentences that already exist independently “in the wild” rather than sentences authored specifically for the entailment task.

result	As a step forward, we demonstrate that one can improve accuracy on SCITAIL by 5% using a new neural model that exploits linguistic structure.

background	We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem.

method	This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult.

method	Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus.

result	These sentences are often linguistically challenging.

objective	The objective of the project is to provide a platform for Chinese language learners to explore and learn classifier uses in a bottom-up fashion.

background	Chinese language teachers often face challenges in finding an effective way to teach classifiers, as the rules for defining which nouns can be associated with which classifiers are not straightforward.

background	Learners often find that existing dictionaries either do not have classifiers as lexical entries, or give very brief explanations that are hardly helpful.

background	Chinese noun classifiers are an indispensible part of the Chinese language, but are difficult for non-native speakers to use correctly.

method	Descriptions of the design and the functions of the e-learning tool are provided in the paper.

background	This paper presents the progress of an ongoing project on the construction of an e-dictionary of Chinese classifiers.

background	Many theoretical studies have explored the nature of Chinese classifiers, but few studies take an empirical approach to the investigation of effective teaching and learning methods of classifiers.

method	The current work is on the design of an e-learning tool database and its connection to the e-dictionary database.

method	Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture.

background	We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs).

result	The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.

background	In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input.

background	We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only.

method	Firstly, we build a convolutional network amenable for fine-tuning the visual and textual representations, where the entire network only contains four components, i.e., convolution layer, pooling layer, rectified linear unit function (ReLU), and batch normalisation.

method	Secondly, we propose instance loss according to viewing each multimodal data pair as a class.

method	This works with a large margin objective to learn the inter-modal correspondence between images and their textual descriptions.

result	Experiments on two generic retrieval datasets (Flickr30k and MSCOCO) demonstrate that our method yields competitive accuracy compared to state-of-the-art methods.

background	Our system, in comparison, differs in two key aspects.

background	Existing work in this field largely uses Recurrent Neural Networks (RNN) for text feature learning and employs off-the-shelf Convolutional Neural Networks (CNN) for image feature extraction.

method	Endto-end learning allows the system to directly learn from the data and fully utilise the supervisions.

background	This paper considers the task of matching images and sentences.

result	Moreover, in language person retrieval, we improve the state of the art by a large margin.

background	The challenge consists in discriminatively embedding the two modalities onto a shared visual-textual space.

objective	We also propose a novel splitting criterion which chooses the split with the highest local AUC.

objective	In this paper, we show how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves.

background	Usually, each classifier is characterised by its estimated true and false positive rates and is represented by a single point in the ROC diagram.

result	We present experiments suggesting that the AUC splitting criterion leads to trees with equal or better AUC value, without sacrificing accuracy if a single labelling is chosen.

objective	In this setting, rather than estimating the accuracy of a single tree, it makes more sense to use the area under the ROC curve (AUC) as a quality metric.

background	ROC analysis is increasingly being recognised as an important tool for evaluation and comparison of classifiers when the operating characteristics (i.e. class distribution and cost parameters) are not known at training time.

background	To the best of our knowledge, this is the first probabilistic splitting criterion that is not based on weighted average impurity.

objective	We propose a memory-centric deep learning system that can transparently expand the memory capacity accessible to the accelerators while also providing fast inter-device communication for parallel training.

objective	Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion.

result	Compared to conventional systems, our proposal achieves an average <inline-formula> <tex-math notation="LaTeX">$2.1\times$</tex-math><alternatives><inline-graphic xlink:href="rhu-ieq1-2823302.gif"/> </alternatives></inline-formula> speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.

background	As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied.

background	While the capacity to collect and store new data grows rapidly, the ability to analyze these data volumes increases at much lower pace.

background	The emerging field of visual analytics focuses on handling massive, heterogenous, and dynamic volumes of information through integration of human judgement by means of visual representations and interaction techniques in the analysis process.

background	In today's applications data is produced at unprecedented rates.

background	This gap leads to new challenges in the analysis process, since analysts, decision makers, engineers, or emergency response teams depend on information "concealed" in the data.

background	Furthermore, it is the combination of related research areas including visualization, data mining, and statistics that turns visual analytics into a promising field of research.

objective	This paper aims at providing an overview of visual analytics, its scope and concepts, and details the most important technical research challenges in the field

objective	The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations.

result	Typically, certain words extracted from the texts of documents and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections.

method	In either case, the documents would be represented by term vectors of the form D= (ti,tj,...

background	AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users’ information queries.

other	1.

objective	These results depend crucially on the choice of effective termweighting systems.

other	Thus, a typical query Q might be formulated as Q = (qa,qbr.. .

other	,4r) (2)

method	ytp) (1) where each tk identifies a content term assigned to some sample document D. Analogously, the information requests, or queries, would be represented either in vector form, or in the form of Boolean statements.

objective	This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.

objective	At last this paper presents some kinds of improved versions of PSO and research situation, and the future research issues are also given.

background	The algorithm is widely used and rapidly developed for its easy implementation and few particles required to be tuned.

background	It comes from the research on the bird and fish flock movement behavior.

method	The main idea of the principle of PSO is presented; the advantages and the shortcomings are summarized.

background	Particle swarm optimization is a heuristic global optimization method and also an optimization algorithm, which is based on swarm intelligence.

background	This strong interest can be explained not only by the importance this task has for many applications but also by the phenomenal advances in this area since the arrival of deep convolutional neural networks (DCNN).

background	in images – attracted a lot of attention from the community during the last 5 years.

result	The survey covers not only the typical architectures (SSD, YOLO, Faster-RCNN) but also discusses the challenges currently met by the community and goes on to show how the problem of object detection can be extended.

background	Object detection – the computer vision task dealing with detecting instances of objects of a certain class (e.g ., ’car’, ’plane’, etc.)

objective	This article reviews the recent literature on object detection with deep CNN, in a comprehensive way, and provides an in-depth view of these recent advances.

objective	This survey also reviews the public datasets and associated state-of-the-art algorithms.

method	The problem has tong been a favourite iir programming courses as one which admits a concise recursive solution.

objective	The objective is to move the n rings one by one until they are all stacked on another peg (B) in such a way that no ring is ever placed on a smaller ring; the other peg (C) can be used as workspace.

background	The famous Towers of Hanoi puzzle consists of 3 pegs (A, B, C) on one of which (A) are stacked n rings of different sizes, each ring resting on a larger ring.

method	This solution hinges on the observation that, when the largest ring is moved from A to B, the n 1 remaining rings must all be on peg C. This immediately leads to the recursive procedure

method	The article further lists 28 design principles for persuasive system content and functionality, describing example software requirements and implementations.

method	It discusses the process of designing and evaluating persuasive systems and describes what kind of content and software functionality may be found in the final product.

other	Some of the design principles are novel.

objective	This article is conceptual and theory-creating by its nature, suggesting a framework for Persuasive Systems Design (PSD).

method	It also highlights seven underlying postulates behind persuasive systems and ways to analyze the persuasion context (the intent, the event, and the strategy).

background	A growing number of information technology systems and services are being developed to change users’ attitudes or behavior or both.

background	Despite the fact that attitudinal theories from social psychology have been quite extensively applied to the study of user intentions and behavior, these theories have been developed for predicting user acceptance of the information technology rather than for providing systematic analysis and design methods for developing persuasive software solutions.

result	Moreover, a new categorization of these principles is proposed, consisting of the primary task, dialogue, system credibility, and social support categories.

background	The primary motivation for replication lies in fault tolerance.

method	We introduce a new model for replication in distributed systems.

method	To fulfil these requirements, we incorporate the idea of directory-oriented replication and extended prefix tab/es in the system design.

result	Through this design, we can simulate a UNIX-like distributed file system whose function is compatible with MS-DOS.

background	Transparency as well as fault-tolerance file access are the highlights of our system design.

method	Although there are different kinds of replication approaches, our model combines the advantages of modular redundancy and primary-stand-by approaches to give more flexibility with respect to system configuration.

method	To implement such a model, we select the IBM PC-net with MS-DOS environment as our base.

method	The implementation consists of a command shell, a DOS manager, and a recovery manager.

method	The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms.

method	Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others.

method	Model selection is discussed both from a Bayesian and classical perspective.

objective	The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning.

objective	Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines.

method	The book contains illustrative examples and exercises.

background	GPs have received growing attention in the machine learning community over the past decade.

method	A wide variety of covariance (kernel) functions are presented and their properties discussed.

method	Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed.

objective	The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.

objective	A process model for rigid body angular motions and angular rate measurements is defined.

result	Test cases included the presence of large initial errors as well as high noise levels.

objective	The filter represents rotations using quaternions rather than Euler angles, which eliminates the long-standing problem of singularities associated with attitude estimation.

background	Each MARG sensor contains a three-axis magnetometer, a three-axis angular rate sensor, and a three-axis accelerometer.

background	The best quaternion is used as part of the measurements for the Kalman filter.

result	As a result of this approach, the measurement equations of the Kalman filter become linear, and the computational requirements are significantly reduced, making it possible to estimate orientation in real time.

result	Extensive testing of the filter with synthetic data and actual sensor data proved it to be satisfactory.

background	This paper presents an extended Kalman filter for real-time estimation of rigid body orientation using the newly developed MARG (Magnetic, Angular Rate, and Gravity) sensors.

method	The Gauss-Newton iteration algorithm is utilized to find the best quaternion that relates the measured accelerations and earth magnetic field in the body coordinate frame to calculated values in the earth coordinate frame.

method	The process model converts angular rates into quaternion rates, which are integrated to obtain quaternions.

objective	To improve these security issues, we propose a Smart Door Lock system based on blockchain.

background	However, the data sent and received of existing Smart Door Lock system is vulnerable to forgery and hacking.

background	This is manifested by the large number of connected devices.

result	Also, this provides data integrity and non-repudiation.

method	Lastly, we propose an algorithm that the Smart Door Lock system judge some situations around itself and operates based on data sent from sensors.

background	IoT (Internet of Things) has a large portion of our life.

background	With the exponential growth of IoT devices, IoT security is becoming important.

background	In particular, Smart Door Lock system is extremely important because it is closely related to the safety of the user.

result	Finally, we show that LVQ is a general histogram classifier and that its risk converges to the Bayesian optimal risk as the appropriate parameters go to infinity with the number of past observations.

background	We show that the learning algorithm performs stochastic approximation.

result	We also present a modification to the learning algorithm which we argue results in convergence of the LVQ for a larger set of initial conditions.

background	In this paper, we show that the LVQ learning algorithm converges to locally asymptotic stable equilibria of an ordinary differential equation.

background	Convergence of the Voronoi vectors is guaranteed under the appropriate conditions on the underlying statistics of the classification problem.

objective	It provides an overview of economic analysis techniques and their applicability to software engineering and management.

objective	This paper summarizes the current state of the art and recent trends in software engineering economics.

method	It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.

method	The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper.

method	Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data.

background	We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes.

result	We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.

result	Potential problems with adapting a management theory to a government setting are discussed.

method	Originally a management theory, stakeholder theory advocates addressing the concerns of all stakeholders in a firm, as opposed to concentration on the interests of senior managers and stockholders.

background	One strategy for coping with theoretical immaturity is to import and adapt theories from other, more mature fields.

result	The paper further discusses how information technology impacts a stakeholder model of governance.

method	Apart from its original profit focus, there is no serious conceptual mismatch between stakeholder theory and government’s objective of providing policy and services for citizens and organizations – society’s stakeholders.

objective	This study reviews Stakeholder Theory (ST) and investigates its potential in relation to e-Government.

result	Finally, the paper makes recommendations for future work in adapting ST to the e-government context.

background	The e-government field, like most young fields, lacks a strong body of well-developed theory.

objective	Starting from effects of insufficient data quality in practice, a definition for information, data and data quality will be worked out.

objective	As key part DQM an approach for operative DQM (planing and measuring data quality) will be illustrated and explained.

result	Finally, based on the research results further conclusions are summarised.

background	High level data quality and the management of ensuring data quality is one of the key success factors for Data Warehousing projects.

background	The following article describes an approach for Data Quality Management, which is based on theories as well as practical experiences.

objective	Based on the concept of total data quality management the Data Quality Management (DQM) for Data-Warehouse-System will be described.

method	Their retrieval results on the MPEG-7 Core Experiment CE-Shape-1 test set as reported in the literature and obtained by a reimplementation are compared and discussed.

method	Fifteen shape similarity measures are shortly described and compared.

background	This paper gives an overview of shape dissimilarity measure properties, such as metric and robustness properties, and of retrieval performance measures.

other	2011 Elsevier Ltd. All rights reserved.

objective	However, an optimisation model may be formulated in order to determine a safety stock level which guarantees the performance measure under the worst case of lead-time demand, of which the distribution is known in an incomplete way.

background	The decision on the safety stock level is based on a performance measure, for example the expected shortage per replenishment period or the probability of a stock-out per replenishment period.

background	Inventory systems with uncertainty go hand in hand with the determination of a safety stock level.

result	It is shown that this optimisation problem can be formulated as a linear programming problem.

method	The performance measure assumes complete knowledge of the probability distribution during lead time, which might not be available.

objective	In case of incomplete information regarding the lead-time distribution of demand, no single figure for the safety stock can de determined in order to satisfy a performance measure.

method	ASUM discovers pairs of {aspect, sentiment} which we call senti-aspects.

result	The results of sentiment classification show that ASUM outperforms other generative models and comes close to supervised classification methods.

method	We applied SLDA and ASUM to reviews of electronic devices and restaurants.

method	In this paper, we tackle the problem of automatically discovering what aspects are evaluated in reviews and how sentiments for different aspects are expressed.

result	The results show that the aspects discovered by SLDA match evaluative details of the reviews, and the senti-aspects found by ASUM capture important aspects that are closely coupled with a sentiment.

method	We first propose Sentence-LDA (SLDA), a probabilistic generative model that assumes all words in a single sentence are generated from one aspect.

method	We then extend SLDA to Aspect and Sentiment Unification Model (ASUM), which incorporates aspect and sentiment together to model sentiments toward different aspects.

result	One important advantage of ASUM is that it does not require any sentiment labels of the reviews, which are often expensive to obtain.

background	User-generated reviews on the Web contain sentiments about detailed aspects of products and services.

background	However, most of the reviews are plain text and thus require much effort to obtain information about relevant details.

objective	The objective is to regulate the intake manifold pressure (MAP) and exhaust gas recirculation (EGR) rate to the specified set-points by coordinated control of the variable geometry turbine (VGT), EGR valve, and EGR throttle.

method	The approach uses tube-MPC to robustly enforce state constraints in the presence of set-bounded engine speed and fueling disturbances.

result	The ability of the controller to handle input and output constraints is demonstrated in both nonlinear model simulations and experimental results.

background	The paper presents the results of a robust Model Predictive Control (MPC) development for the diesel engine air path.

method	Furthermore, a rate-based formulation along with various approximations is applied to reduce the conservativeness and computational complexity of tube-MPC.

other	When appropriate, we reconcile conflicting notation and nomenclature.

background	In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models.

background	Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences.

background	Countless learning tasks require dealing with sequential data.

background	Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities.

background	Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window.

objective	Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them.

background	In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences.

background	In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition.

background	Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes.

method	We first demonstrate that an on-chip power monitor can be built on a modern FPGA using ring oscillators (ROs), and characterize its ability to observe the power consumption of other modules on the FPGA or the SoC. Then, we show that the RO-based FPGA power monitor can be used for a successful power analysis attack on an RSA cryptomodule on the same FPGA.

result	This work introduces and demonstrates remote power side-channel attacks using an FPGA, showing that the common assumption that power side-channel attacks require specialized equipment and physical access to the victim hardware is not true for systems with an integrated FPGA.

result	Additionally, we show that the FPGA-based power monitor can observe the power consumption of a CPU on the same SoC, and demonstrate that the FPGA-to-CPU power side-channel attack can break timing-channel protection for a RSA program running on a CPU.

background	The rapid adoption of heterogeneous computing has driven the integration of Field Programmable Gate Arrays (FPGAs) into cloud datacenters and flexible System-on-Chips (SoCs).

objective	This paper shows that the integrated FPGA introduces a new security vulnerability by enabling software-based power side-channel attacks without physical proximity to a target system.

background	Meanwhile, blockchain widely known as one of the disruptive technologies has emerged in recent years, is experiencing rapid development and has the potential to revolutionize intelligent transport systems.

background	However, emerging applications and services require major changes in underlying network models and computing that require new road network planning.

background	It allows better utilization of the infrastructure and resources of intelligent transport systems, particularly effective for crowdsourcing technology.

method	Blockchain can be used to build an intelligent, secure, distributed and autonomous transport system.

objective	Block-VN is a reliable and secure architecture that operates in a distributed way to build the new distributed transport management system.

background	In recent decades, the ad hoc network for vehicles has been a core network technology to provide comfort and security to drivers in vehicle environments.

method	In addition, we examine how the network of vehicles evolves with paradigms focused on networking and vehicular information.

objective	In this paper, we proposes a vehicle network architecture based on blockchain in the smart city (Block-VN).

objective	We are considering a new network system of vehicles, Block-VN, above them.

method	Finally, we discuss service scenarios and design principles for Block-VN.

method	This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques.

background	More recently, neural network models started to be applied also to textual natural language signals, again with very promising results.

objective	The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.

background	Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing.

method	Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering.

result	Several implications are discussed.

method	These results indicate that unsupervised dimension reduction is closely related to unsupervised learning.

result	Experiments indicate that the new bounds are within 0.5-1.5% of the optimal values.

method	New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix.

result	On learning, the result suggests effective techniques for K-means data clustering.

result	On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noise-reduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data.

background	Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction.

background	K-means clustering is a commonly used data clustering for performing unsupervised learning tasks.

result	DNA gene expression and Internet newsgroups are analyzed to illustrate our results.

result	In addition, our approach can provide users with some explanation why the system recommends certain items.

method	One of the proposed approaches is to consider a recommender system as aMarkov decision process (MDP) problem and try to solve it using reinforcement learning (RL).

objective	A recommender system aims to recommend items that a user is interested in among many items.

method	We formulate a recommender system as a gridworld game by using a biclustering technique that can reduce the state and action space significantly.

background	The need for the recommender system has been expanded by the information explosion.

method	Using biclustering not only reduces space but also improves the recommendation quality effectively handling the cold-start problem.

result	To solve an MDP in a recommender system, they encountered a problem with the large number of discrete actions that bring RL to a larger class of problems.

method	Various approaches have been suggested for providing meaningful recommendations to users.

method	In this paper, we propose a novel RL-based recommender system.

method	However, existing RL-based methods have an obvious drawback.

method	In sequential methods, missing attribute values are replaced by known values first, as a preprocessing, then the knowledge is acquired for a data set with all known attribute values.

method	In this chapter the main emphasis is put on rule induction.

background	These methods are categorized into sequential and parallel.

background	In this chapter methods of handling missing attribute values in data mining are described.

method	Methods of handling attribute values for decision tree generation are only briefly summarized.

method	In parallel methods, there is no preprocessing, i.e., knowledge is acquired directly from the original data sets.

method	Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task.

background	We study the problem of object recognition for categories for which we have no training examples, a task also called zero--data or zero-shot learning.

result	In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes.

objective	To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape.

background	This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them.

result	Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes.

method	Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase.

background	We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos.

method	We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations.

method	Our framework is generic to any input modality, i.e., RGB, depth, and RGB-D videos.

method	We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows.

background	To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality.

background	Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions.

method	We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D.

method	Specifically, at each time slot, each user maps its current state to spectrum access actions based on a trained deep-Q network used to maximize the objective function.

background	After each time slot, each user that has transmitted a packet receives a local observation indicating whether its packet was successfully delivered or not (i.e., ACK signal).

method	To tackle this problem, we develop a distributed dynamic spectrum access algorithm based on deep multi-user reinforcement leaning.

background	We consider the problem of dynamic spectrum access for network utility maximization in multichannel wireless networks.

background	The shared bandwidth is divided into K orthogonal channels, and the users access the spectrum using a random access protocol.

result	Experimental results have demonstrated that users are capable to learn good policies that achieve strong performance in this challenging partially observable setting only from their ACK signals, without online coordination, message exchanges between users, or carrier sensing.

objective	The objective is to find a multi-user strategy that maximizes a certain network utility in a distributed manner without online coordination or message exchanges between users.

background	In the beginning of each time slot, each user selects a channel and transmits a packet with a certain attempt probability.

objective	Obtaining an optimal solution for the spectrum access problem is computationally expensive in general due to the large state space and partial observability of the states.

background	We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location.

method	In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution.

result	We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.

method	We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space.

background	This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside.

method	We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces.

method	This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours.

method	We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.

background	We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones.

background	Generative approaches have thus far been either inflexible, inefficient or non-scalable.

background	The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis.

background	'No-shows' or missed appointments result in under-utilized clinic capacity.

objective	We develop a logistic regression model using electronic medical records to estimate patients' no-show probabilities and illustrate the use of the estimates in creating clinic schedules that maximize clinic capacity utilization while maintaining small patient waiting times and clinic overtime costs.

method	This study used information on scheduled outpatient appointments collected over a three-year period at a Veterans Affairs medical center.

result	Clinics should consider the benefits of implementing scheduling software that includes these methods relative to the cost of no-shows.

method	The call-in process for 400 clinic days was simulated and for each day two schedules were created: the traditional method that assigned one patient per appointment slot, and the proposed method that scheduled patients according to their no-show probability to balance patient waiting, overtime and revenue.

method	Combining patient no-show models with advanced scheduling methods would allow more patients to be seen a day while improving clinic efficiency.

method	This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.

method	On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy.

result	Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place.

objective	Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set.

background	We present a simple, highly modularized network architecture for image classification.

method	Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity.

background	Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology.

result	We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.

result	The code and models are publicly available online.

background	The proliferation of cyber-physical systems introduces the fourth stage of industrialization, commonly known as Industry 4.0.

method	Thus, the smart factory is characterized by a self-organized multi-agent system assisted with big data based feedback and coordination.

method	Moreover, this kind of self-organized system leverages the feedback and coordination by the central coordinator in order to achieve high efficiency.

background	In this paper, we present a smart factory framework that incorporates industrial network, cloud, and supervisory control terminals with smart shop-floor objects such as machines, conveyers, and products.

method	Based on this model, we propose an intelligent negotiation mechanism for agents to cooperate with each other.

method	Then, we provide a classification of the smart objects into various types of agents and define a coordinator in the cloud.

method	Furthermore, the study illustrates that complementary strategies can be designed to prevent deadlocks by improving the agents’ decision making and the coordinator’s behavior.

method	The autonomous decision and distributed cooperation between agents lead to high flexibility.

result	The simulation results assess the effectiveness of the proposed negotiation mechanism and deadlock prevention strategies.

background	The vertical integration of various components inside a factory to implement a flexible and reconfigurable manufacturing system, i.e., smart factory, is one of the key features of Industry 4.0.

background	State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.

background	In this paper, we introduce two new neural architectures—one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers.

other	1

method	Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora.

objective	Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.

background	An empirical assessment of metrics to predict the quality attributes is essential in order to gain insight about the quality of software in the early phases of software development and to ensure corrective actions.

objective	In this paper, we predict a model to estimate fault proneness using Object Oriented CK metrics and QMOOD metrics.

method	The proposed models are validated using dataset collected from Open Source software.

result	The results show that the model predicted using the random forest and bagging methods outperformed all the other models.

background	An understanding of quality attributes is relevant for the software organization to deliver high software reliability.

method	We apply one statistical method and six machine learning methods to predict the models.

method	The results are analyzed using Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis.

result	Hence, based on these results it is reasonable to claim that quality models have a significant relevance with Object Oriented metrics and that machine learning methods have a comparable performance with statistical methods Keywords—Empirical Validation, Object Oriented, Receiver Operating Characteristics, Statistical Methods, Machine Learning, Fault Prediction

method	Next, using this model and trajectory optimization techniques we find locally optimal open-loop policies that allow the system to perform dynamic maneuvers we call grabs.

result	By studying such an extreme example of a soft robot, we can begin to solve hard problems inhibiting the mainstream use of soft machines.

objective	The goal of this work is to develop a soft robotic manipulation system that is capable of autonomous, dynamic, and safe interactions with humans and its environment.

result	In 37 experimental trials with a physical prototype, we successfully perform a grab 92% of the time.

method	Then, we present a strategy for independently identifying all unknown components of the system: the soft manipulator, its distributed fluidic elastomer actuators, as well as drive cylinders that supply fluid energy.

background	First, we develop a dynamic model for a multi-body fluidic elastomer manipulator that is composed entirely from soft rubber and subject to the self-loading effects of gravity.

background	Localization is one important part of Internet of Things(IoT) where the Location of Everything (LoE) system plays a important role to improve most services in IoT area.

method	Then a novel accurate localization scheme named “location orbital” is developed that estimates the current location of mobile objects (users or everything) based on both current and the previous locations.

method	Finally, an implementation of the experiment in a shopping mall is conducted to practically examine performance evaluation of the location-based scheme.

method	For this purpose, we design a smart shopping platform including four components, location of everything component, data collection component, data filtering/analysing component and data mining component.

background	On the other hand, data mining techniques are essential analyses when we have big data from IoT platforms.

result	The experimental results show that the proposed scheme could achieve significant higher precision than other localization techniques.

background	Indeed, integration of location-based methods and data mining analysis process can make a smart system service for IoT scenarios and applications.

objective	We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule).

background	Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image.

objective	In each experiment the data set was divided into two parts, a training set and a test set.

result	These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.

method	This paper describes some easily computable textural features based on graytone spatial dependancies, and illustrates their application in categoryidentification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories.

background	Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery.

background	How many moves does it take to solve Rubik’s Cube?

method	This paper describes a program that is able to find solutions of length 20 or less at a rate of more than 16 million positions a second.

method	Positions are known that require 20 moves, and it has already been shown that there are no positions that require 27 or more moves; this is a surprisingly large gap.

objective	We use this program, along with some new ideas and incremental improvements in other techniques, to show that there is no position that requires 26 moves.

method	Terms such as the “real-time enterprise” and the “zero latency organization” are often used to describe firms that use real-time BI.

background	By moving to real-time, firms can use BI to affect current decision making and business processes.

background	This capability is especially important for customer-facing applications, such as those found in call centers and check-in processes, and helps firms become more customer-centric.

background	The movement to real-time is the latest development in business intelligence (BI) and data warehousing.

background	Real-time data warehousing provides the data that is required to implement realtime BI.

method	This system was also developed as a prototype system that would enable persons with speech impairments to rapidly convert hand-drawn symbols on a pen-enabled device into speech output.

objective	This paper describes an efficient system for entering data into a pen-enabled computer, particularly handheld devices.

objective	While keyboard input typically is faster than handwriting input, this is not true for the small PDA interfaces.

result	We created a library of chatroom abbreviations and shorthand symbols, and developed a k-nn classification system to recognize the symbols.

result	Experimental results show the effectiveness of the system in terms of speed and accuracy.

method	For this reason, we designed and developed a prototype that uses chatroom abbreviations and shorthand symbols to increase the speed of data entry for these devices.

background	We explore deep reinforcement learning methods for multi-agent domains.

method	We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.

method	We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination.

method	We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows.

method	Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies.

method	Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction.

result	We also train a model for car distance estimation on the KITTI dataset.

background	Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor.

result	Results show that our direct perception approach can generalize well to real driving images.

objective	We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving.

method	To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments.

method	In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving.

method	Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously.

result	Source code and data are available on our project website.

result	The contribution from different noise sources in the amplifier and JFET to the overall noise is shown.

method	Gain of the amplifier is about 83 dB. Noise analysis is made for active-type, capacitive-type, and low impedance signal sources.

background	The design of a low-frequency high-input-impedance amplifier having probably the lowest noise ever reported is presented.

background	The amplifier's frequency range is from about 0.07 Hz to about 110 kHz at the -3-dB level.

background	The equivalent input noise voltage spectral density is about 5.6, 1.4, 0.6, and 0.5 nV/radicHz at frequencies 0.1, 1, 10, and 1000 Hz, respectively.

result	Subsequently, we data-refine this program to a constant-space solution in terms of linked structures.

background	We do so in two steps.

method	First, we invert a program that computes the inorder traversal of a binary heap, using the proof rules for program inversion by W. Chen and J.T. Udding.

method	This results in a linear-time solution in terms of binary trees.

background	In this paper we derive a linear-time, constant-space algorithm to construct a binary heap whose inorder traversal equals a given sequence.

method	The ratings of these issues suggest that technology infrastructure concerns, rather than planning and management concerns, have a larger impact on the IS operations of foreign af®liates.

result	Our study also con®rms that IIS issues can signi®cantly impact the strategic, tactical and operational IS decisions of af®liates.

objective	In this study, we perform a three-round Delphi study to identify, rank and evaluate the twenty most signi®cant IIS issues of af®liates.

result	The results indicate that respondents of IS and non-IS executives and af®liates of different international involvement levels have different views on the ratings of IIS issues.

method	This study also reports on statistical analyses to differentiate the impact of different industries, respondents, IS structures and international involvement of af®liates on IIS issues ratings.

background	As business competition becomes global, international information systems (IIS) management presents a signi®cant challenge to multinational corporations and their af®liates.

result	These ®ndings allow some important implications to be drawn for both practitioners and researchers dealing with IIS issues.

background	However, very few empirical studies have been conducted to investigate the management of IIS and the issues that confront the IS executives of such corporations.

other	# 2001 Elsevier Science B.V. All rights reserved.

background	Increasingly, though, many business strategies depend on specific underlying IT capabilities.

method	The “standardized technology architecture stage” has an enterprise-wide IT architecture that provides efficiencies through technology standardization.

background	IT architecture is often assumed to follow business strategy, to align IT with the business’s strategic objectives.

result	And the “modular architecture stage” builds onto enterprise-wide global standards with loosely coupled IT components to preserve the global standards while enabling local differences.

result	The “rationalized data architecture stage” extends the enterprise-wide IT standards to data and processes.

result	Each stage demands different organizational competencies to implement the architecture and prepare the firm to move to the next stage.

objective	My research has identified four IT architectural stages, each with its own requisite competencies.

method	The “application silo architecture stage” consists of IT architectures of individual applications.

background	To develop a synergy between business strategy and IT architecture, firms must develop organizational competencies in IT architecture.

objective	Here we propose a probability graphical model that exploits the payment data to discover customer purchase behavior in the spatial, temporal, payment amount and product category aspects, named STPC-PGM.

background	In other words, personalized campaigns at the right time and in the right place can be treated as the last stage of consumption.

result	Our experiment results show that STPC-PGM is effective in discovering customers' profiling features, and outperforms the state-of-the-art methods in purchase behavior prediction.

result	As a result, the mobility behavior of an individual user could be predicted with a probabilistic graphical model that accounts for all aspects of each customer's relationship with the payment platform.

background	User payment data offer a good dataset to depict customer behavior patterns.

method	For example, by exploring customer behavior patterns, given a target store, a set of potential customers is able to be identified.

background	With the advances in the development of mobile payments, a huge amount of payment data are collected by banks.

method	To achieve real time advertising, we then develop an online framework that efficiently computes the prediction results.

result	In addition, the prediction results are being deployed in the marketing of real-world credit card users, and have presented a significant growth in the advertising conversion rate.

background	A comprehensive understanding of customers' purchase behavior is crucial to developing good marketing strategies, which may trigger much greater purchase amounts.

background	Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training.

result	Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.

background	The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source.

method	KBANN is evaluated by extensive empirical tests on two problems from molecular biology.

method	It maps problem-specific “domain theories”, represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation.

background	KBANN(Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques.

background	By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources.

objective	Therefore a novel classification framework for Gamification in Information Systems with the intention to provide a structured, summarized as well as organized overview was constructed to close this gap of research.

method	A literature review on Gamification in quality outlets combined with a Grounded Theory approach served as a starting point.

result	Findings from the literature review were mapped to the classification framework and analyzed.

background	Gamification evolved to one of the most important trends in technology and therefore gains more and more practical and scientific notice.

result	Moreover it offers a structure for Gamification research which was not available previously.

result	Derived from the classification framework and its outcome future research outlets were identified.

background	Yet academia lacks a comprehensive overview of research, even though a review of prior, relevant literature is essential for advancing knowledge in a field.

method	As a result this paper provides a foundation for current and future research to advance the knowledge on Gamification.

method	Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions.

objective	We present a method that learns word embedding for Twitter sentiment classification in this paper.

method	To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.

result	Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.

background	This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.

method	We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.

background	Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.

background	A questionnaire regarding the use of mobile devices was developed and distributed to 416 students in a Greek University.

background	There were completed 384 questionnaires.

result	However, they do not deal with many of the devices’ operations.

background	Mobile technology is a continuously growing domain and research activities regarding its use are quite intensive.

method	They use their mobiles mostly at home, then at the University.

method	They use their mobiles to communicate (telephone, SMS, email) mostly with their boy/girlfriend, then with their friends.

background	They also tend to use their mobiles to take photos and activate the reminder.

background	Also, they consider health issues as the main reason to limit the use of their mobiles.

result	Finally, there was not a statistically significant relationship between genders and their preferences.

background	The results revealed that students use their mobiles mostly for phone calls and SMS (short message service).

background	We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning.

method	In effect, our method trains the model to be easy to fine-tune.

result	We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.

objective	The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples.

method	In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task.

objective	This paper compares these two types of models by using literature survey results and case-study experiences.

background	EA and ES usage maturity models are used to assess how well c ompanies are capable of deploying these two concepts while striving to achi eve strategic corporate goals.

background	There is an increased awareness of the roles that e nterprise architecture (EA) and enterprise systems (ES) play in today ’s organizations.

result	We conclude that (i) EA and ES usage maturity model s agree on a number of critical success factors and (ii) in a company with a mature architecture function, one is likely to observe, at the early stages of ES initiatives, certain practices associated with a higher level of ES usage ma turity.

background	The existence of various architecture and ES usage models raises questions about how they both refer to each other, e.g. if a higher level of architecture maturity implies a higher ES usage level.

method	Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof).

method	The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach.

background	The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis.

result	Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.

background	Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure.

method	To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function.

background	Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics.

result	We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.

background	This is why pixel-space video prediction is viewed as a promising avenue for unsupervised feature learning.

objective	In this work, we train a convolutional network to generate future frames given an input sequence.

background	However, data scientists are facing large challenges, handling the massive amount of data efficiently and generating insights with real business value.

background	The amount of data stored by banks is rapidly increasing and provides the opportunity for banks to conduct predictive analytics and enhance its businesses.

objective	In this paper, the Intelligent Customer Analytics for Recognition and Exploration (iCARE) framework is presented to analyze banking customer behaviors from banking big data, through analytical modeling methodologies and techniques designed for a key business scenario.

result	In this case, iCARE helps generate insights for active customers based on their transaction behavior, using close to 20 terabytes of data.

method	The advantages of the iCARE framework have been confirmed in a real case study of a bank in southeast China.

method	Combining IBM software platforms and big data processing power with customized data analytical models, the iCARE solution provides deeper customer insights to satisfy a bank’s specific business need and data environment.

objective	We conduct an exhaustive survey of image thresholding methods, categorize them, express their formulas under a uniform notation, and finally carry their performance comparison.

method	We identify the thresholding algorithms that perform uniformly better over nondestructive testing and document image applications.

method	The thresholding methods are categorized according to the information they are exploiting, such as histogram shape, measurement space clustering, entropy, object attributes, spatial correlation, and local gray-level surface.

other	ral he tuffect as 3; a Abstract.

method	© 2004 SPIE and IS&T. [DOI: 10.1117/1.1631316]

method	The comparison is based on the combined performance measures.

method	40 selected thresholding methods from various categories are compared in the context of nondestructive testing applications as well as for document images.

result	We describe results for several different annotation problems.

result	Annotations are produced quickly.

result	The quality is good, and can be checked and controlled.

objective	We show how to outsource data annotation to Amazon Mechanical Turk.

result	We describe some strategies for determining when the task is well specified and properly priced.

result	Doing so has produced annotations in quite large numbers relatively cheaply.

background	We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices.

method	With calibration, this is reduced to 1.34cm and 2.12cm.

method	We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2:5M frames.

background	From scientific research to commercial applications, eye tracking is an important tool across many domains.

method	Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively.

method	Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device.

result	Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results.

background	Despite its range of applications, eye tracking has yet to become a pervasive technology.

other	The code, data, and models are available at http://gazecapture.csail.mit.edu.

method	An EMG control system that achieves control of the PIY Glove is also described.

method	This work paves the way for the implementation of printable pneumatics in real-world applications, particularly robot-assisted hand therapy.

method	PIY Glove uses a novel, fold based design of 3D printed soft actuators to achieve bending motion in the fingers.

method	Fabrication guidelines of the PIY Glove are laid out and characterization of the glove in terms of its range of motion and grip force are also presented.

background	This paper presents the design and preliminary investigation of a fully 3D printed soft robotic hand exoskeleton, Print-it-Yourself (PIY) Glove for stroke patients.

method	The PIY Glove is fabricated with Fused Deposition Modeling (FDM) using consumer based 3D printing technology to lower fabrication costs and allow patients to 3D print a rehabilitative and assistive device at home.

background	The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively.

result	The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature.

background	A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA).

objective	The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively.

result	The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well.

background	Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically.

result	The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems.

background	The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature.

result	Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html .

method	Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet.

result	It is found that convolutional layers in different levels characterize the target from different perspectives.

result	Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.

result	The discoveries motivate the design of our tracking system.

background	It is also found that for a tracking target, only a subset of neurons are relevant.

method	A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy.

objective	We propose a new approach for general object tracking with fully convolutional neural network.

result	A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance.

result	Both layers are jointly used with a switch mechanism during tracking.

result	A focus group consisting of senior researchers in the field of computing was utilised to validate the model.

method	Appropriate relations between these are identified, as well as attributes of the various concepts in the conceptual model.

objective	An ontology engineering approach is followed in developing a conceptual model of the domain using UML, with a focus on studies in the computing disciplines.

background	Recognising the need for the development of research capacity and changing learning paradigms that include online and collaborative approaches, an ontology of research methodology needs to be developed to allow for the shared creation of knowledge in this domain.

objective	A research scheme that is made up of a philosophical world view, a research design, and research methods is proposed.

method	Therefore, this paper switches to density-based embedding and propose KG2E for explicitly modeling the certainty of entities and relations, which learn the representations of KGs in the space of multi-dimensional Gaussian distributions.

method	In fact, different entities and relations may contain different certainties, which makes identical certainty insufficient for modeling.

method	In addition, compared with the symmetric measures used in point-based methods, we employ the KL-divergence for scoring triplets, which is a natural asymmetry function for effectively modeling multiple types of relations.

result	Each entity/relation is represented by a Gaussian distribution, where the mean denotes its position and the covariance (currently with diagonal covariance) can properly represent its certainty.

background	To this end, some proposed models (e.g., TransE) embed entities and relations of a KG into a "point" vector space by optimizing a global loss function which ensures the scores of positive triplets are higher than negative ones.

result	Our experimental results demonstrate that our method can effectively model the (un)certainties of entities and relations in a KG, and it significantly outperforms state-of-the-art methods (including TransH and TransR).

background	The representation of a knowledge graph (KG) in a latent space recently has attracted more and more attention.

background	We notice that these models always regard all entities and relations in a same manner and ignore their (un)certainties.

result	We have conducted extensive experiments on link prediction and triplet classification with multiple benchmark datasets (WordNet and Freebase).

method	In general, online reading has had a negative impact on people’s cognition.

background	Using an exploratory survey, it examined the online and offline reading behaviour of individuals, and determined the underlying patterns, the differences between online and offline reading, and the impacts of the online environment on individuals’ reading behaviour.

method	The findings indicated that there were definite differences between people’s online and offline reading behaviours.

result	Concentration, comprehension, absorption and recall rates were all much lower while reading online than offline.

background	This study explored the impact of the Internet on our reading behaviour.

result	Managerial implications of these results are discussed.

result	Subject Areas: Cross-Functional Product Development Teams, knowledge Management, Learning, New Product Development, and Team Leadership.

method	Moreover, a democratic leadership style, initiation of goal structure by the team leader, and his or her position within the organization were positively related to team learning.

objective	This study examines how leadership characteristics in new product development teams affect the learning, knowledge application, and subsequently the performance of these teams.

method	Using data from a study of 229 members from 52 high-tech new product projects, we empirically demonstrate that team learning has a strong positive effect on the innovativeness and speed to market of the new products.

background	The circuit consists of an instrumentation amplifier (INA) with driven-right-leg circuit (DRL), a 5th order Gm -C low pass filter (Gm-C LPF) operating in sub-threshold mode, and amplifiers.

method	DRL circuit is used to detect small amplitude signal in the presence of large common-mode voltage from the human body.

result	As a result of using the DRL, a small signal can be detected in the presence of large common-mode differential.

result	The CMRR of the INA is 78 dB and the Gm-C LPF has a cutoff frequency of 18 Hz.

result	The circuit was designed in a 0.35mum CMOS process and simulation results have successfully demonstrated the functionalities

method	The circuit consumes 1.23 mW when operating from with a supply voltage of plusmn1.5-V and occupies a core area of 0.94 mm2.

background	In this paper, an electrocardiographic (ECG) signal processing IC, which is used for portable biomedical application, was designed using continuous-time technique.

background	In this paper, we present a novel PRedictive Elastic reSource Scaling (PRESS) scheme for cloud systems.

method	Our approach leverages light-weight signal processing and statistical learning algorithms to achieve online predictions of dynamic application resource requirements.

background	Cloud systems require elastic resource allocation to minimize resource provisioning costs while meeting service level objectives (SLOs).

result	Our experiments show that we can achieve good resource prediction accuracy with less than 5% over-estimation error and near zero under-estimation error, and elastic resource scaling can both significantly reduce resource waste and SLO violations.

method	PRESS unobtrusively extracts fine-grained dynamic patterns in application resource demands and adjust their resource allocations automatically.

method	We have implemented the PRESS system on Xen and tested it using RUBiS and an application load trace from Google.

objective	Global consistency on Internet privacy protection is important to boost the growth of electronic commerce.

background	An increasing number of people are using the Internet, in many instances unaware of the information being collected about them.

objective	New Zealand based web sites are expected to have privacy statements on their web sites under the New Zealand Privacy Act 1993.

background	Businesses should be aware that consumers are looking for privacy protection and a privacy statement can help to ease consumers' concerns.

result	To protect consumers in a globally consistent manner, legislation, self-regulation, technical solutions and combination solutions are different ways that can be implemented.

objective	The incidence of the information gathered from New Zealand web sites and their use of privacy statements is examined here.

background	In particular, web sites utilizing cookies and statements about them are scanned.

background	In contrast, other people concerned about the privacy and security issues are limiting their use of the Internet, abstaining from purchasing products online.

method	The model is constructed via non-linear transformations between the neighboring slices and further morphing.

objective	Afterwards, the obtained 3D-model is used to generate virtual 2D-images of the brain in arbitrary section-plane.

method	We also use rigid-body transforms in the preprocessing stage to align the slices.

method	The paper describes a method of fully automatic 3D-reconstruction of a mouse brain from a sequence of histological coronal 2D slices.

method	We use this approach to construct a highresolution anatomic 3D-model of a mouse brain using well-known Allen Brain Atlas which is publicly available.

objective	The goal to be optimized is the overall number of submitted Web queries.

objective	One original contribution of our research is the formalization and theoretical foundation of the problem.

result	The performance gain achieved with our approach is substantial: compared to the uninformed baseline (without co-occurrence information) the expected savings are up to 20% in the number of submitted queries and runtime.

objective	But, in particular, we develop a co-occurrence probability informed search strategy for the problem.

background	Given a set of keywords, we find a maximum Web query (containing the most keywords possible) that respects user-defined bounds on the number of returned hits.

background	We assume a real-world setting where the user is not given direct access to a Web search engine's index, i.e., querying is possible only through an interface.

method	In addition, an enhanced variant of this method combining a PF with artificial-intelligence search strategies and an omnidirectional local phase estimator, based on the mode of the power spectral density, is also presented.

result	Results obtained with synthetic and real data show a significant improvement with respect to other conventional unwrapping algorithms in some situations.

objective	This paper presents a new phase-unwrapping (PU) algorithm for SAR interferometry that makes use of a particle filter (PF) to perform simultaneously noise filtering and PU.

objective	The formulation of this technique provides independence from noise statistics and is not constrained by the nonlinearity of the problem.

result	Concluding the results of the previous inquiry, this paper contains a lobby tool based on J2ME and C# to increase the matching mechanisms in a local environment.

background	Thus more and more games are released in this section (including a huge number of different mobile phone games).

background	Over the last decade the importance of network games has seen a tremendous growth.

objective	This paper gives an introduction into the differences of current mobile gaming platforms and their capabilities.

background	Mobile gaming in a wireless environment and the availability to play games at any place is receiving major importance.

method	The current survey system features a database to handle the huge amount of answers (the predecessor used a polling system).

method	Furthermore it features a user survey about individual preferences and social coefficients with unexpected results.

background	A large part includes the size reduction of the handheld devices.

background	With the increase in opportunities one must first look at the user behavior to understand how to improve current problems.

background	Thus, the mobile market offers a wide variety of devices, such as the new handhelds like Nintendo DS and Sony PSP.

objective	We propose a flexible new technique to easily calibrate a camera.

method	The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion.

method	Radial lens distortion is modeled.

result	It advances 3D computer vision one step from laboratory environments to real world use.

method	The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations.

method	Either the camera or the planar pattern can be freely moved.

result	Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained.

method	The motion need not be known.

objective	It is well suited for use without specialized knowledge of 3D geometry or computer vision.

result	Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible.

method	Thus, ML is both broad enough to apply to many domains and narrow enough to benefit from domain-specific architectures, such as Googles Tensor Processing Unit (TPU).

background	The end of Moores law and Dennard scaling has led to the end of rapid improvement in general-purpose program performance.

background	It has recently revolutionized vision, speech, language understanding, and many other fields, and it promises to help with the grand challenges facing our society.

method	Hence, ML experts and computer architects must work together to design the computing systems required to deliver on the potential of ML.

method	Moreover, the growth in demand for ML computing exceeds Moores law at its peak, just as it is fading.

method	The computation at its core is low-precision linear algebra.

background	Machine learning (ML), and in particular deep learning, is an attractive alternative for architects to explore.

result	This article offers motivation, suggestions, and warnings to computer architects on how to best contribute to the ML revolution.

background	However, programming sensor networks and applications to be deployed in them is extremely challenging.

other	Nevertheless, many research challenges are still open.

background	Wireless sensor networks (WSNs) constitute a new pervasive and ubiquitous technology.

background	They have been successfully used in various application areas and in future computing environments, WSNs will play an increasingly important role.

background	This aspect is currently changing as different high-level programming abstractions and middleware solutions are coming into the arena.

objective	This paper presents a survey of the current state-of-the-art in the field, establishing a classification and highlighting some likely research challenges and future directions.

background	It has traditionally been an error-prone task since it requires programming individual nodes, using low-level programming issues and interfacing with the hardware and the network.

result	Anhand von zwölf Experteninterviews konnte u.a.

method	Es ist jedoch unklar, inwiefern diese oder andere strukturierte Vorgehensweisen in der Praxis tatsächlich zum Einsatz kommen und wie hilfreich sie sind.

background	Unternehmen sind zunehmend gezwungen, ihre Geschäftsmodelle anzupassen oder sogar neu zu erfinden,

method	festgestellt werden, dass die Weiterentwicklung des Geschäftsmodells eines Unternehmens zumeist nicht als Prozess verstetigt ist, sondern als Reaktion auf

other	eine negative Geschäftsentwicklung erfolgt.

method	Es existieren jedoch in Literatur und Praxis vielfältige Methoden zur kreativen

background	um mit technologischen Entwicklungen und sich verändernden Kundenbedürfnissen Schritt halten zu können.

background	Ein konkurrenzfähiges Geschäftsmodell ist für Unternehmen von existenzieller Bedeutung.

background	Strukturierende Frameworks aus der einschlägigen Literatur kommen nur selten zum Einsatz und werden teilweise explizit abgelehnt.

method	In der wissenschaftlichen Literatur existieren mit der Business Model Canvas und dem St. Galler Business Model Navigator zwei umfangreiche Frameworks zur Beschreibung und Entwicklung von Geschäftsmodellen.

result	Furthermore, it was demonstrated that these effects were significantly greater for heavy Facebook users, passive Facebook users, and users who tend to envy others on Facebook.

result	Based on a 1-week experiment with 1,095 participants in late 2015 in Denmark, this study provides causal evidence that Facebook use affects our well-being negatively.

background	Most people use Facebook on a daily basis; few are aware of the consequences.

method	By comparing the treatment group (participants who took a break from Facebook) with the control group (participants who kept using Facebook), it was demonstrated that taking a break from Facebook has positive effects on the two dimensions of well-being: our life satisfaction increases and our emotions become more positive.

result	, informative missingness.

method	GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network.

objective	In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a.

method	There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance.

result	Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.

objective	Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values.

method	In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts.

method	It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results.

result	On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.

objective	The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform.

method	On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device.

method	A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms.

objective	To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems.

method	We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures.

method	Using this framework, we develop a set of design principles and research questions, using a running case to illustrate some of our ideas.

result	We conclude with a summary of opportunities for IS researchers to extend our knowledge of gamified information systems, and, at the same time, advance existing theories.

background	However, few research and design guidelines exist regarding gamified information systems.

method	We first explicate the idea of gamified information systems, provide real-world examples of successful and unsuccessful systems, and, based on a synthesis of the available literature, present a taxonomy of gamification design elements.

objective	We therefore write this commentary to call upon information systems scholars to investigate the design and use of gamified information systems from a variety of disciplinary perspectives and theories, including behavioral economics, psychology, social psychology, information systems, etc.

background	Excitement surrounding gamification results from its many potential organizational benefits.

background	Gamification, an emerging idea for using game design elements and principles to make everyday tasks more engaging, is permeating many different types of information systems.

method	We then develop a framework for research and design: its main theme is to create meaningful engagement for users; that is, gamified information systems should be designed to address the dual goals of instrumental and experiential outcomes.

method	The first section describes resources that were recommended by ACUTA members who serve either on the ACUTA Higher Education Advisory Panel or the ACUTA Social Networking, New Media, and Web Resources Subcommittee.

method	The other sections of the review identify scholarly publications and presentations that were identified through online library searches about social media or social networks in higher education.

background	In the Spring of 2010, ACUTA conducted a survey of its institutional members regarding their institutions’ use of social networking sites.

result	Abstracts were drawn from the sources indicated at the end of each abstract.

background	This literature review was compiled in conjunction with the survey research.

other	© 2014 Elsevier Ltd.

method	This model is named as the brand communication through digital influencers model.

background	The growing power of bloggers to influence their connected network has emerged as a new communication venue for brands.

objective	This study elaborates upon the role of bloggers in brand communication, and reveals how brands can engage with bloggers, currently considered as online opinion leaders, from the perspective of the two-step flow theory.

method	Based on the findings of the interviews, we propose a model which traces the influencer role of bloggers from the two-step flow theory perspective.

method	This exploratory study reflects current blogger communication implementations, and concludes with a discussion of seven major issues arising from the literature review and interviews (definition of bloggers, blogger selection criteria, digital integration, power of bloggers, long-term relationship building with bloggers, measurement, and budgetary issues in blogger communication).

objective	These areas represent relatively unexplored areas of blogger engagement from both an academic and managerial perspective.

other	All rights reserved.

method	Following clarification of the aims of the study, we report on in-depth interviews with 17 brand and digital agency representatives, selected because they regard communication with bloggers as an important strategy in increasing the influence of their brands among online communities.

method	We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.

background	Previous work has used monolingual parallel corpora to extract and generate paraphrases.

result	We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.

method	We show that this task can be done using bilingual parallel corpora, a much more commonly available resource.

method	Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.

background	Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care.

result	There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge.

background	Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured.

method	In this article, we review the recent literature on applying deep learning technologies to advance the health care domain.

method	Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them.

method	Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health.

method	We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.

method	The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data.

result	However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists.

background	The LTO infrastructure is designed to allow parallel linking of large applications using a special mode, WHOPR.

objective	In this paper we present an overview of the design and implementation of WHOPR and present results of its behavior when optimizing large applications.

background	GCC 4.5.0 introduces support for link time optimization (LTO).

method	We compare WHOPR’s compile time, memory usage, and code quality to the results of the classical file-byfile optimization model, focusing on its effects on GCC itself and the Firefox web browser.

result	We examine critical issues which arise only when considering large applications, such as startup time and code size growth.

result	We conclude with statistical evidence in support of this hypothesis.

method	Using a combination of network clustering algorithms and manually-annotated data we demonstrate that the network of political retweets exhibits a highly segregated partisan structure, with extremely limited connectivity between leftand right-leaning users.

result	We examine two networks of political communication on Twitter, comprised of more than 250,000 tweets from the six weeks leading up to the 2010 U.S. congressional midterm elections.

method	Surprisingly this is not the case for the user-to-user mention network, which is dominated by a single politically heterogeneous cluster of users in which ideologically-opposed individuals interact at a much higher rate compared to the network of retweets.

background	To explain the distinct topologies of the retweet and mention networks we conjecture that politically motivated individuals provoke interaction by injecting partisan content into information streams whose primary audience consists of ideologically-opposed users.

background	In this study we investigate how social media shape the networked public sphere and facilitate communication between communities with different political orientations.

method	Vinyl/ceramic tiles, adhesives, and grout are carried onboard the robot and replenished by the operator.

objective	In order to compete with human installation, a time of 24 seconds per installed tile has to be matched.

objective	The research is motivated by the need to reduce the installation time and cost while guaranteeing consistent quality.

method	A mechanically compliant placement device would place the tile quickly and accurately without damaging the placed and surrounding tiles, emulating a human capability.

method	High resolution imaging is needed to identify tile seams and edges, assess the quality of automatic installation, and locate where the next tile should be placed.

method	Navigation and positioning are performed through a laserbased triangulation system, and by detecting, counting and dead-reckoning off of tiles placed on the floor.

method	The technical solution that is deemed feasible and capable of reducing this time to about 10 seconds, is an autonomous, electrically-powered mobile robot with omni-directional locomotive capability, and stereo cameras and light-striper for sensing.

result	Tile and installation quality are continuously monitored and errors corrected for, based on an overall layout map.

objective	This paper describes the configuration of a floor-tile installation robot for commercial buildings.

background	This technical note describes a new baseline for the Natural Questions (Kwiatkowski et al., 2019).

background	Our model is based on BERT (Devlin et al., 2018) and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30% and 50% relative for the long and short answer tasks respectively.

objective	This baseline has been submitted to the official NQ leaderboard and we plan to opensource the code for it in the near future.

background	The main generalization techniques employed by memory-based learning systems are the nearest-neighbor search, space decomposition techniques, and clustering.

method	The sample size and system complexity are derived for each method.

background	In particular, there are very few rigorous theoretical results regarding memory requirement, sample size, expected performance, and computational complexity.

background	Research on memory-based learning is still in its early stage.

result	Our main result is that we can build memory-based learning systems using new clustering algorithms (Lin & Vitter, 1992a) to PAC-learn in polynomial time using only polynomial storage in typical situations.

objective	In this paper, we propose a model for memory-based learning and use it to analyze several methods— ε-covering, hashing, clustering, tree-structured clustering, and receptive-fields—for learning smooth functions.

method	Our model is built upon the generalized PAC learning model of Haussler

result	(Haussler, 1989) and is closely related to the method of vector quantization in data compression.

background	A memory-based learning system is an extended memory management system that decomposes the input space either statically or dynamically into subregions for the purpose of storing and retrieving functional information.

result	To bridge the gap, the purpose of this study is to increase a better understanding of the drivers and barriers affecting older consumers’ intention to shop online.

result	Consequently, it is notable that older adults show no gender differences in regards to the drivers and barriers.

result	By comparing younger consumers with their older counterparts, in terms of gender the findings indicate that the major factors driving older adults toward online shopping are performance expectation and social influence which is the same with younger.

objective	To this end, this study is developed by integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) and innovation resistance theory.

background	The use of the Internet by older adults is growing at a substantial rate.

result	On the other hand, the major barriers include value, risk, and tradition which is different from younger.

result	2014 Elsevier Ltd. All rights reserved.

background	They are becoming an increasingly important potential market for electronic commerce.

background	However, previous researchers and practitioners have focused mainly on the youth market and paid less attention to issues related to the online behaviors of older consumers.

background	The findings of five previous SSPA and TWTA studies, including the 1991 European Space and Technology Center study, the 1993 National Aeronautics and Space Administration study, and three Boeing studies conducted in 2005, 2008, and 2013, are tabulated and summarized.

result	We find that <2% of TWTAs and 5% of SSPAs experience anomalies.

result	This new study contains the largest number of satellites and amplifiers to date and compares output power, redundancy, and bandwidth capabilities.

result	The ratio of operational to redundant amplifiers is 10 times higher for TWTAs than SSPAs, and the majority of amplifiers over the past 30 years operate with bandwidth less than 100MHz.

objective	Overall, this research was performed to update and clarify how the power and bandwidth needs and redundancy trends of the SatCom community have evolved over the past 30 years.

background	This study captures the state of current satellite transponder technology, specifically, solid-state power amplifiers (SSPAs) and traveling wave tube amplifiers (TWTAs), and describes expected future advances, including GaN SSPAs.

background	The results of these studies are then compared with new analyses of two validated sources of amplifier data: a commercially licensed database, Seradata’s Spacetrak, and a publicly available database, Gunter’s Space Page.

result	The new analyses consider a total of 18,902 amplifiers (6428 TWTAs, 2158 SSPAs, and 10,316 unspecified amplifiers) onboard 565 communications satellites launched from 1982 to 2016.

result	A second analysis is conducted using failure records and telemetry of 16 geostationary satellites equipped with 659 amplifiers: 535 SSPAs and 124 TWTAs.

result	We find an increase in output power from the 1993 study of >200% for Ku-band TWTAs and C-band SSPAs, and >1000% increase for C-band TWTAs.

result	So, the book is very appropriate for you.

background	multisensor data fusion a review of the state of the art is one of the literary work in this world in suitable to be reading material.

method	Developing your countless minds is needed; moreover you are kind of people with great curiosity.

background	That's not only this book gives reference, but also it will show you the amazing benefits of reading a book.

background	Now, we come to offer you the right catalogues of book to open.

background	We note that these models simply put both entities and relations within the same semantic space.

objective	Knowledge graph completion aims to perform link prediction between entities.

result	Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.

method	Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities.

background	In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling.

background	In this paper, we consider the approach of knowledge graph embeddings.

background	Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity.

method	In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces.

result	In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction.

background	When this variable is nominal we have a problem of class imbalance that was already studied thoroughly within machine learning.

result	These approaches change the distribution of the given training data set to decrease the problem of imbalance between the rare target cases and the most frequent ones.

result	Still, important application areas involve forecasting rare extreme values of a continuous target variable.

other	Namely, we propose to address such tasks by sampling approaches.

background	Several real world prediction problems involve forecasting rare values of a target variable.

result	The proposed SmoteR method can be used with any existing regression algorithm turning it into a general tool for addressing problems of forecasting rare extreme values of a continuous target variable.

result	In an extensive set of experiments we provide empirical evidence for the superiority of our proposals for these particular regression tasks.

result	We present a modification of the well-known Smote algorithm that allows its use on these regression tasks.

other	This paper describes a contribution to this type of tasks.

background	For regression tasks, where the target variable is continuous, few works exist addressing this type of problem.

background	Manufacturing organizations are able to accumulate large amounts of plant floor production and environmental data due to advances in data collection, communications technology, and use of standards.

method	This paper proposes multiple methods in which simulation can serve as a DA application or support other DA applications in manufacturing environment to address big data issues.

result	Data analytics (DA) can help understand and gain insights from the big data and in turn help advance towards the vision of smart manufacturing.

method	An example case is discussed to demonstrate one use of simulation.

other	The challenge has shifted from collecting a sufficient amount of data to analyzing and making decisions based on the huge amount of data available.

method	Modeling and simulation have been used by manufacturers to analyze their operations and support decision making.

result	In the presented case, a virtual representation of machining operations is used to generate the data required to evaluate manufacturing data analytics applications.

method	Perceived confidence seemed to play a mediating role, suggesting that attractive men write appealing texts because they are aware of their high mate value.

method	We addressed this issue by having 50 women independently rate 100 photos and free-written texts taken from males’ profiles on a popular dating website.

other	: +1 610 519 7940; fax

other	0747-5632/$ see front matter

other	2011 Elsevier Ltd. A doi:10.1016/j.chb.2011.08.023 ⇑ Corresponding author.

background	In internet dating, individuals with attractive profile photos are viewed more favorably overall, but no research has yet established whether they indeed have more positive qualities.

other	Tel.

method	Photos rated as physically attractive had profile texts that were rated as more attractive, even though photos and texts were rated by different judges.

background	E-mail address: rebecca.brand@villanova.edu (R.J. Attractive people are considered by others to have many positive qualities and in the case of social skills and intelligence, these attributions are often true.

other	Thus, contrary to popular belief, the internet does not seem to ‘‘level the playing field.

result	On the other hand, we could aggregate the evaluations from several evaluators to a single evaluation and such aggregates do rather well, even when they consist of only three to five people.

background	Heuristic evaluation is an informal method of usability analysis where a number of evaluators are presented with an interface design and asked to comment on it.

background	Four experiments showed that individual evaluators were mostly quite bad at doing such heuristic evaluations and that they only found between 20 and 51% of the usability problems in the interfaces they evaluated.

background	Advanced mobile technology continues to shape professional environments.

objective	Among the consequences are changes in technology requirements, such as the need to limit weight and size of the devices.

method	In the current paper, we focus on the factors that users find important in mobile devices.

method	Besides the practical relevance for technology developers and managers, our research results contribute to the discussion about the extent to which previously established theories of technology adoption and use are applicable to mobile technology.

method	Based on a content analysis of online user reviews that was followed by structural equation modeling, we found four factors to be significantly related with overall user evaluation, namely functionality, portability, performance, and usability.

background	Smart cell phones, pocket computers and laptop computers reduce the need of users to remain close to a wired information system infrastructure and allow for task performance in many different contexts.

result	We also discuss the methodological suitability of online user reviews for the assessment of user requirements, and the complementarity of automated and non-automated forms of content analysis.

method	In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems.

objective	To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization.

method	The Big Data governance framework presents criteria different from existing criteria at the data quality level.

method	In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services.

method	It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services.

background	Big Data services in the public sector are an inevitable choice to improve the quality of people's life.

method	This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea.

background	Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA.

method	We propose the Big Data Governance Framework in this paper.

background	While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data.

result	Matrix differentiation results are derived and summarized in tables which can be exploited in a wide range of signal processing related situations

background	A systematic theory is introduced for finding the derivatives of complex-valued matrix functions with respect to a complex-valued matrix variable and the complex conjugate of this variable.

background	In the framework introduced, the differential of the complex-valued matrix function is used to identify the derivatives of this function.

background	In general these methods require complex and inefficient pipelines for model building and fitting.

result	We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions.

background	3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty.

background	Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination.

other	Code and models will be made available at http://aaronsplace.co.uk

result	We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image.

objective	In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans.

objective	Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model.

objective	This project consists of an analysis of the state of play, a scenario-building, a gap analysis and a roadmapping activity.

background	Most of these target the next few years and limited attention has been giving to the long term.

method	It was found that a roadmapping methodology should match the unique characteristics of e-government.

result	The research shows the need of multidisciplinary research.

background	E-government research has become a recognized research domain and many policies and strategies are formulated for e-government implementations.

background	The eGovRTD2020, a European Commission co-funded project, investigated the future research on e-government driven by changing circumstances and the evolution of technology.

method	The use of this methodology has resulted in the identification of a large number of e-government research themes.

method	In this paper the roadmapping methodology fitting the unique characteristics of the e-government field is presented and the results are briefly discussed.

method	We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.

method	They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information.

objective	Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination.

background	For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques.

method	We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models.

result	In this paper, we first discuss three causes of unfairness in machine learning.

background	With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals’ lives.

background	Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on.

result	These results support the need for an increased recognition for aesthetic in the typical evaluation process of data visualization techniques.

result	Our findings demonstrate a correlation between latency in task abandonment and erroneous response time in relation to visualization's perceived aesthetic.

objective	This paper investigates the results of an online survey of 285 participants, measuring both perceived aesthetic as well as the efficiency and effectiveness of retrieval tasks across a set of 11 different data visualization techniques.

background	Aesthetic seems currently under represented in most current data visualization evaluation methodologies.

method	This study measured parameters such as speed of completion, accuracy rate, task abandonment and latency of erroneous response.

result	The data visualizations represent an identical hierarchical dataset, which has been normalized in terms of color, typography and layout balance.

result	Multiple applications, including walking path prediction, destination prediction, and tracking, demonstrate the effectiveness of Behavior-CNN on pedestrian behavior modeling.

method	A pedestrian behavior encoding scheme is designed to provide a general representation of walking paths, which can be used as the input and output of CNN.

method	The proposed Behavior-CNN is trained with real-scene crowd data and then thoroughly investigated from multiple aspects, including the location map and location awareness property, semantic meanings of learned filters, and the influence of receptive fields on behavior modeling.

objective	In this paper, a deep neural network (Behavior-CNN) is proposed to model pedestrian behaviors in crowded scenes, which has many applications in surveillance.

method	We describe a polynomial time solution to the air traffic control (ATC) problem, which is a typical C&C problem.

background	Past approaches, using multiprocessors (MP), for real-time computing have had great difficulty in meeting real problem requirements.

method	This solution uses a static, non-preemptive table driven schedule using a SIMD architecture called an associative processor (AP).

result	The AP is an ideal processor for set and database operations since its single thread instruction stream can operate on an entire set of data with each instruction.

result	The AP eliminates multi-thread instructions, which account for much of the MP intractability mentioned above.

background	A different paradigm is needed for real-time command and control (C&C) problems.

background	We review some reasons why C&C problems that require a solution on a MP architecture may be intractable, and then show an architecture where these reasons for intractability are nonexistent.

background	This paper proposed a system that helps users automatically find a free parking space at the least cost based on new performance metrics to calculate the user parking cost by considering the distance and the total number of free places in each car park.

background	This paper introduces a novel algorithm that increases the efficiency of the current cloud-based smart-parking system and develops a network architecture based on the Internet-of-Things technology.

background	This cost will be used to offer a solution of finding an available parking space upon a request by the user and a solution of suggesting a new car park if the current car park is full.

result	We also successfully implemented the proposed system in the real world.

objective	The simulation results show that the algorithm helps improve the probability of successful parking and minimizes the user waiting time.

method	To address these issues, we present a definition model to help define both cyber warfare and cyber war.

other	© 2014 Elsevier Ltd.

background	The topic of cyber warfare is a vast one, with numerous sub topics receiving attention from the research community.

background	We first examine the most basic question of what cyber warfare is, comparing existing definitions to find common ground or disagreements.

method	The paper then identifies nine research challenges in cyber warfare and analyses contemporary work carried out in each.

background	We conclude by making suggestions on how the field may best be progressed by future efforts.

other	All rights reserved.

background	We discover that there is no widely adopted definition and that the terms cyber war and cyber warfare are not well enough differentiated.

method	The resulting category scheme enables further development of solution-components in the application domain of Industry 4.0.

background	Established technologies like Internet of Things, Cloud or Big Data are propagated solution-components of Industry 4.0.

result	As a result, two general use cases for Big Data applications in Industry 4.0 were identified and characterized.

method	This research uses the method of content analysis to extract requirements of Industry 4.0 from current research publications.

background	So far, there is no founded elaboration of IT-requirements and no differentiated discussion on how solution-components fulfil these requirements.

background	Industry 4.0 stands for the 4 Industrial revolution and the new paradigm of autonomous and decentralized control in production.

background	From the IT-perspective this involves a new level of networking, data integration and data processing in production.

background	Products and production systems are enhanced to Cyber Physical Systems which have the capability to communicate with each other, to build ad-hoc networks and for self-control and self-optimization.

objective	Objective of analysis is a structured compilation of requirements regarding data processing.

result	Furthermore, this paper shows how the requirements can be matched to the capabilities of Big Data software solutions.

method	Although an exponential distribution has been used for the traffic density, it is also valid for different probability distributions for traffic densities, as well as for other significant parameters of the model.

objective	In this paper, we derive a stochastic model for the number of accidents in a platoon of vehicles equipped with a warning collision notification system, which is able to inform all the vehicles about an emergency event.

background	However, to properly develop such applications, the influence of different driving parameters on the event of vehicle collision must be assessed at an early design stage.

method	In fact, the assumption of communications being used is key to simplify the derivation of a stochastic model.

background	Improvement of traffic safety by cooperative vehicular applications is one of the most promising benefits of vehicular ad hoc networks (VANETs).

result	We validate the proposed model with Monte Carlo simulations.

method	The model enables the computation of the average number of collisions that occur in the platoon, the probabilities of the different ways in which the collisions may take place, as well as other statistics of interest.

result	With this model, one can quickly evaluate numerically the influence of different model parameters (vehicle density, velocities, decelerations, and delays) on the collision process and draw conclusions that shed relevant guidelines for the design of vehicular communication systems, as well as chain collision avoidance applications.

method	Moreover, the actual communication system employed is independent of the model since it is abstracted by a message delay variable, which allows it to be used to evaluate different communication technologies.

result	Illustrative examples of application are provided, although a systematic characterization and evaluation of different scenarios is left as future work.

other	2012 Elsevier Ltd. All rights reserved.

other	Address: University of R Engineering, Department of Enterprise Engineering, Rome, Italy.

method	We utilize a multi-criteria approach combined with fuzzy linguistic variables, in the variation of the 2-tuple, creating a hierarchy of CSR components with the purpose of integrating financial and non-financial sustainability dimensions and strategic perspectives.

background	‘‘If a tree falls in the forest and no one is around to hear it, does it make a sound?

other	0957-4174/$ see front matter 2012 Elsevier Ltd. A http://dx.doi.org/10.1016/j.eswa.2012.07.028 ⇑ Corresponding author.

other	Tel: +39 0672597799; fax: +39 06725979 E-mail addresses: roberta.costa@uniroma2.it (R. C oma2.it (T. Menichini).

result	’’ and, paraphrasing the proverbial philosophy question, if a company has a strong CSR commitment but nobody recognizes it, does it produce any benefits?

result	The hierarchy provides a multidimensional model that allows to evaluate the multifaceted social behavior of a company: the same company can be perceived simultaneously as responsible or irresponsible depending on the considered dimension and perspective.

objective	In this paper the analysis of CSR activities, as perceived by stakeholders, is realized utilizing global reporting initiative (GRI) indicators structured under balanced scorecard (BSC) perspectives and sustainability dimensions.

background	Business returns from corporate social responsibility (CSR) practices, such as customers loyalty and company reputation, depend heavily on how stakeholders perceive the company social behavior, making the measure of stakeholder perception a key issue in the process of CSR assessment.

objective	In this paper, we review the user satisfaction research in the IS field.

background	User satisfaction with information systems (IS) is considered an important indicator of information systems success and has been the subject of numerous research studies since the field’s inception.

method	We also discuss how the study of user satisfaction and use of the construct in IS research has evolved and matured over time.

result	Finally, we discuss antecedents and outcomes of user satisfaction identified in IS research and provide suggestions for future research.

method	We discuss the roots of user satisfaction research as it pertains to satisfaction studies in marketing research and how these studies have been used to inform the IS context.

method	Specifically, we describe a passive attack, in which an attacker can find the PIN used during the pairing process.

background	This paper describes the implementation of an attack on the Bluetooth security mechanism.

method	We then describe the cracking speed we can achieve through three optimizations methods.

result	Our results show that a 4-digit PIN can be cracked in less than 0.3 sec on an old Pentium III 450MHz computer, and in 0.06 sec on a Pentium IV 3Ghz HT computer.

method	Our fastest optimization employs an algebraic representation of a central cryptographic primitive (SAFER+) used in Bluetooth.

method	Unlike the traditional methods, we investigated a scheme for classifying musical instruments using the learned features from CNNs.

method	Consequently, we fed the characteristic timbre of the particular instrument into a neural network, which cannot be extracted using a phase-blinded representations such as a spectrogram.

result	By combining our proposed MRPs and spectrogram images with a multi-column network, the performance of our proposed classifier system improves over a system that uses only a spectrogram.

result	Furthermore, the proposed classifier also outperforms the baseline result from traditional handcrafted features and classifiers.

background	A new musical instrument classification method using convolutional neural networks (CNNs) is presented in this paper.

method	To create the learned features from CNNs, we not only used a conventional spectrogram image, but also proposed multiresolution recurrence plots (MRPs) that contain the phase information of a raw input signal.

method	The considered optimization problem has applications in control, for example in ℓ1 regularized MPC.

method	The ADMM algorithm is easy to implement, converges fast to a solution of moderate accuracy, and enables separation of the optimization problem into sub-problems that may be solved in parallel.

background	We present an Alternating Direction Method of Multipliers (ADMM) algorithm for solving optimization problems with an ℓ1 regularized least-squares cost function subject to recursive equality constraints.

method	We show that the most costly step of the proposed ADMM algorithm is equivalent to solving an LQ regulator problem with an extra linear term in the cost function, a problem that can be solved efficiently using a Riccati recursion.

result	The numerical examples confirm fast convergence to sufficient accuracy and a linear complexity in the MPC prediction horizon.

method	We apply the ADMM algorithm to an example of ℓ1 regularized MPC.

background	Information technology should have much to offer linguistics, not only through the opportunities offered by large-scale data analysis and the stimulus to develop formal computational models, but through the chance to use language in systems for automatic natural language processing.

background	The paper discusses these possibilities in detail, and then examines the actual work that has been done.

objective	There are different reasons for this, and not all good ones: information technology deserves more attention from linguists.

objective	It is evident that this has so far been primarily research within a new field, computational linguistics, which is largely motivated by the demands, and interest, of practical processing systems, and that information technology has had rather little influence on linguistics at large.

background	We consider learning representations of entities and relations in KBs using the neural-embedding approach.

method	Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq ^ CityInCountrypb, cq ùñ Nationalitypa, cq.

method	Under this framework, we compare a variety of embedding models on the link prediction task.

objective	We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions.

result	We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication.

result	More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art confidence-based rule mining approach in mining horn rules that involve compositional reasoning.

method	We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase).

background	The multitude of tasks that are needed to execute a software lifecycle can overwhelm SQA resources making selection of tasks to audit similar to shooting at a moving target, blindfolded.

result	The result is a sample of process tasks which is truly representative ofthe work performed.

method	This method combines standard statistical sampling techniques with risk analysis to determine and appropriate sampling size.

background	The Mission Operations Directorate In foramtion System (MODI$) Group, an organization within NASA's Johnson Space Center, has developed a statistical method to determine a representative sample size for the number of process tasks to be audited by SQA.

background	Soflware Quality Assurance (SQA) organizations are responsible for assuring that defined processes are followed by project teams.

method	The performance of the proposed method is assessed in the face recognition problem under different challenges.

method	Other applications and several extensions are also discussed

objective	This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features.

method	The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor.

result	Due to the complexity of stock market data, development of efficient models for predicting is very difficult.

method	This study attempted to develop two efficient models and compared their performances in predicting the direction of movement in the daily Istanbul Stock Exchange (ISE) National 100 Index.

result	Experimental results showed that average performance of ANN model (75.74%) was found significantly better than that of SVM model (71.52%).

method	The models are based on two classification techniques, artificial neural networks (ANN) and support vector machines (SVM).

other	Tel.

other	: +90 332 2234350; fax E-mail address: melekacar@yahoo.com (M. Acar B Prediction of stock price index movement is regarded as a challenging task of financial time series prediction.

method	Two comprehensive parameter setting experiments for both models were performed to improve their prediction performances.

method	Ten technical indicators were selected as inputs of the proposed models.

other	0957-4174/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.eswa.2010.10.027 ⇑ Corresponding author.

result	An accurate prediction of stock price movement may yield profits for investors.

result	The proposed model is expected to report more realistic coding efficiency as PSNR is not always correlated with perceived visual quality.

method	To consider the intrinsic nature of bounded rating scales, a logistic function is used to fit the rate-distortion (R-D) values.

background	However, this model might not be an accurate predictor of the true coding efficiency as it relies on PSNR measurements.

method	We call this approach Subjective Comparison of ENcoders based on fItted Curves (SCENIC).

objective	Therefore, in this paper, we propose a model to calculate the average coding efficiency based on subjective quality scores, i.e., mean opinion scores (MOS).

method	The average MOS and bit rate differences are computed between the fitted R-D curves.

background	The Bjøntegaard model is widely used to calculate the coding efficiency between different codecs.

method	The statistical property of subjective scores is considered to estimate corresponding confidence intervals on the calculated average MOS and bit rate differences.

objective	Objective procedures to derive the true optimal grid resolution that maximizes the predictive capabilities or information content of a map are further discussed.

objective	This paper discusses empirical and analytical rules to select a suitable grid resolution for output maps and based on the inherent properties of the input data.

background	The choice of grid resolution was related with the cartographic and statistical concepts: scale, computer processing power, positional accuracy, size of delineations, inspection density, spatial autocorrelation structure and complexity of terrain.

method	This methodology can now be integrated within a GIS package to help inexperienced users select a suitable grid resolution without doing extensive data preprocessing.

method	These were further related with the concepts from the general statistics and information theory such as Nyquist frequency concept from signal processing and equations to estimate the probability density function.

method	Selection of grid resolution was demonstrated using four datasets: (1) GPS positioning data— the grid resolution was related to the area of circle described by the error radius, (2) map of agricultural plots—the grid resolution was related to the size of smallest and narrowest plots, (3) point dataset from soil mapping—the grid resolution was related to the inspection density, nugget variation and range of spatial autocorrelation and (4) contour map used for production of digital elevation model—the grid resolution was related with the spacing between the contour lines i.e. complexity of terrain.

method	Three standard grid resolutions for output maps were finally recommended: (a) the coarsest legible grid resolution—this is the largest resolution that we should use in order to respect the scale of work and properties of a dataset; (b) the finest legible grid resolution—this is the smallest grid resolution that represents 95% of spatial objects or topography; and (c) recommended grid resolution—a compromise between the two.

method	One should at least try to avoid using resolutions that do not comply with the effective scale or inherent properties of the input dataset.

other	r 2005 Elsevier Ltd.

method	It was concluded that no ideal grid resolution exists, but rather a range of suitable resolutions.

other	Source code and animations The code used to generate this guide along with its figures is available on GitHub.

other	2 There the reader can also find an animated version of the figures.

objective	We did our best to be as precise, informative and up to the point as possible, but should there be anything you feel might be an error or could be rephrased to be more precise or com-prehensible, please don't refrain from contacting us.

background	Special thanks to Ethan Schoonover, creator of the Solarized color scheme, 1 whose colors were used for the figures.

background	Feedback Your feedback is welcomed!

background	All models are wrong, but some are useful.

method	Likewise, drop us a line if you think there is something that might fit this technical report and you would like us to discuss – we will make our best effort to update this document.

background	2 Acknowledgements The authors of this guide would like to thank David Warde-Farley, Guillaume Alain and Caglar Gulcehre for their valuable feedback.

background	The dominant approach for many NLP tasks are recurrent neura l networks, in particular LSTMs, and convolutional neural networks.

background	We present a new archite ctur for text processing which operates directly on the character level and uses o nly small convolutions and pooling operations.

background	However , these architectures are rather shallow in comparison to the deep convolutional n etworks which are very successful in computer vision.

objective	We are able to show that the performa nce of this model increases with the depth: using up to 29 convolutional layer s, we report significant improvements over the state-of-the-art on several public t ext classification tasks.

result	To the best of our knowledge, this is the first time that very de ep convolutional nets have been applied to NLP.

background	This approach, however, makes cryptocurrency platforms step away from their original purpose and enter the domain of database-replication protocols, notably, the classical state-machine replication, and in particular its Byzantine fault-tolerant (BFT) variants.

background	Bitcoin became a success story, despite its consensus latencies on the order of an hour and the theoretical peak throughput of only up to 7 transactions per second.

background	In the early days of Bitcoin, the performance of its probabilistic proof-of-work (PoW) based consensus fabric, also known as blockchain, was not a major issue.

method	We also discuss recent proposals to overcoming these scalability limits and outline key outstanding open problems in the quest for the “ultimate” blockchain fabric(s).

background	The situation today is radically different and the poor performance scalability of early PoW blockchains no longer makes sense.

background	Bitcoin cryptocurrency demonstrated the utility of global consensus across thousands of nodes, changing the world of digital transactions forever.

method	Specifically, the trend of modern cryptocurrency platforms, such as Ethereum, is to support execution of arbitrary distributed applications on blockchain fabric, needing much better performance.

method	In this paper, we contrast PoW-based blockchains to those based on BFT state machine replication, focusing on their scalability limits.

method	Additionally to our comparative analysis, we present an original evaluation about the most common assumptions in recent OR/MS literature in DOM.

objective	Our purpose is to evaluate how OR/MS research in DOM has evolved in the last years and to what extent the gaps identified by Altay and Green (2006) have been covered.

background	Our work is a continuation of a previous review from Altay and Green (2006).

background	Potential consequences of disasters involve overwhelming economic losses, large affected populations and serious environmental damages.

background	Given these devastating effects, there is an increasing interest in developing measures in order to diminish the possible impact of disasters, which gave rise to the field of disaster operations management (DOM).

result	Based on our findings we provide future research directions in order to make improvements in the areas where lack of research is detected.

background	In this paper we review recent OR/MS research in DOM.

background	Our findings show no drastic changes or developments in the field of OR/MS in DOM since the publication of Altay and Green (2006).

method	We look at hierarchical self-organizing maps, and mixture models.

other	C © 2011 Wiley Periodicals, Inc.

method	We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments.

method	We review grid-based clustering, focusing on hierarchical density-based approaches.

method	Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.

method	Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors.

background	The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data.

method	This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks.

method	This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.

method	The receiver together with an external baseband processor detects the distance and relative speed by conducting an FFT-based algorithm.

background	A fully-integrated FMCW radar system for automotive applications operating at 77 GHz has been proposed.

result	Millimeter-wave PA and LNA are incorporated on chip, providing sufficient gain, bandwidth, and sensitivity.

result	Fabricated in 65-nm CMOS technology, this prototype provides a maximum detectable distance of 106 meters for a mid-size car while consuming 243 mW from a 1.2-V supply.

background	Utilizing a fractional- synthesizer as the FMCW generator, the transmitter linearly modulates the carrier frequency across a range of 700 MHz.

background	This assay allows for observing the healing process in vitro in which the cells on the edges of the artificial wound migrate toward the wound area.

method	In this paper, we present an application of automatic cell tracking in phase-contrast microscopy images to wound healing assay.

method	Our cell tracking system can track individual cells during the healing process and provide detailed spatio-temporal measurements of cell behaviors.

background	For further investigation, more detailed measurements of the cell behaviors are required.

result	The application demonstrates the effectiveness of automatic cell tracking for quantitative and detailed analysis of the cell behaviors in wound healing assay in vitro.

background	The influence of different culture conditions can be measured by observing the change in the size of the wound area.

method	The cell behaviors under three different culture conditions have been analyzed.

background	The wound healing assay in vitro is widely used for research and discovery in biology and medicine.

method	To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner.

objective	Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements.

result	In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost.

objective	Email: harshitg@gatech.edu Summary Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications.

objective	However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances.

method	To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion.

method	The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput.

method	To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed.

method	We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies.

background	Correspondence Harshit Gupta, School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA.

background	This class ofMCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available.

background	Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complexmodels.

method	The lack of a domain specific language allows for great flexibility and direct interaction with the model.

method	This paper is a tutorial-style introduction to this software package.

background	Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models.

result	Subjects Data Mining and Machine Learning, Data Science, Scientific Computing and Simulation

background	Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code.

background	PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed.

background	This paper presents a method, Clustering-Based SVM (CB-SVM), that maximizes the SVM performance for very large data sets given a limited amount of resource, e.g., memory.

method	Unlike traditional pattern recognition and machine learning, real-world data mining applications often involve huge numbers of data records.

result	These samples carry statistical summaries of the data and maximize the benefit of learning.

other	Our experiments on synthetic and real-world data sets show that CB-SVM is highly scalable for very large data sets and very accurate in terms of classification.

result	Our analyses show that the training complexity of CB-SVM is quadratically dependent on the number of support vectors, which is usually much less than that of the entire data set.

background	Support vector machines (SVMs) have been promising methods for classification and regression analysis due to their solid mathematical foundations, which include two desirable properties: margin maximization and nonlinear classification using kernels.

result	CB-SVM applies a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples.

background	However, despite these prominent properties, SVMs are usually not chosen for large-scale data mining problems because their training complexity is highly dependent on the data set size.

background	Thus it is too expensive to perform multiple scans on the entire data set, and it is also infeasible to put the data set in memory.

result	A supervised deep convolutional neural network is trained to classify each image patch in the collected images.

objective	However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavement and possible shadows with similar intensity.

background	Inspired by recent success on applying deep learning to computer vision and medical problems, a deep-learning based method for crack detection is proposed in this paper.

result	Quantitative evaluation conducted on a data set of 500 images of size 3264 χ 2448, collected by a low-cost smart phone, demonstrates that the learned deep features with the proposed deep learning framework provide superior crack detection performance when compared with features extracted with existing hand-craft methods.

method	Automatic detection of pavement cracks is an important task in transportation maintenance for driving safety assurance.

method	In this treatment, the multiple observation probability is expressed as a combination of individual observation probabilities without losing generality.

background	On the other hand, as these models have a complex structure and also because the involved data sets usually contain uncertainty, it is difficult to analyze the multiple observation training problem without certain assumptions.

method	By generalizing Baum's auxiliary function into this framework and building up an associated objective function using the Lagrange multiplier method, it is proven that the derived training equations guarantee the maximization of the objective function.

objective	This paper presents a formal treatment of HMM multiple observation training without imposing the above assumption.

result	Furthermore, we show that Levinson's training equations can be easily derived as a special case in this treatment.

background	They have been applied in speech recognition and handwriting recognition because of their great adaptability and versatility in handling sequential signals.

method	This combinatorial method gives one more freedom in making different dependence-independence assumptions.

background	For many years researchers have used Levinson's training equations in speech and handwriting applications, simply assuming that all observations are independent of each other.

background	ÐHidden Markov models (HMMs) are stochastic models capable of statistical learning and classification.

background	Index TermsÐHidden Markov model, forward-backward procedure, Baum-Welch algorithm, multiple observation training.

background	Apparently, there is still a lack of acceptance of mobile payment services among consumers.

result	The empirical results show particularly strong support for the effects of compatibility, individual mobility, and subjective norm.

background	However, mobile payment is surprisingly not among the frequently used mobile services, although technologically advanced solutions exist.

result	Our study offers several implications for managers in regards to marketing mobile payment solutions to increase consumers’ intention to use these services.

background	Mobile technology has become increasingly common in today’s everyday life.

other	2009 Elsevier B.V. All rights reserved.

method	The conceptual model developed and tested in this research thus focuses on factors determining consumers’ acceptance of mobile payment services.

result	Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks.

background	Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively.

result	A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.

background	Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs.

method	Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected.

method	In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results.

result	We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features.

result	In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network.

background	Computer graphics research has concentrated on creating photo-realistic images of synthetic objects.

method	These renderings are traditionally represented by a rectangular array of pixels that tile the image plane.

result	These abstract images filter and refine visual information before it is presented to the viewer.

result	By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.

method	As an alternative to photo-realism, it is possible to create abstract images using an ordered collection of brush strokes.

objective	These images communicate surface shading and curvature, as well as the depth relationships of objects in a scene.

background	Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs.

method	We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps.

result	We have also open-sourced our implementation in TensorFlow.

background	With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features.

objective	In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems.

background	Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort.

result	Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models.

background	However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.

method	The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed.

objective	In this paper we discuss the development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis.

result	The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying datasets.

background	The sender and receiver for each transaction are identified only by cryptographic publickey ids.

background	This leads to a common misconception that it inherently provides anonymous use.

background	Bitcoins have recently become an increasingly popular cryptocurrency through which users trade electronically and more anonymously than via traditional electronic transfers.

background	While Bitcoin’s presumed anonymity offers new avenues for commerce, several recent studies raise user-privacy concerns.

method	(ii) We run the annotated graph through our graph-analysis framework to find and summarize activity of both known and unknown users.

background	Bitcoin’s design keeps all transactions in a public ledger.

method	Our approach is two-fold: (i) We annotate the public transaction graph by linking bitcoin public keys to real people either definitively or statistically.

objective	We explore the level of anonymity in the Bitcoin system.

method	We expect that the OpenSGX framework can serve as an open platform for SGX research, with the following contributions.

background	Hardware technologies for trusted computing, or trusted execution environments (TEEs), have rapidly matured over the last decade.

background	In fact, TEEs are at the brink of widespread commoditization with the recent introduction of Intel Software Guard Extensions (Intel SGX).

result	Second, to show OpenSGX’s use cases, we applied OpenSGX to protect sensitive information (e.g., directory) of Tor nodes and evaluated their potential performance impacts.

objective	To address this problem, we develop an open source platform, called OpenSGX, that emulates Intel SGX hardware components at the instruction level and provides new system software components necessarily required for full TEE exploration.

method	OpenSGX provides a platform for SGX development, meaning that it provides not just emulation but also operating system components, an enclave program loader/packager, an OpenSGX user library, debugging, and performance monitoring.

background	Despite such rapid development of TEE, software technologies for TEE significantly lag behind their hardware counterpart, and currently only a select group of researchers have the privilege of accessing this technology.

method	First, we develop a fully functional, instruction-compatible emulator of Intel SGX for enabling the exploration of software/hardware design space, and development of enclave programs.

result	Therefore, we believe OpenSGX has great potential for broader communities to spark new research on soon-to-becommodity Intel SGX.

objective	This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification.

method	We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results.

result	Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.

result	PublishedIn: Pattern Recognition

method	This section also suggests possible ways of improving speech emotion recognition systems.

background	Conclusions about the performance and limitations of current speech emotion recognition systems are discussed in the last section of this survey.

background	This paper is a survey of speech emotion classification addressing three important aspects of the design of a speech emotion recognition system.

background	The second issue is the design of an appropriate classification scheme and the third issue is the proper preparation of an emotional speech database for evaluating system performance.

background	Recently, increasing attention has been directed to the study of the emotional content of speech signals, and hence, many systems have been proposed to identify the emotional content of a spoken utterance.

background	The first one is the choice of suitable features for speech representation.

result	Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.

method	We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels.

result	Due to their increased parallelism, they are up to 16 times faster at train and test time.

background	Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences.

method	Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size.

result	We believe that these findings provide new insights for viral marketing and suggest that topological measures such as indegree alone reveals very little about the influence of a user.

method	Third, influence is not gained spontaneously or accidentally, but through concerted effort such as limiting tweets to a single topic.

method	We make several interesting observations.

objective	First, popular users who have high indegree are not necessarily influential in terms of spawning retweets or mentions.

background	In this paper, using a large amount of data collected from Twitter, we present an in-depth comparison of three measures of influence: indegree, retweets, and mentions.

background	Directed links in social media could represent anything from intimate friendships to common interests, or even a passion for breaking news or celebrity gossip.

background	Such directed links determine the flow of information and hence indicate a user’s influence on others—a concept that is crucial in sociology and viral marketing.

result	Second, most influential users can hold significant influence over a variety of topics.

objective	Based on these measures, we investigate the dynamics of user influence across topics and time.

objective	This paper aims to examine how DT methods and tools foster innovation in teams.

result	A case study approach, based on two workshops, examined three DT methods with a software tool.

result	The paper proposes guidelines for utilizing DT methods and tools in innovation

result	The findings support the use of DT methods and tools as a way of incubating ideas and creating innovative solutions within teams when team collaboration and software limitations are balanced.

background	Understanding how innovation within teams can be supported by DT methods and tools captivates the interest of business communities.

background	Design thinking (DT) is regarded as a system of three overlapping spaces—viability, desirability, and feasibility—where innovation increases when all three perspectives are addressed.

objective	In this paper, we propose an automated neural architecture search approach for designing resourceconstrained mobile CNN models.

result	Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks.

method	Unlike in previous work, where mobile latency is considered via another, often inaccurate proxy (e.g., FLOPS), in our experiments, we directly measure real-world inference latency by executing the model on a particular platform, e.g., Pixel phones.

background	Designing convolutional neural networks (CNN) models for mobile devices is challenging because mobile models need to be small and fast, yet still accurate.

result	On the ImageNet classification task, our model achieves 74.0% top-1 accuracy with 76ms latency on a Pixel phone, which is 1.5× faster than MobileNetV2 (Sandler et al. 2018) and 2.4× faster than NASNet (Zoph et al. 2018) with the same top-1 accuracy.

method	To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that permits layer diversity throughout the network.

background	Although significant effort has been dedicated to design and improve mobile models on all three dimensions, it is challenging to manually balance these trade-offs when there are so many architectural possibilities to consider.

result	On the COCO object detection task, our model family achieves both higher mAP quality and lower latency than MobileNets.

objective	We propose to explicitly incorporate latency information into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency.

background	But these are labor-intensive for large and complicated systems thereby error prone.

method	The proposed multibody dynamics model of the mechanism offers an accurate and fast method to analyze the dynamics of the mechanism knowing that there is no such work available for scissor lifts.

method	In this work, the mechanism is modeled and simulated in order to evaluate several application-specific requirements such as dynamics, position accuracy etc.

method	Here the multibody dynamics model of the mechanism is developed in bond graph formalism because it offers flexibility for modeling of closed loop kinematic systems without any causal conflicts and control laws can be included.

method	There are several procedures for deriving dynamic equations of rigid bodies in classical mechanics (i.e. Classic Newton-D'Alembert, Newton-Euler, Lagrange, Hamilton, kanes to name a few).

method	Scissor lifting mechanism is the first choice for automobiles and industries for elevation work.

background	This paper describes the implementation of general multibody system dynamics on Scissor lift Mechanism (i.e. four bar parallel mechanism) within a bond graph modeling framework.

background	The system has a one degree of freedom.

result	The simulation gives a clear idea about motor torque sizing for different link lengths of the mechanism over a linear displacement.

background	Android is a modern and popular software platform for smartphones.

objective	However, in this paper we show that a privilege escalation attack is possible.

background	This allows developers and users to restrict the execution of an application to the privileges it has (mandatorily) assigned at installation time.

result	Our results immediately imply that Android’s security model cannot deal with a transitive permission usage attack and Android’s sandbox model fails as a last resort against malware and sophisticated runtime attacks.

background	The exploitation of vulnerabilities in program code is hence believed to be confined within the privilege boundaries of an application’s sandbox.

objective	We show that a genuine application exploited at runtime or a malicious application can escalate granted permissions.

background	Among its predominant features is an advanced security model which is based on application-oriented mandatory access control and sandboxing.

method	Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework.

method	In this paper, we utilize recurrent neural networks (RNNs) to address this problem.

background	While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image.

background	Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results.

result	Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models.

method	These techniques, although working well, fail to explicitly exploit the label dependencies in an image.

background	This approach has limited the extent to which we understand how these diagrams are involved in biological reasoning.

background	These studies, however, proceed from identification of diagrams on the basis of their spare visual appearance, and do not draw on a foundational theory of the nature of diagrams as representations.

background	Biologists depend on visual representations, and their use of diagrams has drawn the attention of philosophers, historians, and sociologists interested in understanding how these images are involved in biological reasoning.

objective	The features that make these figures distinctive as representational types, furthermore, illuminate the ways in which they are involved in biological reasoning.

objective	In this paper I characterize three different kinds of figures among those previously identified as diagrams.

background	Autonomously traversing rough terrain and such obstacles while ensuring the safety of the robot is a challenging task in mobile robotics.

method	We first use the completeness of a graph search on a regular grid to quickly find an initial path in a low dimensional space, considering only the platform's operating limits instead of the complete state.

method	We then take this initial path to focus the RRT* search in the continuous high-dimensional state space including the actuators of the robot.

method	We do not rely on a detailed structure/terrain classification or use any predefined motion sequences.

result	Simulation results prove our method to be effective in solving planning queries in such environments.

background	Mobile robots with reconfigurable chassis are able to traverse unstructured outdoor environments with boulders or rubble, and overcome challenging structures in urban environments, like stairs or steps.

method	Hence, our planner can be applied to urban structures, like stairs, as well as rough unstructured environments.

objective	In this paper we introduce a two-phase motion planning algorithm for actively reconfigurable tracked robots.

method	In the context of efficiency, it provides sufficient conditions for correctness of an initial program which may subsequently be improved for efficiency while preserving correctness.

background	It is of interest to programming methodologists because (1) correctness of backtracking programs may be difficult to ascertain experimentally and (2) efficiency is often of paramount importance.

method	The value of control structure abstraction in the context of correctness is that proofs of general properties of a class of programs with similar control structures are separated from proofs of specific properties of individual programs of the class.

background	Backtracting is a well-known technique for solving combinatorial problems.

background	This paper applies a programming methodology, which we call control structure abstraction, to the backtracking technique.

method	This paper provides a survey on state-of-the-art multi-output regression methods, that are categorized as problem transformation and algorithm adaptation methods.

background	In recent years, a plethora of approaches have been proposed to deal with the increasingly challenging task of multi-output regression.

method	In addition, we present the mostly used performance evaluation measures, publicly available data sets for multi-output regression real-world problems, as well as open-source software frameworks.

background	However, little is known about the PM usage in health care since the constituent research about PM is primarily focussed on the industrial sector.

method	For this purpose, an exploratory survey for the health care sector is presented.

background	In today’s fast changing health care sector, decision makers are facing a growing demand for both clinical and administrative information in order to comply with legal and customer-specific requirements.

background	Performance Management (PM) is thus becoming increasingly important to catch up with the rising informational demands.

background	This article discusses the issues that arise in the design and implementation of expert systems.

background	These issues include: task selection; the stages of development of expert system projects; knowledge acquisition; languages and tools; development and run-time environments; and organizational and institutional issues.

result	The article closes with some speculation about the future development of expert systems.

result	Experimental results demonstrate excellent voltage regulation, insensitivity to load variations, and low output voltage distortion as well as the stability of the system under both linear and nonlinear loads.

background	This approach relies only on a single output voltage measurement to reduce the system cost as well as measurement noise and disturbance injected by output current and/or inductor current measurements.

method	A Lyapunov stability analysis is utilized to demonstrate system stability.

background	This paper presents a filter-based control scheme for an H-Bridge inverter with output LC filter.

background	To reduce the controller sensitivity to the system parameters, the proposed controller is developed for unknown system parameters.

method	Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper.

result	Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27% global accuracy, 48.30% average class accuracy and 37.29% average intersectionover-union score.

method	Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs).

objective	In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data.

method	With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy.

method	Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation.

method	The benefit is an easier system to build (no need for hand-crafting features and classifiers).

background	Unfortunately, these methods require vast experience to design the classifier including one or more of the popular computer vision feature descriptors.

background	General social media networks are striving to isolate adult images and videos from normal ones.

background	Intelligent image analysis methods can help to automatically detect and isolate questionable images in media.

method	We propose to build a classifier based on one of the recently flourishing deep learning techniques.

background	It is no secret that pornographic material is now a one-clickaway from everyone, including children and minors.

method	Convolutional neural networks contain many layers for both automatic features extraction and classification.

result	Additionally, our experiments show that it is even more accurate than the state of the art methods on the most recent benchmark dataset.

method	We focus on the latest 3GPP standards and review relevant research papers, and propose three key aspects of online charging, with respect to information utilization: a) signaling aspect, b) inter-domain aspect, and c) serviceand component-based aspect.

other	Copyright c © 2012 John Wiley & Sons, Ltd.

objective	We do not attempt to predict which information will be utilized in such systems and for what purpose, but instead we summarize open issues in view of the emerging trend of exploiting the user, network and service related information in service provisioning.

objective	This work is motivated by the challenge to identify which information is used, and how it is used in online charging related processes, and also to explore whether it could be utilized “better” or “smarter” to improve future online charging systems functionality.

background	Modern charging systems use user, network, and service related information when performing online charging.

background	Compared, however, to the overall information available and used in network management processes as a whole, charging systems only use a limited subset.

method	We present a state of the art review by grouping the works found in the literature based on the aspects they are associated with, and compare them based on proposed comparison criteria.

result	The discussion presented at the end of the paper indicates three common open issues, namely: 1) lack of common charging information specification and structure, 2) lack of mechanisms for information sharing between stakeholders in the service delivery process, and 3) lack of a common framework for sharing information while protecting user privacy.

method	Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge.

result	A significant performance gain on classification of all diagnosis groups was achieved in our experiments.

method	In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of AD and its prodromal stage, Mild Cognitive Impairment (MCI).

background	The accurate diagnosis of Alzheimer's disease (AD) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped.

background	Although many studies have applied machine learning methods for computer-aided-diagnosis (CAD) of AD recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models.

method	We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.

method	We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1

background	Human evaluations can take months to finish and involve human labor that can not be reused.

background	Human evaluations of machine translation are extensive but expensive.

background	Based on the real data of a Chinese commercial bank’s credit card, in this paper, we classify the credit card customers into four classifications by K-means.

result	The information is not only helpful for the bank to understand related characteristics of different customers, but also marketing representatives to find potential customers and to implement target marketing.

method	Then we built forecasting models separately based on four data mining methods such as C5.0, neural network, chi-squared automatic interaction detector, and classification and regression tree according to the background information of the credit cards holders.

method	Conclusively, we obtain some useful information of decision tree regulation by the best model among the four.

background	Over the last ten years, argumentation has come to be increasingly central as a core study within Artificial Intelligence (AI).

objective	Our aims in this introduction are, firstly, to place these contributions in the context of the historical foundations of argumentation in AI and, subsequently, to discuss a number of themes that have emerged in recent years resulting in a significant broadening of the areas in which argumentation based methods are used.

background	The articles forming this volume reflect a variety of important trends, developments, and applications covering a range of current topics relating to the theory and applications of argumentation.

other	All rights reserved.

method	We continue by outlining how a number of foundational contributions provided the basis for the formulation of argumentation models and their promotion in AI related settings and then consider a number of new themes that have emerged in recent years, many of which provide the principal topics of the research presented in this volume.

other	© 2007 Elsevier B.V.

method	We begin by presenting a brief overview of the issues of interest within the classical study of argumentation: in particular, its relationship— in terms of both similarities and important differences—to traditional concepts of logical reasoning and mathematical proof.

method	Qualitative data analysis suggests that participants attended to small cues online, mediated the tension between impression management pressures and the desire to present an authentic sense of self through tactics such as creating a profile that reflected their "ideal self," and attempted to establish the veracity of their identity claims.

result	This study provides empirical support for Social Information Processing theory in a naturalistic context while offering insight into the complicated way in which "honesty" is enacted online.

background	Thirty-four individuals active on a large online dating site participated in telephone interviews about their online dating experiences and perceptions.

objective	This study investigates self-presentation strategies among online dating participants, exploring how participants manage their online presentation of self in order to accomplish the goal of finding a romantic partner.

background	The widespread deployment and adoption of the Dynamic Adaptive Streaming over HTTP (DASH) standard is making Internet video-on-demand a `standard' Internet application similar in impact as email and web browsing.

result	In one network scenario, we observed that a backlogged TCP flow achieved a throughput of 6 Mbps while a Netflix session (under similar path conditions) consumed less than 3 Mbps of bandwidth.

result	However, the application algorithm is clearly intertwined with the underlying TCP mechanisms during periods of volatile network conditions.

background	While video streaming has been widely deployed and studied for decades, DASH-based streaming is very different as it involves adaptation both by the application and by TCP.

result	Our results suggest that Netflix adaptation defaults to underlying TCP mechanisms during periods of heavy, sustained network congestion.

objective	The contribution of the research presented in this paper is twofold: first, we characterize the bandwidth consumption of a widely deployed DASH application (i.e., Netflix); second, we provide insight in how different implementations and different access networks can impact bandwidth consumption.

background	The dynamics and implications of multiple levels of end-to-end congestion control are not well understood.

result	Implications for future research on SNS users’ goal-directed consumption behaviors are discussed.

method	Applying uses and gratifications theory (UGT), this study examined consumers’ use of one of four social networking sites (SNSs): Facebook, Twitter, Instagram, or Snapchat, for following brands, and their influence on brand community-related outcomes.

result	Results (N = 297) indicated Snapchat users scored highest for passing time, sharing problems, and improving social knowledge, while Instagram users scored highest for showing affection, following fashion, and demonstrating sociability.

result	Attention to social comparison, SNS trust, tie strength, and homophily also significantly moderated the relationship between frequent use of each SNS to follow brands, and brand community-related outcomes.

result	Twitter users had highest brand community identification and membership intention, while Instagram users had highest brand community engagement and commitment.

method	Subsequently, evidence of reliability and validity on the basis of analyzing data from a quota sample of 260 adult respondents is presented.

result	Finally, this paper concludes by discussing limitations that could be addressed in future studies.

background	The e-commerce literature has rarely addressed the measurement of customer perceptions of website service quality in digital marketing environments.

objective	This article validates and refines a comprehensive model and instrument for measuring customer-perceived service quality of websites that market digital products and services.

method	After a discussion of the conceptualization and operationalization of the service quality construct, the procedure used in modifying items, collecting data, and validating a multiple-item scale is described.

result	The final EC-SERVQUAL instrument with good reliability and validity will be essential to the development and testing of e-business theories, and provide researchers with a common framework for explaining, justifying, and comparing differences across results.

background	It is argued that the current SERVQUAL and IS-SERVQUAL instruments need to be refined and validated to fit the digital marketing environment, as they are targeted primarily towards either traditional retailing or information systems contexts.

method	Implications for practice and research are then explored.

background	In Europe, SMEs are a vital part of the economy, and the challenges they encounter need to be addressed as a matter of urgency.

background	Small and medium enterprises (SMEs) have proved themselves to be slow adopters of the new technology of big data analytics and are in danger of being left behind.

method	It considers the ‘state-of-the-art’ of IT with respect to usability and usefulness for SMEs and discusses how SMEs can overcome the barriers preventing them from adopting existing solutions.

background	Big data is big news, and large companies in all sectors are making significant advances in their customer relations, product selection and development and consequent profitability through using this valuable commodity.

other	Copyright © 2016 John Wiley & Sons, Ltd.

objective	This paper identifies barriers to SME uptake of big data analytics and recognises their complex challenge to all stakeholders, including national and international policy makers, IT, business management and data science communities.

method	The paper then considers management perspectives and the role of maturity models in enhancing and structuring the adoption of data analytics in an organisation.

method	The history of total quality management is reviewed to inform the core aspects of implanting a new paradigm.

result	The paper concludes with recommendations to help SMEs develop their big data capability and enable them to continue as the engines of European industrial and business success.

objective	The paper proposes a big data maturity model for SMEs as a first step towards an SME roadmap to data analytics.

method	The module has abilities of high-tension measurability and flexible tension control.

result	Furthermore, utilizing the module advantage of design facilitation, we made two types of tendon-driven robots and confirmed effectiveness of the module.

method	In order to achieve flexible tension control, we developed the new tension measurement mechanism with high-tension measurability and the new motor driver which enables current based motor control.

result	We demonstrate the tension control ability of the module by several experiments.

objective	We propose a sensor-driver integrated muscle module by integrating necessarily components for tendon-driven robot which is likely to complicate.

background	Although our study confirms that content from fake news websites is increasing, these sites do not exert excessive power.

background	Fake news coverage itself is diverging and becoming more autonomous topically.

background	This study examines the agenda-setting power of fake news and fact-checkers who fight them through a computational look at the online mediascape from 2014 to 2016.

background	Emerging news media are also responsive to the agendas of fake news, but to a lesser degree.

background	While fact-checkers are autonomous in their selection of issues to cover, they were not influential in determining the agenda of news media overall, and their influence appears to be declining, illustrating the difficulties factcheckers face in disseminating their corrections.

background	In 2016, partisan media appeared to be especially susceptible to the agendas of fake news, perhaps due to the election.

background	Instead, fake news has an intricately entwined relationship with online partisan media, both responding and setting its issue agenda.

method	It starts with the most prevailing definitions and theories on presence, most of which attribute special roles for the mental process of attention and for mental models of the virtual space.

background	Most research on this subject is related to the concept of presence.

method	An investigation shows there has been substantial research for developing methods for measuring presence and research regarding factors that contribute to presence.

background	However, a thorough understanding of the reason why VR is effective and what effect it has on the human psyche is still missing.

method	A review of the phenomena thought to be effected by presence shows that there is still a strong need for research on this subject because little conclusive evidence exists regarding the relationship between presence and phenoma such as emotional responses to virtual stimuli.

background	Virtual Reality (VR) is starting to be used in psychological therapy around the world.

result	Knowledge of these contributing factors can play a vital role in development of new VR applications, but key knowledge elements in this area are still missing.

objective	This paper gives an up-to-date overview of research in this diverse field.

background	Clustering is often an essential first step in data mining intended to reduce redundancy, or define data categories.

result	And a thorough performance evaluation in Amazon's EC2 verifies that the scalability of our algorithm sustains when the datasets scale up.

method	We further show that Spark is a natural fit for the parallelization of single-linkage clustering algorithm due to its natural expression of iterative process.

background	Hierarchical clustering, a widely used clustering technique, can offer a richer representation by suggesting the potential group structures.

method	Our algorithm can be deployed easily in Amazon's cloud environment.

background	However, parallelization of such an algorithm is challenging as it exhibits inherent data dependency during the hierarchical tree construction.

method	In this paper, we design a parallel implementation of Single-linkage Hierarchical Clustering by formulating it as a Minimum Spanning Tree problem.

background	There is a crucial need in the society and industry for security solution in social media.

background	Social media systems heavily depend on users for content contribution and sharing.

background	Information is spread across social networks quickly and effectively.

background	Business entities or public figures set up social networking pages to enhance direct interactions with online users.

objective	In this demo, we propose SocialSpamGuard, a scalable and online social media spam detection system based on data mining for social network security.

background	However, at the same time social media networks become susceptible to different types of unwanted and malicious spammer or hacker actions.

method	We employ our GAD clustering algorithm for large scale clustering and integrate it with the designed active learning algorithm to deal with the scalability and real-time detection challenges.

background	We have entered the era of social media networks represented by Facebook, Twitter, YouTube and Flickr.

background	Internet users now spend more time on social networks than search engines.

background	A 12-bit low-power successive approximation register analog-to-digital converter (SAR ADC) is presented using dynamic latch comparator to realize power consumption of 47.86μW under 1.8V supply voltage.

background	Fully differential structure and hybrid 9-bit charge-redistribution capacitive and 3-bit resistor string DAC techniques are adopted to achieve balance between high precision and small area.

background	Fabricated in SMIC 0.18-μm 1P6M mixed-signal CMOS technology, the ADC only occupies 0.39mm active area and the DNL/INL achieve 0.6LSB and 0.8LSB respectively.

result	Experimental result shows that the power density of each 7nm FinFET circuit is 3-20 times larger than that of 45nm CMOS circuit under the spacer-defined technology.

result	Experimental result also shows that the back-gate signal enables a better control of power consumption for independent-gate FinFETs.

method	A Liberty-formatted standard cell library is established by selecting the appropriate number of fins for the pull-up and pull-down networks of each logic cell.

method	The layout of both shorted-gate and independent-gate standard cells are then characterized according to lambda-based layout design rules for FinFET devices.

background	In this paper, a power density analysis is presented for 7nm FinFET technology node based on both shorted-gate (SG) and independent-gate (IG) standard cells operating in multiple supply voltage regimes.

method	Finally, the power density of 7nm FinFET technology node is analyzed and compared with the 45 nm CMOS technology node for different circuits.

method	Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time.

result	With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.

background	We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier.

method	Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the onehot input vectors.

background	We find that only a few manipulations are needed to greatly decrease the accuracy.

method	We use the oracle to train a deterministic left-to-right dependency parser that is less sensitive to error propagation, using an online training procedure that also explores parser configurations resulting from non-optimal sequences of transitions.

method	In such cases, the oracle provides transitions that will lead to the best reachable tree from the given configuration.

background	The standard training regime for transition-based dependency parsers makes use of an oracle, which predicts an optimal transition sequence for a sentence and its gold tree.

method	We present an improved oracle for the arc-eager transition system, which provides a set of optimal transitions for every valid parser configuration, including configurations from which the gold tree is not reachable.

method	The oracle is efficient to implement and provably correct.

result	This new parser outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets.

background	While the topic of privacy has been traditionally studied in the context of cryptography and information-hiding, recent emphasis on data mining has lead to renewed interest in the field.

objective	In this chapter, we will introduce the topic of privacy-preserving data mining and provide an overview of the different topics covered in this book.

background	The field of privacy has seen rapid advances in recent years because of the increases in the ability to store data.

background	In particular, recent advances in the data mining field have lead to increased concerns about privacy.

method	These questionnaires were subsequently applied in survey studies in order to establish the structure of the concept and to identify the items that are suited for the measurement of its dimensions.

background	A generic measure based on this construct can support the design of such systems.

method	One questionnaire was subjected to an initial validation.

objective	The current paper describes the construction of two questionnaires for the measurement of this concept.

background	Social connectedness, i.e. the experience of belonging and relatedness between people, is a central concept in understanding and evaluating communication media, in particular awareness systems.

result	We conclude with some preliminary suggestions regarding (design) approaches to foster social connectedness.

method	Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a firststage sentence representation.

method	Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations.

result	With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.

method	Instead of using target sentence to attend words in source sentence, we utilized the sentence’s first-stage representation to attend words appeared in itself, which is called ”Inner-Attention” in our paper .

objective	In our approach, the encoding of sentence is a two-stage process.

result	Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of ”Inner-Attention” mechanism.

background	In this paper, we proposed a sentence encoding-based model for recognizing text entailment.

background	Nowadays we are living an era which is marked by efforts for originality, innovation, collaboration, accumulation of experience and integration with and through many inventions such as computers, the internet, and technologies that have facilitated knowledge sharing and communication among people.

objective	This research seeks to extend the existing literature on KMS and knowledge sharing by proposing a conceptual framework, namely ECCCT (Evolution, Collaboration, Connection, Codification and Technical) that can be used to study how knowledge management systems (KMS) facilitate knowledge sharing to support decision making processes in multinational corporations (MNC).

result	The work will assist managers in MNC in finding new ways of leveraging and sharing knowledge.

method	All participants are KMS professionals, managers and employees in MNC who are using KMS in different sectors and at different levels.

method	In this research, 42 semi-structured interviews have been conducted with participants from 35 MNC in 11 countries.

result	The procedure used to find a viable order of synthesis of the parts in the stack construction is described.

method	Two patterns the fanout and takeout are described which are key in solving the routing and synchronization problems.

background	This is an extension of the Turing Machine built previously by the author [10].

background	In this paper we present a Universal Turing Machine build in the Cellular Automaton Conway's Game of Life.

method	It is example of spatio-temporal collision based computation and has infinite tape provided by two stack structures which grow continuously using collision based construction.

result	An experimental evaluation is also performed to demonstrate robustness, effectiveness and computational efficiency of several methods used widely in practice.

other	© 2017 Elsevier B.V. All rights reserved.

background	However, the raw point cloud is often noisy and contains outliers.

method	This paper makes an attempt to present a comprehensive analysis of the state-of-the-art methods for filtering point cloud.

method	Therefore, it is crucial to remove the noise and outliers from the point cloud while preserving the features, in particular, its fine details.

background	In recent years, 3D point cloud has gained increasing attention as a new representation for objects.

method	The existing methods are categorized into seven classes, which concentrate on their common and obvious traits.

result	To gain business value, organizations need to incorporate community building as part of the implementation of social media.

background	While these platforms can be used as another means to deliver familiar e-commerce applications, when firms fail to fully engage their customers, they also fail to fully exploit the capabilities of social media platforms.

background	Social media platforms such as Twitter and Facebook enable the creation of virtual customer environments (VCEs) where online communities of interest form around specific firms, brands, or products.

objective	This article begins with a discussion the common information theoretical concepts of the two laws, and then examines each law with respect to its origins, theoretical formulation, theoretical development, research, and applications and examines the possible contributing factors responsible for the failure of Hick-Hyman Law to gain momentum in the field.

method	HUMAN–COMPUTER INTERACTION, 2005, Volume 20, pp.

other	315–352 Copyright © 2005, Lawrence Erlbaum Associates, Inc. Steven Seow recently earned his Ph.D. in Experimental Psychology at Brown University.

background	A search of the current human–computer interaction (HCI) literature, however, will reveal that the Hick-Hyman Law failed to gain momentum in the field of HCI, whereas Fitts’ Law received, and continues to receive, substantial attention.

background	In the early 1980s, Card, Moran, and Newell (1983) presented the laws as design principles for developers to maximize usability in the design of human–computer interfaces.

other	He is currently a Usability Engineer at Microsoft Corporation.

background	The Hick-Hyman Law and Fitts’ Law are two surviving human performance principles based on Shannon and Weaver’s (1949) Information Theory.

method	This is both a novel method in process prediction, which has previously relied on explicit process models in the form of Hidden Markov Models (HMM) or annotated transition systems, and also a novel application for deep learning methods.

objective	In this paper, we describe an initial application of deep learning with recurrent neural networks to the problem of predicting the next process event.

background	This in turn requires accurate prediction models for process outcomes and for the next process event, based on runtime information available at the prediction and decision point.

background	Runtime management requires the ability to identify processes that are at risk of not meeting certain criteria in order to offer case managers decision information for timely intervention.

background	Predicting the final state of a running process, the remaining time to completion or the next activity of a running process are important aspects of runtime process management.

background	Internet of things(IoT), being an innovative revolution over the Internet, becomes a new platform for E-business.

method	We also experiment our design and make a comprehensive discuss.

background	Nowadays, the development of traditional business models become more and more mature that people use them to guide various kinds of E-business activities.

background	However, old business models could hardly fit for the E-business on the IoT. In this article, we 1) propose an IoT E-business model, which is specially designed for the IoT E-business; 2) redesign many elements in traditional E-business models; 3) realize the transaction of smart property and paid data on the IoT with the help of P2P trade based on the Blockchain and smart contract.

background	Nonetheless, there exist an increasing number of large companies that are offering cloud computing infrastructure products and services that do not entirely resemble the visions of these individual compo-

background	Recently the cloud computing paradigm has been receiving significant excitement and attention in the media and blogosphere.

background	To some, cloud computing seems to be little more than a marketing umbrella, encompassing topics such as distributed computing, grid computing, utility computing, and softwareas-a-service, that have already received significant research focus and commercial implementation.

background	Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years.

background	Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures.

background	The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters.

background	It promises greater automation so as to increase both product quality and human productivity.

background	The construction of such systems involves many distributed design choices.

objective	This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.

background	If optimized jointly, these parameters can result in significant improvements.

background	These parameters are often specified and hard-coded into the software by various developers or teams.

method	Through a questionnaire survey, we test and analyze the research model and its related hypotheses by making use of structural equation modeling.

background	In recent years social media has become more and more popular all around the world.

objective	This study aims to examine the influence of social media in the e-commerce context and to find how it impacts users' visit intention and purchase intention.

result	The last one in turn influences visit intention and purchase intention in e-commerce.

result	In conclusion, we discuss the research findings and suggest some implications for researchers and practitioners.

result	The results indicate that social media interaction ties and social media commitment positively affect normative social influence and informational social influence.

result	We also show some research results on PAPR characteristics and channel-dependent resource scheduling of SC-FDMA.

method	In this paper, we give an in-depth overview of SC-FDMA with focus on physical layer and resource management aspects.

background	SC-FDMA has drawn great attention as an attractive alternative to OFDMA, especially in the uplink communications where lower PAPR greatly benefits the mobile terminal in terms of transmit power efficiency.

background	Single carrier frequency division multiple access (SC-FDMA) which utilizes single carrier modulation at the transmitter and frequency domain equalization at the receiver is a technique that has similar performance and essentially the same overall structure as those of an OFDMA system.

background	SC-FDMA is currently a working assumption for the uplink multiple access scheme in 3GPP Long Term Evolution (LTE).

background	One prominent advantage over OFDMA is that the SC-FDMA signal has lower peak-to-average power ratio (PAPR).

method	Applications of the ERP technique are illustrated with research on first and second language acquisition, bilingualism, and aphasia.

background	This article is a brief overview of ERPs and languageprocessing research.

objective	It discusses how ERPs are derived, provides the pros and cons of using ERPs for language-processing research, and gives a summary of the major ERP components relevant to research on speech perception (mismatch negativity), word and sentence comprehension (N400, left anterior negativity, P600), and word production (lateralized readiness potential, N200).

background	Since the publication of the first papers on event-related brain potentials (ERP) and language in the 1980s, the field of electrophysiology of language has evolved a great deal.

objective	Additionally, it addresses current controversies concerning the interpretation of these components.

background	The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment.

method	Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers.

method	Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF).

background	Such sets of samples are riddled with redundancies—any overlapping pixels are constrained to be the same.

objective	Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches.

background	To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches.

method	By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude.

method	To encourage further developments, our tracking framework was made open-source.

method	Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1).

method	For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart.

method	These 10 aspects can be used as a checklist by management to ensure that a comprehensive plan has been defined and introduced.

other	a 2004 Elsevier Ltd. All rights reserved.

objective	This paper identifies 10 essential aspects, which, if not taken into account in an information security governance plan, will surely cause the plan to fail, or at least, cause serious flaws in the plan.

background	However, obtaining annotated 3D pose data requires a complex motion capture setup which is generally limited to controlled settings.

result	Our complete pipeline improves upon the state-of-the-art by 11.8%, and works at 30 FPS on a commodity graphics card.

background	Deep learning methods for 3D human pose estimation from RGB images require a huge amount of domain-specific labeled data for good in-the-wild performance.

method	We propose a semi-supervised learning method using a structure-aware loss function which is able to utilize abundant 2D data to learn 3D information.

method	Furthermore, we present a simple temporal network which uses additional context present in pose sequences to improve and temporally harmonize the pose estimates.

background	However, questions and answers might be related to each other in complicated ways which cannot be captured by single-vector representations.

method	For each attention step, in addition to common attention mechanisms, we adopt sequential attention which utilizes context information for computing context-aware attention weights.

objective	In this paper, we propose Multihop Attention Networks (MAN) which aim to uncover these complex relations for ranking question and answer pairs.

method	Unlike previous models, we do not collapse the question into a single vector, instead we use multiple vectors which focus on different parts of the question for its overall semantic representation and apply multiple steps of attention to learn representations for the candidate answers.

method	Via extensive experiments, we show that MAN outperforms state-of-the-art approaches on popular benchmark QA datasets.

background	These models often represent a question by a single vector and find its corresponding matches by attending to candidate answers.

result	Empirical studies confirm the effectiveness of sequential attention over other attention mechanisms.

background	Attention based neural network models have been successfully applied in answer selection, which is an important subtask of question answering (QA).

method	Note that a symmetric balanced incomplete block design is utilized for key generation, which substantially reduces the burden on members to derive a common conference key.

method	On the other hand, a common conference key is derived based on the key agreement to enable group members to share and store their data securely.

method	On the one hand, group members can communicate anonymously with respect to the group signature, and the real identities of members can be traced if necessary.

result	Both theoretical and experimental analyses demonstrate that the proposed scheme is secure and efficient for group data sharing in cloud computing.

background	With the popularity of cloud computing, how to achieve secure and efficient data sharing in cloud environments is an urgent problem to be solved.

background	Group data sharing in cloud environments has become a hot topic in recent decades.

background	In addition, how to achieve both anonymity and traceability is also a challenge in the cloud for data sharing.

objective	This paper focuses on enabling data sharing and storage for the same group in the cloud with high security and efficiency in an anonymous manner.

method	By leveraging the key agreement and the group signature, a novel traceable group data sharing scheme is proposed to support anonymous multiple users in public clouds.

method	We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.

background	Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features.

background	We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting.

method	This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples.

result	In addition to presenting detailed complexity results for ROCK, we also conduct an experimental study with real-life as well as synthetic data sets to demonstrate the effectiveness of our techniques.

method	We show that traditional clustering algorithms that use distances between points for clustering are not appropriate for boolean and categorical attributes.

result	For data with categorical attributes, our findings indicate that ROCK not only generates better quality clusters than traditional algorithms, but it also exhibits good scalability properties.

method	We develop a robust hierarchical clustering algorithm ROCK that employs links and not distances when merging clusters.

background	Clustering algorithms usually employ a distance metric based (e.g., euclidean) similarity measure in order to partition the database such that data points in the same partition are more similar than points in different partitions.

objective	In this paper, we study clustering algorithms for data with boolean and categorical attributes.

method	Instead, we propose a novel concept of links to measure the similarity/proximity between a pair of data points.

background	Clustering, in data mining, is useful to discover distribution patterns in the underlying data.

method	Our methods naturally extend to non-metric similarity measures that are relevant in situations where a domain expert/similarity table is the only source of knowledge.

background	Previous research suggests that IM users often multitask while conversing online.

result	Additional analyses revealed that the more time participants reported spending on IM, the lower their reading comprehension scores.

method	Participants who IMed while performing the reading task took significantly longer to complete the task, indicating that concurrent IM use negatively affects efficiency.

background	Participants in the present study (N = 69) completed a reading comprehension task uninterrupted or while concurrently holding an IM conversation.

background	To date, no one has yet examined the cognitive effect of concurrent IM use.

background	Instant messaging (IM) has become one of the most popular forms of computer-mediated communication (CMC) and is especially prevalent on college campuses.

result	Finally, we found that the more time participants reported spending on IM, the lower their self-reported GPA.

other	Implications and future directions are discussed.

method	Concurrent IM use did not affect reading comprehension scores.

method	The scaling property applies to scaling by integers which are relatively prime to the length of the DFT.

result	The time reversal property of DFT is identified as a special case of this theorem.

objective	This paper presents the analogue of the time or frequency scaling theorem of continuous time/frequency Fourier Transform (FT) to the realm of Discrete Fourier Transform (DFT).

background	Commonly accepted problems are for example to cope with change and that defects all too often are detected too late in the software development process.

result	The case study aims at validating or contradicting the beliefs of what the problems are in waterfall development through empirical research.

method	To address this research gap, we compare the problems in literature with the results of a case study at Ericsson AB in Sweden, investigating issues in the waterfall model.

background	Many problems have been reported related to the model.

background	However, many of the problems mentioned in literature are based on beliefs and experiences, and not on empirical evidence.

background	Waterfall development is still a widely used way of working in software development companies.

objective	This study presented a brief overview of potentiometric electrochemical sensors (ISE and ISFET) for soil NPK detection.

result	The opportunities and challenges for electrochemical sensors in soil testing were

background	Soil testing is the basis for nutrient recommendation and formulated fertilization.

background	DRAMs and microprocessors became critical to the semiconductor industry, yet were unknown during the original formulation of Moore's law

background	The seemingly unshakeable accuracy of Moore's law - which states that the speed of computers; as measured by the number of transistors that can be placed on a single chip, will double every year or two - has been credited with being the engine of the electronics revolution, and is regarded as the premier example of a self-fulfilling prophecy and technological trajectory in both the academic and popular press.

background	Although many factors have kept Moore's law as an industry benchmark, it is the entry of foreign competition that seems to have played a critical role in maintaining the pace of Moore's law in the early VLSI transition.

background	Many different kinds of chips used many competing logic families.

other	IBM Watson

background	ive Text Summarization using Sequence-to-sequence RNNs and Beyond Ramesh Nallapati IBM

other	Watson nallapati@us.ibm.com Bowen Zhou IBM

other	Watson zhou@us.ibm.com Cicero dos Santos

result	However, we also present a factorial respecification that satisfies more recent model fit thresholds.

objective	This paper measures flow in the context of gamification and investigates the psychometric properties of the Dispositional Flow Scale-2 (DFS-2).

result	The results show that the original DFS-2 factorial structure does result in a similar model fit as the original work.

other	All rights reserved.

result	Beyond validating the original DFS-2 instrument in the context of gamification, the psychometric analysis and the respecifications suggest that the components of flow divide into highly correlated conditions of flow (which were also found to be more salient in the context of gamification: autotelic experience, balance of skill and challenge, control, clear goals, and feedback) and into possible outcomes (merging action-awareness, concentration, loss of sense of time, and loss of selfconsciousness) from achieving flow.

other	2014 Elsevier Ltd.

result	We employ data gathered from users of an exercise gamification service (N = 200).

background	Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words.

background	When it comes to texts, one of the most common fixed-length features is bag-of-words.

method	Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models.

result	Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.

result	Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.

objective	In this paper, we proposeParagraph Vector , an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.

background	Many machine learning algorithms require the input to be represented as a fixed-length feature vector.

method	Our algorithm represents each document by a dense vector which is trained to predict words in the document.

background	For example, “powerful,” “strong” and “Paris” are equally distant.

background	The world of telecommunications, especially mobile communications, continues to evolve with innovative technologies and high-speed data services.

method	Subsequently, critical success factors in each economy is drawn and compared with the other, to elicit future directions.

background	In many economies, mobile phones have overtaken fixed lines.

method	The basic standard for comparison has been Global Systems for Mobile Communications (GSM) and its data services, which was adopted by both economies.

objective	In this dynamic context, we have envisaged to study mobile communication diffusion in Germany and India, from a historical comparative perspective.

method	Niching methods are the extension of EAs to address this issue.

background	However, many complex search problems require the identification and maintenance of multiple solutions.

result	Based on successful data fit, we propose the well-known logistic model to describe our experimental results.

method	We analyze our algorithm, introduce an experimental setup, and compare its performance with a previous ES niching method, known as the ES dynamic niching algorithm.

background	Evolutionary algorithms (EAs) have the tendency to converge quickly into a single solution in the search space.

result	In our comparison we introduce for the first time a new analytical tool for niching analysis, and in particular the early niching formation process.

method	In our study, we propose an evolution strategy (ES) niching method, based on the covariance matrix adaptation (CMA) mechanism.

result	6: The approach has been applied to a web-based system and the results obtained are quite promising.

background	2: This process is typically costly and time-consuming.

method	4: In that respect, we have developed a test framework that facilitates capturing and modeling of component interactions.

method	5: These interactions are analyzed to select a subset of test cases to be executed during regression testing.

background	0: Typically, COTS undergo frequent upgrades.

background	3: An efficient approach to regression testing would be to execute a subset of the system test suite that provides sufficient confidence in the system behavior.

background	1: Organizations while deploying these upgrades ensure the correctness of existing systems by carrying out exhaustive regression tests.

method	In this study, modeling and simulation of a speed sensored field-oriented control (FOC) of a permanent magnet synchronous motor (PMSM) drive is developed by using MATLAB Function blocks in MATLAB/Simulink.

method	The superiority of the method over commonly used "Code Generation" tools is also emphasized.

result	Finally, the results of the simulation and experiments are compared.

result	The results of the simulation are presented.

result	Then, the MATLAB programming based codes developed in simulation are implemented in a TMS320F28335 floating-point MCU by using C programming language and the experimental results are obtained.

result	First, a MATLAB/Simulink model of the FOC of PMSM drive is developed by using MATLAB programming in MATLAB Functions similar to C coding techniques.

method	This method allows easier algorithm and software development stages for experimental studies compared to the classical block diagram approach.

method	3: In strengthening the formulation, we develop some valid inequalities for the zero-one quadratic (knapsack) polytope and a columngeneration formulation that eliminates the symmetry of ring configurations.

method	4: Also, we prescribe an effective tabu search procedure for finding a goodquality feasible solution, which is also used as a starting column for the column generation procedure.

objective	1: The problem seeks to partition the set of demand pairs to a number of rings and a mesh cluster, and to determine the location of the optical cross-connect system (OXC), while minimizing the total cost of optical add-drop multiplexers (OADMs), OXCs, and fiber links.

objective	0: This article deals with a ring–mesh network design problem arising from the deployment of an optical transport network.

method	2: We formulate this problem as a zero-one integer programming problem.

result	5: Computational results show that the proposed solution procedure provides tight lower and upper bounds within a reasonable time bound.

background	The video game software industry has a reputation for volatile, chaotic projects yet, in spite of dramatic growth in global revenues, surprisingly little academic work has examined these projects.

objective	This study reports a preliminary investigation into this under-researched area.

result	Among the risk factors mentioned, two are specific to the unique context of video game development.

method	We interviewed eight video game producers from a range of companies, using a critical incident method to explore risk management practices and risk perceptions.

result	The risk of failing to match the development strategy to the project was identified as a major cause of problems during the development process, and a new risk - the 'fun factor' - was a key element threatening the success of the final game release.

result	Our results revealed that in lieu of formal risk management practices, these managers relied on prototyping, pre-production decision points, and agile approaches to contain risk on their projects.

objective	In this paper, we develop a solid foundation for phase noise that is valid for any oscillator, regardless of operating mechanism.

method	This leads us to a precise characterisation of timing jitter and spectral dispersion, for computing which we develop efficient numerical methods.

background	Although progress has been made in understanding the phenomenon, there still remain significant gaps, both in its fundamental theory and in numerical techniques for its characterisation.

background	Phase noise is a topic of theoretical and practical interest in electronic circuits, as well as in other fields such as optics.

result	We demonstrate our techniques on practical electrical oscillators, and obtain good matches with measurements even at frequencies close to the carrier, where previous techniques break down.

method	We obtain an exact, nonlinear equation for phase error, which we solve without approximations for random perturbations.

objective	We establish novel results about the dynamics of stable nonlinear oscillators in the presence of perturbations, both deterministic and random.

background	The Self-Organizing Map (SOM) algorithm has attracted an ever increasing amount of interest among researches and practitioners in a wide variety of elds.

method	The list is intended to serve as a source for literature surveys.

background	The SOM and a variant of it, the LVQ, have been analyzed extensively, a number of variants of them have been developed and, perhaps most notably, they have been applied extensively within elds ranging from engineering sciences to medicine, biology, and economics.

method	We have collected a comprehensive list of 3343 scienti c papers that use the algorithms, have bene ted from them, or contain analyses of them.

result	We have provided both a thematic and a keyword index to help nding articles of interest.

background	Recently, there are many approaches proposed for mining roles using automated technologies.

background	However, it lacks a tool set that can be used to aid the application of role mining approaches and update role states.

method	In this demonstration, we introduce a tool set, RMiner, which is based on the core of WEKA, an open source data mining tool.

method	RMiner implements most of the classic and latest role mining algorithms and provides interactive tools for administrator to update role states.

result	The running examples of RMiner are presented to demonstrate the effectiveness of the tool set.

result	We present experimental results that demonstrate the effectiveness of the item2vec method and show it is competitive with SVD.

background	Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities.

background	Among them, the Skip-gram with Negative Sampling (SGNS), also known as word2vec, was shown to provide state-of-the-art results on various linguistics tasks.

method	The method is capable of inferring item-item relations even when user information is not available.

method	Inspired by SGNS, we describe a method we name item2vec for item-based CF that produces embedding for items in a latent space.

background	Recently, several works in the field of Natural Language Processing (NLP) suggested to learn a latent representation of words using neural embedding algorithms.

objective	In this paper, we show that item-based CF can be cast in the same framework of neural word embedding.

method	The search is performed on six general voltage-input voltage-output transfer functions, derived using a small-signal two-port network transistor model.

objective	This paper systematically investigates the design of single transistor second-order active filters out-lining all possible architectures and possible impedance settings using an exhaustive MAPLE search code for all stable cases with the minimum number of passive elements (two resistors and two energy storage elements).

result	The results presented here fill both a circuit-theoretic gap and an application-oriented gap related to single transistor filter design which, apart from a few examples in the literature, has not been rigorously addressed.

method	Selected LC and RC filter designs are verified experimentally.

background	Arrival patterns determin e whether tasks will be treated as periodic, sporadic, or aperiodic.

background	As many algorithms focus on specific sets of task types and constraints only, system design has to focus on those supported by a particular algorithm, at the expense

background	These can be in the form of standard timing constraints, such as period and deadline, or complex, e.g., to express application specific or non temporal constraints, reliability, performance, etc.

background	Many industrial applications with real-time demands are composed of mixed sets of tasks with a variety of requirements.

background	The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context.

objective	In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent.

other	Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.

method	Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly.

background	Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition.

result	Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.

objective	In this paper, we recognize a few recent efforts and discuss a number of viable directions for developing a potential EMO algorithm for solving many-objective optimization problems.

background	Having developed multiobjective optimization algorithms using evolutionary optimization methods and demonstrated their niche on various practical problems involving mostly two and three objectives, there is now a growing need for developing evolutionary multiobjective optimization (EMO) algorithms for handling many-objective (having four or more objectives) optimization problems.

method	The proposed NSGA-III is applied to a number of many-objective test problems with three to 15 objectives and compared with two versions of a recently suggested EMO algorithm (MOEA/D).

method	Thereafter, we suggest a reference-point-based many-objective evolutionary algorithm following NSGA-II framework (we call it NSGA-III) that emphasizes population members that are nondominated, yet close to a set of supplied reference points.

result	This paper presents results on unconstrained problems, and the sequel paper considers constrained and other specialties in handling many-objective optimization problems.

method	While each of the two MOEA/D methods works well on different classes of problems, the proposed NSGA-III is found to produce satisfactory results on all problems considered in this paper.

background	The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection.

method	Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research.

method	To achieve this we first combine a state-ofthe-art classifier (Residual-101 [14]) with a fast detection framework (SSD [18]).

result	Results are shown on both PASCAL VOC and COCO detection.

method	We then augment SSD+Residual101 with deconvolution layers to introduce additional largescale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector.

result	Our DSSD with 513 × 513 input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN [3] on each dataset.

background	While these two contributions are easily described at a high-level, a naive implementation does not succeed.

background	This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost.

method	We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system.

method	We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.

background	Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data.

objective	In this article, we introduce a custom multi-chip machine-learning architecture along those lines.

background	A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses.

background	However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on-chip storage of a multi-chip system.

background	The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive.

method	The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning.

method	We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors.

method	In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM.

background	Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction.

background	Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results.

result	Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.

background	What will 5G be?

background	To support this, the core network will also have to reach unprecedented levels of flexibility and intelligence, spectrum regulation will need to be rethought and improved, and energy and cost efficiencies will become even more critical considerations.

background	Indeed, 5G will need to be a paradigm shift that includes very high carrier frequencies with massive bandwidths, extreme base station and device densities, and unprecedented numbers of antennas.

method	This paper discusses all of these topics, identifying key challenges for future research and preliminary 5G standardization activities, while providing a comprehensive overview of the current literature, and in particular of the papers appearing in this special issue.

background	The previous four generations of cellular technology have each been a major paradigm shift that has broken backward compatibility.

background	What it will not be is an incremental advance on 4G.

background	However, unlike the previous four generations, it will also be highly integrative: tying any new 5G air interface and spectrum together with LTE and WiFi to provide universal high-rate coverage and a seamless user experience.

background	The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities.

method	We highlight two areas of research&#x02013;regularization strategies and methods that learn or optimize multimodal fusion structures&#x02013;as exciting areas for future work.

objective	We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures.

background	We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field.

method	We considered neural networks trained using hybrid learning approaches, support vector machines, decision trees and a concurrent hybrid model involving decision trees and neural networks.

method	This paper summarizes the performance of four machine learning paradigms applied to modeling the severity of injury that occurred during traffic accidents.

background	We believe that to obtain the greatest possible accident reduction effects with limited budgetary resources, it is important that measures be based on scientific and objective surveys of the causes of accidents and severity of injuries.

background	Engineers and researchers in the automobile industry have tried to design and build safer automobiles, but traffic accidents are unavoidable.

background	These behavioral and roadway accident patterns can be useful to develop traffic safety control policies.

result	Experiment results reveal that among the machine learning paradigms considered the hybrid decision tree-neural network approach outperformed the individual approaches.

background	Patterns involved in dangerous crashes could be detected if we develop accurate prediction models capable of automatic classification of type of injury severity of various traffic accidents.

objective	This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation.

method	GEOS then feeds the description to a geometric solver that attempts to determine the correct answer.

result	In our experiments, GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions.1 Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text.

objective	We model the problem of understanding geometry questions as submodular optimization, and identify a formal problem description likely to be compatible with both the question text and diagram.

objective	Therefore, this research aims to explore and describe the impact of IT on culture, using Saudi Arabia as a context for our study.

objective	In particular, we are interested in answering the following question: How does IT impact culture?

result	The resulting framework can serve as a basis for further investigation to understand individuals’ culturally linked behavior to various IT in diverse contexts The ultimate goal is to lay a foundation for much needed research on the IT role in cultural change.

background	Many of these technologies facilitate and mediate activities whose modes and means bind closely to culture.

background	Research has suggested that communication technologies can affect values and norms for behavior.

background	The Internet, World Wide Web, and related ICTs, have rapidly spread to a large number of countries and great variety of cultures.

method	We use phenomenological method, and provide an integration of a framework for IT-driven impact on culture.

background	The role of frameworks in information systems has recently received a great deal of critical attention.

result	While firm support is found for the assumption that the level of information attributes varies across system type in the direction postulated, there is evidence that the ability to differentiate the component attributes is affected by such factors as field dependency and mode of presentation.

background	One prominent indictment, which has been directed at even commonly accepted frameworks, is that they lack empirical support, and in fact are not constructed in operational terminology.

method	This article reports the results of an experimental ab study using MBA students as subjects to investigate the tenets of the Gorry and Scott Morton framework (Gorry and Scott Morton, 1971).

objective	In this paper, we provide a comprehensive survey on deep FER, including datasets and algorithms that provide insights into these intrinsic problems.

method	Competitive performances on widely used benchmarks are also summarized in this section.

method	For the state of the art in deep FER, we review existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences, and discuss their advantages and limitations.

method	First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets.

method	We then extend our survey to additional related issues and application scenarios.

result	Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.

method	We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions of applicable implementations for each stage.

background	With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER.

background	Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose and identity bias.

background	The targeted population is more likely to convert from advertising so the response lift between the targeted and untargeted group to the advertising is likely an overestimate of the impact of targeted advertising.

method	We propose a difference-in-differences estimator to account for this selection bias by decomposing the impact of targeting into selection bias and treatment effects components.

result	We find that the treatment effect on the targeted group is about twice as large for brand-related searches, but naively estimating this effect without taking into account selection bias leads to an overestimation of the lift from targeting on brand-related searches by almost 1,000%.

method	Using several large-scale online advertising campaigns, we test the effectiveness of targeted advertising on brand-related searches and clickthrough rates.

background	Advertisers are demanding more accurate estimates of the impact of targeted advertisements, yet no study proposes an appropriate methodology to analyze the effectiveness of a targeted advertising campaign, and there is a dearth of empirical evidence on the effectiveness of targeted advertising as a whole.

result	Finally, initial EMI-measurements of the system are also presented.

method	Analytical relationships for calculating the power components average and rms current ratings are derived to facilitate the rectifier design.

method	A laboratory prototype with an output power of 5 kW is built and measurements taken from this prototype confirm the operation of the proposed current controller.

background	In the course of the More Electric Aircraft program frequently active three-phase rectifiers in the power range of several kilowatts are required.

method	It is shown that the three-phase -switch rectifier (comprising three -connected bidirectional switches) is well suited for this application.

method	The system is analyzed using space vector calculus and a novel PWM current controller modulation concept is presented, where all three phases are controlled simultaneously; the analysis shows that the proposed concept yields optimal switching sequences.

method	We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder—one for encoding and one for decoding.

method	The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training.

result	When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.

background	While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain.

objective	In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem.

other	Which degree and university are the best for my future?

other	Which is the best investment for supporting the education of my children?

other	Which digital camera should I buy?

other	Which book should I buy for my next vacation?

other	Which movie should I rent?

other	Which web sites will I find interesting?

other	What is the best holiday for me and my family?

method	We discuss the architecture and a prototype implementation that can process hooks from a virtual machine running Windows XP on Xen.

objective	While current research has focused on moving these vulnerable security tools into an isolated virtual machine, this approach cripples security tools by preventing them from doing active monitoring.

background	Host-based security tools such as anti-virus and intrusion detection systems are not adequately protected on today's computers.

method	This paper describes an architecture that takes a hybrid approach, giving security tools the ability to do active monitoring while still benefiting from the increased security of an isolated virtual machine.

background	Malware is often designed to immediately disable any security tools upon installation, rendering them useless.

result	We conclude with a security analysis and show the performance of a single hook to be 28 musecs in the best case.

method	To select the optimal network architecture, comprehensive analysis of various MLP, CNN, CNN-RNN, CNN-LSTM and CNN-GRU with its topologies, network parameters and network structures is used.

background	Recently, Convolutional neural network (CNN) architectures in deep learning have achieved significant results in the field of computer vision.

result	This is mainly due to the reason that CNN have capability to extract high level feature representations that represents the abstract form of low level feature sets of network traffic connections.

result	CNN and its variant architectures have significantly performed well in comparison to the classical machine learning classifiers.

objective	To transform this performance toward the task of intrusion detection (ID) in cyber security, this paper models network traffic as time-series, particularly transmission control protocol / internet protocol (TCP/IP) packets in a predefined time range with supervised learning methods such as multi-layer perceptron (MLP), CNN, CNN-recurrent neural network (CNN-RNN), CNN-long short-term memory (CNN-LSTM) and CNN-gated recurrent unit (GRU), using millions of known good and bad network connections.

method	To measure the efficacy of these approaches we evaluate on the most important synthetic ID data set such as KDDCup 99.

method	The models in each experiment are run up to 1000 epochs with learning rate in the range [0.01-05].

background	The human forearm is composed of two long, thin bones called the radius and the ulna, and rotates using two axle joints.

method	For this, we need to miniaturize the muscle modules.

method	Moreover, we used miniature motors and developed a way to dissipate the motor heat to the bone structure.

objective	We aimed to develop a forearm based on the body proportion, weight ratio, muscle arrangement, and joint performance of the human body in order to bring out its benefits.

method	In addition, we enabled the muscle module to also be used as the bone structure.

method	To approach this task, we arranged two muscle motors inside one muscle module, and used the space effectively by utilizing common parts.

result	Also, we performed some motions such as soldering, opening a book, turning a screw, and badminton swinging using the benefits of the radioulnar structure, which have not been discussed before, and verified that Kengoro can realize skillful motions using the radioulnar joint like a human.

result	Through these approaches, we succeeded in developing a forearm with a radioulnar joint based on the body proportion, weight ratio, muscle arrangement, and joint performance of the human body, while keeping maintainability and reliability.

background	It requires both horizontal and vertical scalability solutions to manage Network File Systems (NFS) and load balancing DNS and HTTP requests.

method	In this paper, we present an architecture of a completely distributed and decentralized Peer-to-Peer (P2P) crawler called Apoidea, which is self-managing and uses geographical proximity of the web resources to the peers for a better and faster crawl.

objective	1 This paper describes a decentralized peer-to-peer model for building a Web crawler.

background	Most of the current systems use a centralized client-server model, in which the crawl is done by one or more tightly coupled machines, but the distribution of the crawling jobs and the collection of crawled results are managed in a centralized system using a centralized URL repository.

background	Centralized solutions are known to have problems like link congestion, being a single point of failure, and expensive administration.

method	We use Distributed Hash Table (DHT) based protocols to perform the critical URL-duplicate and content-duplicate tests.

background	Apache Cassandra is a leading distributed database of choice when it comes to big data management with zero downtime, linear scalability, and seamless multiple data center deployment.

objective	This work i) proposes the first query-driven big data modeling methodology for Apache Cassandra, ii) defines important data modeling principles, mapping rules, and mapping patterns to guide logical data modeling, iii) presents visual diagrams for Cassandra logical and physical data models, and iv) demonstrates a data modeling tool that automates the entire data modeling process.

background	With increasingly wider adoption of Cassandra for online transaction processing by hundreds of Web-scale companies, there is a growing need for a rigorous and practical data modeling approach that ensures sound and efficient schema design.

background	With the increasingly ubiquitous nature of Social networks and Cloud computing, users are starting to explore new ways to interact with, and exploit these developing paradigms.

result	This paper outlines our vision of, and experiences with, creating a Social Storage Cloud, looking specifically at possible market mechanisms that could be used to create a dynamic Cloud infrastructure in a Social network environment.

method	We propose leveraging the pre-established trust formed through friend relationships within a Social network to form a dynamic“Social Cloud”, enabling friends to share resources within the context of a Social network.

method	We believe that combining trust relationships with suitable incentive mechanisms (through financial payments or bartering) could provide much more sustainable resource sharing mechanisms.

background	Social networks are used to reflect real world relationships that allow users to share information and form connections between one another, essentially creating dynamic Virtual Organizations.

background	Unfortunately, traditional clustering methods cannot impose constrains on cluster sizes.

method	A potential application would be obtaining clusters with equal cluster size.

background	Data clustering is a frequently used technique in finance, computer science, and engineering.

result	Simulation results on multidimensional data demonstrate that the k-means algorithm with the proposed modifications can fulfill cluster size constraints and lead to more accurate and robust results.

method	The modified k-means algorithm can be used to obtain clusters in preferred sizes.

method	In this paper, we propose some vital modifications to the standard k-means algorithm such that it can incorporate size constraints for each cluster separately.

method	Moreover, the modified algorithm makes use of prior knowledge of the given data set for selectively initializing the cluster centroids which helps escaping from local minima.

background	In most of the applications, cluster sizes are either constrained to particular values or available as prior knowledge.

background	We are still at an early stage and thus require a deeper understanding of how the blockchain potentials can be realized, and what are the opportunities and challenges in so doing.

background	Recently, the Bitcoin-underlying blockchain technology gained prominence as a solution that offers the realization of distributed trust-free systems, where economic transactions are guaranteed by the underlying blockchain.

objective	Following a design science approach, we developed a proof of concept prototype that has the potential to replace a trust-based coffee shop payment solution that is based on an analogue, pre-paid punch card solution.

method	The demonstrator provides a starting point to evaluate the strengths and weaknesses of the blockchain technology when replacing a trust-based by a trust-free transaction system.

result	We conclude that the secure and trust-free blockchain-based transaction has the potential to change many existing trust-based transaction systems, but that scalability issues, costs, and volatility in the transaction currency are hindrances.

objective	The present study aimed to identify changes in brain activation following cognitive-behavioral therapy (CBT) in subjects suffering from specific phobia.

method	Using functional magnetic resonance imaging (fMRI), brain activation to spider videos was measured in 28 spider phobic and 14 healthy control subjects.

result	CBT strongly reduced phobic symptoms in the TG while the WG remained behaviorally unchanged.

method	Before therapy, brain activation did not differ between both groups of phobics.

method	As compared to control subjects, phobics showed greater responses to spider vs. control videos in the insula and anterior cingulate cortex (ACC).

method	Phobics were randomly assigned to a therapy-group (TG) and a waiting-list control group (WG).

background	Little is known about the effects of successful psychotherapy on brain function in subjects with anxiety disorders.

method	Both groups of phobics were scanned twice.

result	In the second scanning session, a significant reduction of hyperactivity in the insula and ACC was found in the TG compared to the WG.

method	Between scanning sessions, CBT was given to the TG.

result	We show show our technique can be adapted for use in a stereo vision system.

background	Unfortunately, traditional image registration techniques tend to be costly.

method	We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration.

method	Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing.

background	Image registration finds a variety of applications in computer vision.

method	Our technique is faster because it examines far fewer potential matches between the images than existing techniques.

background	Backpropagation computes an error signal for the output neurons and spreads it over the hidden neurons.

background	Contrastive Hebbian learning involves clamping the output neurons at desired values and letting the effect spread through feedback connections over the entire network.

result	This suggests that the functionality of backpropagation can be realized alternatively by a Hebbian-type learning algorithm, which is suitable for implementation in biological networks.

result	In this case, the change in network state caused by clamping the output neurons turns out to be the same as the error signal spread by backpropagation, except for a scalar prefactor.

background	Backpropagation and contrastive Hebbian learning are two methods of training networks with hidden neurons.

method	To investigate the relationship between these two forms of learning, we consider a special case in which they are identical: a multilayer perceptron with linear output units, to which weak feedback connections have been added.

background	Data from 2,071 US participants and 645 Hong Kong and Taiwan participants is used to provide a cross-cultural validation of the developed scale.

background	Analysis of actual in-game behavioral metrics is also provided to demonstrate predictive validity of the scale.

background	Understanding gaming motivations is important given the growing trend of incorporating game-based mechanisms in non-gaming applications.

background	In this paper, we describe the development and validation of an online gaming motivations scale based on a 3-factor model.

background	These 3D models can also be transmitted in real-time to remote users.

method	From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication.

method	This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space.

background	We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation.

result	This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.

background	Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras.

background	This research demo describes the implementation of a mobile AR-supported educational course application, AR Circuit, which is designed to promote the effectiveness of remote collaborative learning for physics.

background	The application employs the TCP/IP protocol enabling multiplayer functionality in a mobile AR environment.

method	The server phone will capture the video frames, process the video frame, and send the current frame and the markers transformation matrices to the client phone.

method	One phone acts as the server and the other acts as the client.

method	We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set.

objective	In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS).

result	The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.

result	The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification.

objective	Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model.

background	Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network.

method	To perform clustering tasks of the data points, an associated dynamical system is built, and its topological invariant property is investigated.

result	The experimental results show that the proposed method works successfully for clustering problems with arbitrary shapes.

background	In this letter, we develop a gaussian process model for clustering.

method	The variances of predictive values in gaussian processes learned from a training data are shown to comprise an estimate of the support of a probability density function.

method	The constructed variance function is then applied to construct a set of contours that enclose the data points, which correspond to cluster boundaries.

background	The lithium and Ni-MeH battery technologies are less than 40 years old and have taken over the electronics industry and are on the same track for the transportation industry and the utility grid.

background	Advanced energy storage has been a key enabling technology for the portable electronics explosion.

objective	An electric economy will demand more electrification of the transportation sector and it is likely that all vehicles sold by the end of this decade will have some level of hybridization.

objective	Energy storage capabilities in conjunction with the smart grid are expected to see a massive leap forward over the next 25 years.

objective	If renewable energy, or even lower cost energy, is to become prevalent energy storage is a critical component in reducing peak power demands and the intermittent nature of solar and wind power.

objective	In this review, energy storage from the gigawatt pumped hydro systems to the smallest watt-hour battery are discussed, and the future directions predicted.

background	In essence, a news dissemination ecosystem involves three dimensions on social media, i.e., a content dimension, a social dimension, and a temporal dimension.

background	Identifying and mitigating fake news also presents unique challenges.

background	However, social media also enables the wide propagation of “fake news”, i.e., news with intentionally false information.

objective	In this chapter, we will review network properties for studying fake news, introduce popular network types and propose how these networks can be used to detect and mitigate fake news on social media.

background	Fake news on social media can have significant negative societal effects.

background	To tackle these challenges, many existing research efforts exploit various features of the data, including network features.

background	Social media is becoming increasingly popular for news consumption due to its easy access, fast dissemination, and low cost.

objective	It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes.

objective	This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding.

method	We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC [6] while being simpler.

result	We show competitive results in word error rate on the Librispeech corpus [18] with MFCC features, and promising results from raw waveform.

method	An auxiliary server (the ``honeychecker'') can distinguish the user password from honeywords for the login routine, and will set off an alarm if a honeyword is submitted.

background	An adversary who steals a file of hashed passwords and inverts the hash function cannot tell if he has found the password or a honeyword.

background	We propose a simple method for improving the security of hashed passwords: the maintenance of additional ``honeywords'' (false passwords) associated with each user's account.

method	The attempted use of a honeyword for login sets off an alarm.

method	Within each category, we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior.

method	Besides, for each category, we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains.

result	Finally, we outline open issues in research and challenges faced while adopting deep anomaly detection techniques for real-world problems.

objective	The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection.

background	Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains.

method	We have grouped state-of-the-art deep anomaly detection research techniques into different categories based on the underlying assumptions and approach adopted.

method	Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness.

method	Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model.

background	Despite their success, neural networks are still hard to design.

background	Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding.

method	On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.

result	Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-ofthe-art.

method	In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.

method	On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.

background	Receiver operating characteristic (ROC) analysis is an established method of measuring diagnostic performance in medical imaging studies.

background	Recently researchers have begun to report ROC curve results for ANN classifiers.

background	Traditionally, artificial neural networks (ANN's) have been applied as a classifier to find one "best" detection rate.

method	Here, the authors propose a different technique for generating ROC curves for a two class ANN classifier.

method	The current standard method of generating ROC curves for an ANN is to vary the output node threshold for classification.

result	They show that this new technique generates better ROC curves in the sense of having greater area under the ROC curve (AUC), and in the sense of being composed of a better distribution of operating points.

result	Our work shows that purel y heuristic-based approaches can achieve good results, especially for lucid t ables.

objective	To reuse such tables, appropriate methods need to b e develop, which capture the structure and the content information.

background	Tables are a common structuring element in many documents, s uch as PDF files.

method	We have d e loped several heuristics which together recognize and decompose tables i n PDF files and store the extracted data in a structured data format (XML) for easi er reuse.

method	Additionally, we implemented a prototype, which gives the user the ab ility of making adjustments on the extracted data.

result	Our results suggest that the PLEX framework provides anchor points for evaluators to reflect during heuristic evaluations.

method	In this paper, we apply the PLEX framework in the evaluation of two game prototypes that explored novel physical interactions between mobile devices using Near-Field Communication, by means of three separate studies.

result	More broadly, the framework categories can be used as a checklist to assess different attributes of playfulness of a product or service.

background	The Playful Experiences (PLEX) framework is a categorization of playful experiences based on previous theoretical work on pleasurable experiences, game experiences, emotions, elements of play, and reasons why people play.

background	While the framework has been successfully employed in design-related activities, its potential as an evaluation tool has not yet been studied.

result	Experiments conducted in five public datasets have demonstrated that the proposed approach can outperform some well-known swarm-based techniques.

method	The wrapper approach combines the power of exploration of the bats together with the speed of the Optimum-Path Forest classifier to find the set of features that maximizes the accuracy in a validating set.

method	In this paper we propose a new nature-inspired feature selection technique based on the bats behaviour, which has never been applied to this context so far.

objective	Feature selection aims to find the most important information from a given set of features.

objective	As this task can be seen as an optimization problem, the combinatorial growth of the possible solutions may be in-viable for a exhaustive search.

method	We also report about the advantages and limitations of prior research, the applied evaluation methods and implications for future decision support research.

objective	To enhance the understanding of machine learning and its use in decision support systems, we report the results of our content analysis of design-oriented research published between 1994 and 2013 in major Information Systems outlets.

background	Machine learning is a useful technology for decision support systems and assumes greater importance in research and practice.

background	Whilst much of the work focuses technical implementations and the adaption of machine learning algorithms to application domains, the factors of machine learning design affecting the usefulness of decision support are still understudied.

method	The findings suggest that the usefulness of machine learning for supporting decision-makers is dependent on the task, the phase of decision-making, and the applied technologies.

result	Our findings suggest that future decision support research should shed more light on organizational and people-related evaluation criteria.

background	Large sums of money are laundered every year, posing a threat to the global economy and its security.

method	Key steps of data preparation, data transformation, and data analytics techniques have been discussed; existing machine learning algorithms and methods described in the literature have been categorised, summarised, and compared.

method	In particular, solutions of anti-money laundering typologies, link analysis, behavioural modelling, risk scoring, anomaly detection, and geographic capability have been identified and analysed.

objective	This paper aims to provide a comprehensive survey of machine learning algorithms and methods applied to detect suspicious transactions.

background	Money laundering has been affecting the global economy for many years.

result	Finally, what techniques were lacking or under-addressed in the existing research has been elaborated with the purpose of pinpointing future research directions.

background	Money laundering encompasses illegal activities that are used to make illegally acquired funds appear legal and legitimate.

result	Other IR-related tasks, e.g., question answering and information extraction, seem to be better suited.

method	only yield very small improvements or even a decrease in accuracy.

background	The results are not encouraging.

method	This makes them hard to use on large collections.

result	We review NLP techniques and come to the conclusion that (a) NLP needs to be optimized for IR in order to be effective and (b) document retrieval is not an ideal application for NLP, at least given the current state-of-the-art in NLP.

background	Simple methods (stopwording, porter-style stemming, etc.) usually yield significant improvements, while higher-level processing (chunking, parsing, word sense disambiguation, etc.)

background	Many Natural Language Processing (NLP) techniques have been used in Information Retrieval.

method	At the same time, higher-level methods increase the processing and storage cost dramatically.

background	Similar to cloud computing but with distinct characteristics, fog computing faces new security and privacy challenges besides those inherited from cloud computing.

result	In this paper, we have surveyed these challenges and corresponding solutions in a brief manner.

background	Fog computing is a promising computing paradigm that extends cloud computing to the edge of networks.

background	The package supports the detection of multiple region intersections, unions and 'solitary' genomic regions.

background	The significance of actually observed overlaps is estimated by comparing them with empirical null distributions generated by random shuffling of the input regions.

background	We present the MULTOVL application suite that detects and statistically analyses multiple overlaps of genomic regions in a fast and efficient manner.

background	In this paper, we present a framework for analyzing the issues in selecting views to materialize so as to achieve the best combination of good query performance and low view maintenance.

background	Selecting views to materialize is one of the most important decisions in designing a data warehouse.

method	We also map the materialized view design problem as O-l integer programming problem, whose solution can guarantee an optimal solution.

method	We first develop a heuristic algorithm which can provide a feasible solution based on individual optimal query plans.

other	0950-5849/$ see front matter 2009 Elsevier B.V. A doi:10.1016/j.infsof.2009.08.004 * Corresponding author.

other	: +49 3

other	Tel.

objective	The latter, which consist in constructing new schemata, are resource-consuming and thus detrimental to intuitive use (IV).

background	Graphical interface use involves schemata operations that range from transfer to induction.

method	Relevance for the design cycle of innovative interfaces is critically reviewed, and integration with existing intuitive-use design frameworks is proposed.

result	These considerations are built upon instructional design studies suggesting that assessment should precede and inform the application of design techniques geared toward IV.

background	The former apply existing knowledge, such as prior schemata, and are effortless, preconscious and intuitive.

method	A quantitative method is proposed to manipulate and screen schemata operations at the level of an interface's states and features.

result	Finally, conclusion and future perspective of e-banking development will be discussed.

background	E-banking is such idea.

background	Abstract— The new information technology is becoming an important factor in the future development of financial services industry, and especially banking industry.

background	Growing international trading and problems in transferring money have motivated researchers to introduce a new structure.

method	This paper presents a through survey of e-banking describing definition, barriers, benefits from the customers’, economy, and bank point of views, and main issues and challenges such as risk management and factors responsible for e-banking development.

background	Most of banks are using the Internet as a new distribution channel.

objective	The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible.

method	In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.

background	The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification.

background	They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks.

background	Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data.

result	Finally, other issues that must be addressed to realize a complete 5G architecture vision are discussed.

result	An initial proof of concept evaluation is presented to demonstrate the potential of the proposal.

method	A two-layer architecture is proposed, consisting of a radio network and a network cloud, integrating various enablers such as small cells, massive MIMO, control/user plane split, NFV, and SDN.

method	Three main concepts are integrated: ultra-dense small cell deployments on licensed and unlicensed spectrum, under control/user plane split architecture, to address capacity and data rate challenges; NFV and SDN to provide flexible network deployment and operation; and intelligent use of network data to facilitate optimal use of network resources for QoE provisioning and planning.

objective	This article presents an architecture vision to address the challenges placed on 5G mobile networks.

background	It has now reached to the technology known as “Blue eyes technology” that can sense and control human emotions and feelings through gadgets.

method	This paper implements a new technique known as Emotion Sensory World of Blue eyes technology which identifies human emotions (sad.happy.exclted or surprised) using image processing techniques by extracting eye portion from the captured image which is then compared with stored images of data base.

method	After identifying mood the songs will be played to make human emotion level normal.

background	The world of science cannot be measured in terms of development and progress.

background	It shows how far human mind can work and think.

background	The eyes, fingers, speech are the elements which help to sense the emotion level of human body.

method	This paper intends to apply a business model analysis methodology in order to better understand the strategic approaches of these actors.

background	Similar to other emerging industries, it is characterized by a large number of uncertainties at different levels, in particular concerning technology, business strategy and consumer demand.

background	Mobile business is a young promising industry created by the emergence of wireless data networks.

objective	This paper focuses on the strategic uncertainties, where a large number of actors are trying a number of strategic approaches to position themselves in the most favourable position in the value system.

method	We argue that successful business models are likely to be the ones that best address the economic peculiarities underlying this industry, like mobility, network effects and natural monopolies.

background	While these edge paradigms share several features, most of the existing research is compartmentalised; no synergies have been explored.

objective	In our results, we will show that all edge paradigms should consider the advances in other paradigms.

background	This is especially true in the field of security, where most analyses focus only on one edge paradigm, while ignoring the others.

background	To fulfil these requirements, various paradigms, such as fog computing, mobile edge computing, and mobile cloud computing, have emerged in recent years.

background	For various reasons, the cloud computing paradigm is unable to meet certain requirements (e.g. low latency and jitter, context awareness, mobility support) that are crucial for several applications (e.g. vehicular networks, augmented reality).

objective	The main goal of this study is to holistically analyse the security threats, challenges, and mechanisms inherent in all edge paradigms, while highlighting potential synergies and venues of collaboration.

method	Our evaluation of competitiveness utilizes customer reviews, an abundant source of information that is available in a wide range of domains.

background	In any competitive business, success is based on the ability to make an item more appealing to customers than the competition.

result	Finally, we evaluate the quality of our results and the scalability of our approach using multiple datasets from different domains.

background	What are the features of an item that most affect its competitiveness?

method	We present efficient methods for evaluating competitiveness in large review datasets and address the natural problem of finding the top-k competitors of a given item.

background	A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness between two items?

background	Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution.

background	Who are the main competitors of a given item?

method	In this paper, we present a formal definition of the competitiveness between two items, based on the market segments that they can both cover.

result	Human evaluation also shows that our model produces higher quality summaries.

method	However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable.

background	For longer documents and summaries however these models often include repetitive and incoherent phrases.

method	We introduce a neural network model with a novel intraattention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL).

method	Models trained only with supervised learning often exhibit “exposure bias” – they assume ground truth is provided at each step during training.

result	We evaluate this model on the CNN/Daily Mail and New York Times datasets.

background	Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences.

result	Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models.

method	In this paper, we report on the development of a Mandarin-English codeswitching spontaneous speech corpus: SEAME.

background	We call such sentences intra-sentential code-switch sentences.

method	This paper describes the corpus design and the analysis of collected corpus.

method	The corpus collected consists of intra-sentential code-switching utterances that are recorded under both interview and conversational settings.

background	In Singapore and Malaysia, people often speak a mixture of Mandarin and English within a single sentence.

method	The corpus is developed as part of a multilingual speech recognition project and will be used to examine how Mandarin-English codeswitch speech occurs in the spoken language in South-East Asia.

method	Additionally, it can provide insights into the development of large vocabulary continuous speech recognition (LVCSR) for code-switching speech.

background	At the same time, growing anecdotal evidence from case studies indicates KPs’ enormous potential.

method	We describe three major challenges to successful KP deployment: (1) sufficient contribution, (2) favorable organizational culture, and (3) knowledge integration—and validate these as applicable to KPs through a review of 42 empirical papers.

background	Knowledge Portals (KPs) are highly integrative Knowledge Management Systems (KMS) that promise to synthesize widely dispersed knowledge and to interconnect individuals in order to provide a ‘one-stop knowledge shop’.

objective	In this paper, we take some initial steps towards a theory for KPs that more distinctly conceptualizes KPs and emphasizes a KP’s role to unify networking and repository KMS features.

background	Yet, KPs face major challenges in practice, as the intricacies of knowledge exchange are subject to varied individual and social factors.

method	Finally, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels.

method	First, we introduce a variation of maxout activation, called max-feature-map (MFM), into each convolutional layer of CNN.

method	Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship.

method	MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps.

result	The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning.

result	Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces.

method	Second, three networks are carefully designed to obtain better performance, meanwhile, reducing the number of parameters and computational costs.

background	When training data are obtained from the Internet, the labels are likely to be ambiguous and inaccurate.

background	The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit the large amount of training data.

objective	This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels.

objective	In this brief, a systematic design procedure for a second-order all-digital phase-locked loop (PLL) is proposed.

result	The all-digital PLL design inherits the frequency response and stability characteristics of the analog prototype PLL

method	The design procedure is based on the analogy between a type-II second-order analog PLL and an all-digital PLL.

background	Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information.

background	Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains.

result	Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.

background	The models should not expose private information in these datasets.

method	Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy.

method	In our research model, it is hypothesized that good parent-child relationship positively correlates with good interpersonal relationships, which in turn are hypothesized to correlate with undesirable social anxiety.

method	In addition, both parent-child and interpersonal relationships are hypothesized to negatively correlate with Internet addiction, whereas the level of social anxiety is hypothesized to positively correlate with Internet addiction.

background	Previous studies have presented conflicting claims regarding reasons that people become addicted to the Internet.

result	Finally, the more social anxiety and discontent with their peer interactions the participants experienced, the more addicted they were to the Internet.

result	In addition, interpersonal relationships, the parent-child relationship, and social anxiety all influence Internet addiction, as predicted by the model.

result	The results of this study confirm the research model hypotheses, indicating that the quality of parent-child relationship is indeed positively correlated to the quality of our participants' interpersonal relationships and that frustrating interpersonal relationships may raise the level of social anxiety.

objective	In this study, we attempted to identify predictors of Internet addiction based on Sullivan's interpersonal theory and Internet addiction literature.

background	This is because, without knowing the current location, it is very hard to autonomously execute any driving maneuvers for the future.

objective	To enhance the existing system's localization capability, this work presents an effort of developing a vision-based lateral localization algorithm.

method	The algorithm aims at reliably counting, with or without observations of lane-markings, the number road-lanes and identifying the index of the road-lane on the roadway that our vehicle happens to be driving on.

background	However, on urban driving environments, due to poor satellite geometry and disruption of radio signal reception, their longitudinal and lateral errors are too significant to be used to guide an autonomous system.

result	Testings the proposed algorithms against inter-city and inter-state highway videos showed promising results in terms of counting the number of road-lanes and the indices of the current road-lanes.

background	The existing solutions for localization rely on a combination of Global Navigation Satellite System (GNSS), an inertial measurement unit, and a digital map.

background	For urban driving, knowledge of ego-vehicle's position is a critical piece of information that enables advanced driver-assistance systems or self-driving cars to execute safety-related, autonomous driving maneuvers.

background	However, much knowledge, particularly knowledge with rich tacit dimensions, is transferred informally through processes of socialization and intemaiization.

other	Journal of Management Information Systems /Summer 200\, Vol.

other	95-114. ©2001 M.E. Sharpe, Inc. 0742-1222 / 2001 $9.50 + 0.00.

objective	We focus on two transfer mechanisms—mentoring and storytelling—that can leverage the knowledge of an organization, particularly its tacit knowledge, to build core capabilities.

other	18, No. 1, pp.

method	Finally, we present recommendations for specific managerial practices that follow from our analysis.

method	We draw on relevant research in leaming and cognitive psychology to clarify the conditions under which mentoring and storytelling can be most effective as carriers of knowledge.

other	96 SWAP, LEONARD, SHIELDS, AND ABRAMS

background	Core capabilities may be transferred fonnally and explicitly.

background	The core capabilities of an organization include critical skills of employees, management systems, and norms and values.

background	There has been much interest in the Sharing Economy in recent years, accompanied with the hope that it will change and specifically make better use of existing resources.

method	We argue that even though there are limits to the Sharing Economy today, it still has potential benefits for a future of scarcity---but only if the practice of sharing is approached with a dual focus on sharing and on limits at the same time.

result	Finally we conclude that even though we have begun to explore the future of sharing, there is still a need to further develop ideas of how the underlying infrastructure for this movement will look.

background	It could even be said that the Sharing Economy ought to align well with Computing within Limits and its underlying premises.

method	In this paper however, we take a critical stance and will elaborate on the intersection between the Sharing Economy and Limits (including pinpointing potential conflicts) so as to identify and discuss a 'Limits-compliant Sharing Economy'.

background	It intuitively makes sense, from a sustainability point of view, that the sharing of resources is good.

method	The second method is an application of a modifed Hellinger’s method applied to histograms of signal amplitude and other speech features.

background	The later situation can occur if it is desired to create a database of spontaneous data from data which consists of spontaneous discourse responding to prepared prompts.

method	The first method uses an expansion of the of the probability distribution in terms of the statistical moments.

method	This paper outlines and compares three methods for automatically classifying spontaneous and non-spontaneous speech and presents the cxperimcntal results comparing the performance of the methods.

method	The third method is based on a measure of the nonGaussianity of the data.

background	Examples of situations in which this problem may arise include forensic evidence situations, sorting voice-mail responses from voice-mail menus, and automatic segmentation of spontaneous responses from prepared questions.

method	All three methods are based on an analysis of the probability distributions of prosodic fcatures extracted from the speech signal.

background	There are many situations in which it is desirable to be able to distinguish spontaneous speech and speech which is non-spontaneous.

background	This paper gives an overview of automatic speaker recognition technology, with an emphasis on text-independent recognition.

background	Speaker recognition has been studied actively for several decades.

method	We start with the fundamentals of automatic speaker recognition, concerning feature extraction and speaker modeling.

method	The recent progress from vectors towards supervectors opens up a new area of exploration and represents a technology trend.

method	We elaborate advanced computational techniques to address robustness and session variability.

other	2009 Elsevier B.V. All rights reserved.

method	We also provide an overview of this recent development and discuss the evaluation methodology of speaker recognition systems.

result	We conclude the paper with discussion on future directions.

objective	We give an overview of both the classical and the state-of-the-art methods.

method	Most importantly, DeepHit smoothly handles competing risks; i.e. settings in which there is more than one possible event of interest.

background	A fundamental problem is to understand the relationship between the covariates and the (distribution of) survival times (times-to-event).

background	Survival analysis (time-to-event analysis) is widely used in economics and finance, engineering, medicine and many other areas.

background	Much of the previous work has approached the problem by viewing the survival time as the first hitting time of a stochastic process, assuming a specific form for the underlying stochastic process, using available data to learn the relationship between the covariates and the parameters of the model, and then deducing the relationship between covariates and the distribution of first hitting times (the risk).

result	Comparisons with previous models on the basis of real and synthetic datasets demonstrate that DeepHit achieves large and statistically significant performance improvements over previous state-of-the-art methods.

objective	This paper proposes a very different approach to survival analysis, DeepHit, that uses a deep neural network to learn the distribution of survival times directly.

method	DeepHit makes no assumptions about the underlying stochastic process and allows for the possibility that the relationship between covariates and risk(s) changes over time.

background	However, previous models rely on strong parametric assumptions that are often violated.

result	We showed that combining GGA pseudo-potential with TB09 functionals would improve greatly the band gaps compared with the GGA calculations.

other	All rights reserved.

other	© 2013 Elsevier B.V.

result	The details of our implementation and code samples are also given.

method	Using the functionals of TB09 we calculated the band gaps of some semiconductors and compared the resultswith previous calculations and experiments.

method	In this paper we present an implementation of the modified Becke–Johnson meta-GGA functional (TB09) in the PWSCF program of Quantum ESPRESSO package via the Libxc library.

method	We developed a continuous simulation model of the inc-V process, and the initial version is used to investigate the characteristics of the inc-V compared to V. The preliminary finding from the simulations of an example project is that the inc-V process is able to improve the traditional V process by saving effort, shortening duration, and increasing product quality.

background	Along with the exponentially growing complexity of modern vehicle systems, however, the late verification and validation in the conventional V-model expand in uncontrollable ways that result in higher cost of development and higher risk of failure than ever.

result	The finding also show how the advance of development technology impacts the systems engineering processes.

objective	This paper describes an inc-V development process for automotive industry that improves the conventional V-model and variants by introducing and institutionalizing early and continuous integrated verification enabled by simulation-based development.

background	V-model and its variants have become the most common process models adopted in automotive industry guiding the development of systems on a variety of refinement levels.

background	Additionally the data sets are heterogeneous and usually limited in size in comparison with the computer vision problems.

result	The experimental results demonstrate the superior ability of the proposed approach for both tasks.

background	Brain tumors segmentation has a special importance and difficulty due to the difference in appearances and shapes of the different tumor regions in magnetic resonance images.

method	We exploit conditional Generative Adversarial Network (cGAN) and train a semantic segmentation Convolution Neural Network (CNN) along with an adversarial network that discriminates segmentation maps coming from the ground truth or from the segmentation network for BraTS 2017 segmentation task[15,4,2,3].

result	The proposed model achieves on validation data a DICE score, Sensitivity and Specificity respectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment system.

background	Automated medical image analysis has a significant value in diagnosis and treatment of lesions.

objective	In this paper we propose a novel end-to-end trainable architecture for brain tumor semantic segmentation through conditional adversarial training.

method	We also propose an end-to-end trainable CNN for survival day prediction based on deep learning techniques for BraTS 2017 prediction task [15,4,2,3].

background	The recently proposed adversarial training has shown promising results in generative image modeling.

background	In addition, we find that the effect of awareness on the CSR–value relation is reversed for firms with a poor prior reputation as corporate citizens.

background	This evidence is consistent with the view that CSR activities can add value to the firm but only under certain conditions.

background	T paper shows that corporate social responsibility (CSR) and firm value are positively related for firms with high customer awareness, as proxied by advertising expenditures.

background	For firms with low customer awareness, the relation is either negative or insignificant.

method	We detect an individual emotion in each video frame and the decision on the stress level is made on sequence level.

result	Experimental results show that the developed system operates very well on simulated data even with generic models.

objective	In this work a real-time non-intrusive monitoring system is developed, which detects the emotional states of the driver by analyzing facial expressions.

background	Monitoring the attentive and emotional status of the driver is critical for the safety and comfort of driving.

result	An additional pose normalization step reduces the impact of pose mismatch due to camera setup and pose variation, and hence improves the detection accuracy further.

method	The system considers two negative basic emotions, anger and disgust, as stress related emotions.

objective	In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem.

objective	In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases).

result	Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN.

method	Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking.

result	The results show the importance of optimizing models for the right criterion.

method	We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt.

method	We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN.

background	Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products).

method	There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN).

method	The learning method is based on stochastic gradient descent with bootstrap sampling.

other	0201000296B04062001.

background	With this text, you gain an understanding of the fundamental concepts of algorithms, the very heart of computer science.

background	Later chapters go into sorting, searching and graphing algorithms, the string-matching algorithms, and the Schonhage-Strassen integer-multiplication algorithm.

other	Addison-Wesley Pub.

other	Co., 1974, , 470 pages.

background	It introduces the basic data structures and programming techniques often used in efficient algorithms.

background	Covers use of lists, push-down stacks, queues, trees, and graphs.

background	Provides numerous graded exercises at the end of each chapter.

background	The ETSI NFV Industry Specification Group has recently published drafts focused on standardization and implementation of NFV.

method	We describe NFV implementation with network overlay and SDN technologies.

method	In our discussion, we cover the first steps in understanding the role of NFV in implementing CoMP, D2D communication, and ultra densified networks.

objective	The main objective of this article is to explore the potential of NFV in enhancing 5G radio access networks' functional, architectural, and commercial viability, including increased automation, operational agility, and reduced capital expenditure.

method	Harnessing the potential of 5G and network functions virtualization, we discuss how NFV can address critical 5G design challenges through service abstraction and virtualized computing, storage, and network resources.

background	To realize its potential, 5G must provide considerably higher network capacity, enable massive device connectivity with reduced latency and cost, and achieve considerable energy savings compared to existing wireless technologies.

background	5G wireless technology is paving the way to revolutionize future ubiquitous and pervasive networking, wireless applications, and user quality of experience.

result	Experiments with different benchmarks demonstrate the high quality of QUINT.

method	Additionally, QUINT is able to harness language compositionality for answering complex questions without having any templates for the entire question.

background	Templates are an important asset for question answering over knowledge graphs, simplifying the semantic parsing of input utterances and generating structured queries for interpretable answers.

background	Stateof-the-art methods rely on hand-crafted templates with limited coverage.

objective	This paper presents QUINT, a system that automatically learns utterance-query templates solely from user questions paired with their answers.

objective	1: The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatorial explosion of insignificant relationships.

method	2: We discuss the use of association rules mining algorithm to push information automatically, and proposed mixed weighted association rules mining algorithm that apply to information push.

method	3: We identify the related information set and the vertical weight through the analyzing of users’ behavior, and use the Google’s PageRank algorithm to define the horizontal weight of information.

result	4: At last, we evaluate our algorithm against the traditional Apriori algorithm in information push, thereby justifying empirically the strength of our approach.

background	0: Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight.

method	Based on the special module system and the hierarchical topology of structural patterns in traditional Chinese architectures, the approach parameterizes the wooden elements of buildings and formalizes the construction rules for different architecture styles.

method	In the approach, XML-based description files are generated for displaying the construction process.

background	The cai-fen system was a module system used for the carpentry of Song architectures, which was specified by the governmental manual, the Yingzao Fashi (State Building Standards) compiled by Li

method	We present a rule-based approach for generation of ancient Chinese architectures from the Song dynasty.

background	Ancient Chinese architecture from the Song dynasty is a prominent example of the ancient oriental architectures.

method	The fundamental difference between our approach and previous works is that we apply and implement the module system in digitalization of ancient Chinese architecture.

method	What the approach generates are standard architectures that strictly follow the ancient Chinese governmental manual.

other	Jie [1103].

method	To demonstrate the efficiency of our approach, architectures in different styles have been generated based on their corresponding rules.

result	Finally, other issues that must be addressed to realize a complete 5G architecture vision are discussed.

result	An initial proof of concept evaluation is presented to demonstrate the potential of the proposal.

method	Three main concepts are integrated: ultra-dense small cell deployments on licensed and unlicensed spectrum, under control/user plane split architecture, to address capacity and data rate challenges; NFV and SDN to provide flexible network deployment and operation; and intelligent use of network data to facilitate optimal use of network resources for QoE provisioning and planning.

method	A two-layer architecture is proposed, consisting of a radio network and a network cloud, integrating various enablers such as small cells, massive MIMO, control/user plane split, NFV, and SDN.

objective	This article presents an architecture vision to address the challenges placed on 5G mobile networks.

method	We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser.

method	We propose a simple method for learning a stacked pipeline of models which we call “stack-propagation”.

method	At test time, our parser does not require predicted POS tags.

background	Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates.

result	On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.

objective	We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations.

background	Although scholars have long studied knowledge sharing drivers within software development teams, our knowledge remains fragmented by the divergent efforts that are based on and contribute to theoretical perspectives.

objective	This study provides a review of the extant literature (1993–2011) on knowledge sharing drivers in software teams and establishes a classification framework using an organizational change perspective.

result	A synthesis of the literature uncovers diverse themes and gaps in the existing body of knowledge, suggests several paths for advancing theory on knowledge sharing in software development contexts, and discusses implications for practitioners concerned with knowledge sharing in software

background	Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing.

background	Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them.

objective	In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention.

method	Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks.

result	We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K.

method	The selection and fusion form a feedback connecting the top-down and bottom-up computation.

result	Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.

background	Wireless personal area network (WPAN) is small-ranged network centered at an individual for interconnecting personal devices.

background	For such a network, the bootstrapping mechanism with which the devices establish a secure group key is of critical importance.

method	The proposed self-authenticated key agreement protocol utilizes the IGM's integrity guaranteed property, works together with the prescheduling mechanism to achieve message self-authentication, thus protecting the secure bootstrapping process from the node impersonation attack and the man-in-the-middle attack without leveraging any out-of-band channels.

method	Toward this end, we designed an integrity-guaranteed message (IGM) structure, a self-authenticated key agreement protocol, and a prescheduling mechanism in allusion to the IEEE 802.15.4 standard for WPANs.

objective	In this paper, we aim to develop a fully automated bootstrapping mechanism with only in-band channels with approvable security.

background	Most existing bootstrapping mechanisms require out-of-band channels and involve human interactions for authentication.

method	The IGM structure guarantees that an adversary cannot modify the IGM message without being detected, thus protects the message integrity without the requirement of shared secrets between the sender and the receiver devices.

result	We analyze the security performance of the proposed schemes, and show that they can be seamless interoperative with the existing IEEE 802.15.4 standard.

background	A recent study by Banerjee et al. (1998) proposed and tested an information technology (IT) ethics model.

background	Moreover, they found that factors affecting ethical intention are situational and depend upon the ethical dilemma.

background	Further research was suggested and recommended, among other things, replications with different samples.

background	They found that personal normative beliefs, organizational ethical climate, and organization-scenario were significant indicators of ethical behavioral intention.

objective	The present study furthers the development/validation of the IT ethical model by utilizing a large sample of students in the same organizational climate (a university).

objective	In this paper, we propose a convolutional neural network (CNN) based object localization method, called C-CNN: cascaded convolutional neural network, to overcome the disadvantages of the conventional methods.

result	The experimental results show that our method can achieve real time detection at the rate of 14FPS and be robust with a small size of training data.

background	Even so, these algorithms are not robust to detect the small, blurred, or large deformed target in industrial environment.

background	For stability, well-designed LED lighting must be mounted to uniform and stabilize lighting condition.

result	The detection accuracy is much higher than traditional methods and state-of-the-art methods.

method	Then two CNNs are adopted to further evaluate the passed windows and the windows around.

background	Traditionally, the normalized cross correlation (NCC) based or shape based template matching methods are utilized in machine vision to locate an object for a robot pick and place or other automatic equipment.

method	A relatively deep model net-4 is applied to adjust the passed windows at last and the adjusted windows are regarded as final positions.

method	Our C-CNN method first applies a shallow CNN densely scanning the whole image, most of the background regions are rejected by the network.

result	Furthermore, the structural performances between the two analyses (mechanical and thermomechanical) were compared as well.

result	The prediction results of temperature distribution, deformation, stress and contact pressure are presented.

method	Structural performance of the disc-pad model such as deformation and Von-Mises stress is predicted.

result	The results of this investigation may assist brake engineers to choose a suitable analysis in order to critically evaluate structural and contact behaviour of the disc brake assembly.

method	The first analysis is performed on the disc-pad model without the presence of thermal properties.

method	The case of thermoelasticity on the same model with the inclusion of convection, adiabatic and heat flux elements were also studied.

objective	The purpose of this work is to present a study of the thermomechanical behavior of the automotive disc brake during the braking phase.

method	First, we identify the vanishing point and the associated line segments corresponding to the shelves.

objective	Towards this goal, we propose an algorithm to detect shelves from images captured with a handheld digital camera.

background	Retailers need to monitor products on store shelves in order to maintain adequate stocking and in many cases to comply with a placement arrangement defined by a planogram.

background	It is possible that such inspections be performed automatically or semi-automatically using computer vision techniques.

method	Second, we divide the image into equal-angle wedges centered at the vanishing point and project the associated line segments into the wedges.

method	Finally, we identify shelves by analyzing the projections.

result	Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.

background	It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics.

result	GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster.

other	to GSDMM).

objective	In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr.

background	Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook.

result	We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge.

method	It is also validated by comparing the measurements of a simple scenario with the expected values.

method	The value of this module is demonstrated using an easy to follow example.

result	Finally, the performance of flow monitoring is characterized and shown to introduce small overheads.

background	However, simulators in general and NS-3 in particular, require significant programming effort from the researcher in order to collect those metrics.

objective	This paper reports a contribution for NS-3 consisting of a new flow monitoring module that makes it easier to collect and save to persistent storage a common set of network performance metrics.

method	The module automatically detects all flows passing through the network and stores in a file most of the metrics that a researcher might need to analyze about the flow, such as bitrates, duration, delays, packet sizes, and packet loss ratio.

background	When networking researchers meet the task of doing simulations, there is always a need to evaluate the value of such models by measuring a set of well known network performance metrics.

method	Our design introduces the concept of Transfer Tokens (XTokens) to practically use finegrained rule-specific tokens without increasing the number of OAuth permission prompts compared to current platforms.

background	Trigger-Action platforms are web-based systems that enable users to create automation rules by stitching together online services representing digital and physical resources using OAuth tokens.

background	Unfortunately, these platforms introduce a longrange large-scale security risk: If they are compromised, an attacker can misuse the OAuth tokens belonging to a large number of users to arbitrarily manipulate their devices and data.

method	We present the design and evaluation of Decentralized Trigger-Action Platform (DTAP), a trigger-action platform that implements this principle by overcoming practical challenges.

method	We introduce Decentralized Action Integrity, a security principle that prevents an untrusted trigger-action platform from misusing compromised OAuth tokens in ways that are inconsistent with any given user’s set of trigger-action rules.

result	Our evaluation indicates that DTAP poses negligible overhead: it adds less than 15ms of latency to rule execution time, and reduces throughput by 2.5%.

method	DTAP splits currently monolithic platform designs into an untrusted cloud service, and a set of user clients (each user only trusts their client).

background	0: The order-preserving pattern matching problem has gained attention in recent years.

result	6: We show with experimental results that the new proposed algorithm is more efficient than the previous solutions.

other	8: The Authors.

background	2: Typically, the text and the pattern consist of numbers.

method	5: In this paper, we present a fast order-preserving pattern matching algorithm, which uses specialized word-size packed string matching instructions, grounded on the single instruction multiple data instruction set architecture.

other	9: Software: Practice and Experience Published by John Wiley & Sons Ltd.

background	4: This model works on computer words, reading and processing blocks of characters at once, so that usual arithmetic and logic operations on words can be performed in one unit of time.

background	1: It consists in finding all substrings in the text, which have the same length and relative order as the input pattern.

background	3: Since recent times, there has been a tendency to utilize the ability of the word RAM model to increase the efficiency of string matching algorithms.

other	7: © 2016

method	The framework distinguishes four major strategies of MDM project initiations all featuring their specific assets and drawbacks.

background	Above any doubt, however, MDM initiatives confront organizations with multi-faceted and complex challenges that call for a more strategic approach to MDM.

result	The usefulness of our artifact is illustrated in a short case narrative.

objective	In this paper we introduce a framework for approaching MDM projects that has been developed in the course of a design science research study.

background	Just recently much Information Systems (IS) research focuses on master data management (MDM) which promises to increase an organization's overall core data quality.

background	In this paper, we propose a new framework for access control in IoT based on the blockchain technology.

method	As a proof of concept, we establish an initial implementation with a Raspberry PI device and local blockchain.

background	Security and privacy are huge challenges in Internet of Things (IoT) environments, but unfortunately, the harmonization of the IoT-related standards and protocols is hardly and slowly widespread.

method	Unlike financial bitcoin transactions, FairAccess introduces new types of transactions that are used to grant, get, delegate, and revoke access.

objective	Our first contribution consists in providing a reference model for our proposed framework within the Objectives, Models, Architecture and Mechanism specification in IoT. In addition, we introduce FairAccess as a fully decentralized pseudonymous and privacy preserving authorization management framework that enables users to own and control their data.

result	Finally, we discuss some limitations and propose further opportunities.

method	To implement our model, we use and adapt the blockchain into a decentralized access control manager.

result	Copyright © 2017 John Wiley & Sons, Ltd.

method	The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region.

result	Experiments on two types of crowd flows in Beijing and New York City (NYC) demonstrate that the proposed ST-ResNet outperforms six well-known methods.

method	We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data.

method	ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions.

objective	We propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the inflow and outflow of crowds in each and every region of a city.

background	Forecasting the flow of crowds is of great importance to traffic management and public safety, and very challenging as it is affected by many complex factors, such as inter-region traffic, events, and weather.

method	For each property, we design a branch of residual convolutional units, each of which models the spatial properties of crowd traffic.

method	More specifically, we employ the residual neural network framework to model the temporal closeness, period, and trend properties of crowd traffic.

method	Specifically, we propose to: (a) augment Faster RCNN with a semantic segmentation network; (b) use segmentation for top-down contextual priming; (c) use segmentation to provide top-down iterative feedback using two stage training.

result	Our results indicate that all three contributions improve the performance on object detection, semantic segmentation and region proposal generation.

background	Most of these gains are attributed to bottom-up, feedforward ConvNet frameworks.

background	The field of object detection has seen dramatic performance improvements in the last few years.

background	However, in case of humans, top-down information, context and feedback play an important role in doing object detection.

objective	This paper investigates how we can incorporate top-down information and feedback in the state-of-the-art Faster R-CNN framework.

objective	This paper introduces relevant aspects of Industry 4.0 in relation to strategic planning, key technologies, opportunities, and challenges.

method	Strategic planning includes construction of a CPS network, discussion of two major themes which are based on the smart factory and intelligent production, achieving three integrations (horizontal integration, vertical integration and end-to-end integration) and achieving eight plans which consist of the formulation of system standardization, efficient management etc.

background	Industry 4.0 (the fourth industrial revolution) encapsulates future industry development trends to achieve more intelligent manufacturing processes, including reliance on Cyber-Physical Systems (CPS), construction of Cyber-Physical Production Systems (CPPS), and implementation and operation of smart factories.

method	Finally, it referred to the enlightenment for China's manufacturing industries, to build China's Industry 4.0.

method	For this purpose, data mining methods have been suggested in many previous works.

result	The paper also describes an open source implementation of LogCluster.

result	In this paper, we present the LogCluster algorithm which implements data clustering and line pattern mining for textual event logs.

background	Modern IT systems often produce large volumes of event logs, and event pattern discovery is an important log management task.

method	Both a central site nonblocking protocol and a decentralized nonblocking protocol are presented.

objective	This paper investigates the properties of nonblocking protocols.

background	Many applications require nonblocking protocols.

method	Necessary and sufficient conditions for a protocol to be nonblocking are presented and from these conditions a method for designing them is derived.

background	Protocols that allow operational sites to continue transaction processing even though site failures have occurred are called nonblocking.

background	Information systems development is a high-risk undertaking, and failures remain common despite advances in development tools and technologies.

objective	This paper illustrates learning failure in systems development and recommends tactics for overcoming it.

background	Not only have many organizations failed to learn, but they have also learned to fail.

background	Organizations fail to learn from their experience in systems development because of limits of organizational intelligence, disincentives for learning, organizational designs and educational barriers.

background	In this paper, we argue that one reason for this is the collapse of organizational intelligence required to deal with the complexities of systems development.

background	Over time they accept and expect poor performance while creating organizational myths that perpetuate short-term optimization.

result	The results indicate that these three factors and privacy concern have significant effects on continuance usage.

other	All rights reserved.

method	Social influence includes three processes: compliance, identification and internalization, which are respectively represented by subjective norm, social identity, and group norm.

result	The results suggest that service providers should address the issues of social influence and privacy concern to encourage mobile SNS continuance usage.

other	2014 Elsevier Ltd.

objective	This research examines the continuance usage of mobile SNS in China by integrating both the perspectives of social influence and privacy concern.

background	Retaining users and facilitating continuance usage are crucial to the success of mobile social network services (SNS).

result	We demonstrate how our definition can be used to measure the similarity in a number of different domains.

method	We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model.

background	Similarity is an important and widely used concept.

background	Previous definitions of similarity are tied to a particular application or a form of knowledge representation.

background	Text in natural images is an important source of information, which can be utilized for many real-world applications.

method	To address this problem, we propose a novel convolutional neural network variant, called Multi-scale Spatial Partition Network (MSP-Net).

method	The whole image is classified as a text image (an image containing text) as long as one of the blocks is predicted to contain text.

method	The network classifies images very efficiently by predicting all blocks simultaneously in a single forward propagation.

method	The network classifies images that contain text or not, by predicting text existence in all image blocks, which are spatial partitions at multiple scales on an input image.

result	Through experimental evaluations and comparisons on public datasets, we demonstrate the effectiveness and robustness of the proposed method.

objective	This work focuses on a new problem: distinguishing images that contain text from a large volume of natural images.

result	Experiments on an analogical reasoning task, a word similarity task, and a word completion task have all demonstrated that knowledge-powered deep learning can enhance the effectiveness of word embedding.

background	Fortunately, text itself already contains welldefined morphological and syntactic knowledge; moreover, the large amount of texts on the Web enable the extraction of plenty of semantic knowledge.

background	The basis of applying deep learning to solve natural language processing tasks is to obtain high-quality distributed representations of words, i.e., word embeddings, from large amounts of text data.

background	Therefore, it makes sense to design novel deep learning algorithms and systems in order to leverage the above knowledge to compute more effective word embeddings.

background	However, text itself usually contains incomplete and ambiguous information, which makes necessity to leverage extra knowledge to understand it.

method	Our study explores these types of knowledge to define new basis for word representation, provide additional input information, and serve as auxiliary supervision in deep learning, respectively.

objective	In this paper, we conduct an empirical study on the capacity of leveraging morphological, syntactic, and semantic knowledge to achieve high-quality word embeddings.

method	In order to detect credit card fraud, we employed one-class classification approach in big data paradigm.

background	Banking and financial industries are facing severe challenges in the form of fraudulent transactions.

method	In this paper, we implemented parallelization of the auto-associative neural network in the hybrid architecture.

background	Credit card fraud is one example of them.

method	We implemented a hybrid architecture of Particle Swarm Optimization and Auto-Associative Neural Network for one-class classification in Spark computational framework.

background	Technological advances have made FPGAs an attractive platform for the acceleration of complex scientific applications.

method	The design pres ented is a modular, very deeply pipelined architecture that exploits the fine-grained parallelism of the calculations.

background	These applications demand high performance and hi ghprecision floating point arithmetic.

objective	In this paper, we present a design for calculating the Lennard-Jones potential and for ce as is done in molecular dynamics simulations.

method	This architecture employs IEEE 754 double precision floating point units, incl uding a square root unit developed for this kernel.

method	With the Xilinx Virtex-II Pro as a target device, an implementation using two pipelines operating in parallel achieves 3.9 GFLOPS.

background	ABMS promises to have far-reaching effects on the way that businesses use computers to support decision-making and researchers use electronic laboratories to support their research.

background	Some have gone so far as to contend that ABMS "is a third way of doing science," in addition to traditional deductive and inductive reasoning (Axelrod 1997).

background	Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents.

objective	This tutorial describes the theoretical and practical foundations of ABMS, identifies toolkits and methods for developing agent models, and illustrates the development of a simple agent-based model.

background	Applications range from modeling agent behavior in the stock market, supply chains, and consumer markets, to predicting the spread of epidemics, the threat of bio-warfare, and the factors responsible for the fall of ancient civilizations.

background	Computational advances have made possible a growing number of agent-based models across a variety of application domains.

objective	In this paper, one such attempt is made to design a multipurpose portable intelligent device named MEDIBOX which helps the patients take their medications at the right time.

objective	This box is a proficient system which maintains the parameters like temperature and humidity in a controlled range recommended by the drug manufacturer and thus maintains the potency of the medicines even if the patient is travelling.

background	Recently there has been attempts to design new medical devices which monitor the medications and help aged people for a better assisted living.

method	Related to this, we have developed a Host Management System (HMS) which is capable of cloud-based installation and monitoring that stores and controls the MEDIBOX functionality for further analysis and future modification in design aspects.

background	The health and wellness sector is critical to human society and as such should be one of the first to receive the benefits of upcoming technologies like IoT. Some of the Internet of Medical Things (IoMT) are connected to IoT networks to monitor the day-to-day activities of the patients.

other	The ITS4 source distribution is available at h t tp : //www.rstcorp.

method	This method is eficient enough to offer real-time feedback to developers during coding while producing few false negatives.

background	We describe ITS4, a tool for statically scanning security-critical C source code for vulnerabilities.

other	com/its4.

method	Unlike other techniques, our method is also simple enough to scan C + + code despite the complexities inherent in the language.

result	Using ITS4 we found new remotelyexploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software.

background	Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and eficiency.

objective	In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.

method	This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.

result	We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.

background	Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.

background	We use attachment theory to examine the link between Facebook users' relational orientation (in terms of attachment styles: anxiety and avoidance) and their relational activities.

objective	Our research examines whether and how the two key relational processes identified in offline social relationships (self-expression and responsiveness) are manifested on online social networks and related to attachment styles.

background	Knowing individuals' relational orientation is imperative for effective offline, as well as online, interactions and collaborations.

method	We describe our dataset of 640 Facebook users, their attachment scale survey results, and their 525,334 posts.

method	We define four features that map onto relational activities on Facebook: status updates and status updates with emotional words (self-expression); comments and likes (responsiveness).

result	A key takeaway of our research is that without relying on self-reported surveys, a computational analysis of a Facebook user's self-expressing and responding activities alone can reveal the user's underlying relational orientation (i.e., attachment style).

result	We find significant relationships between the users' attachment styles and their self-expression and responsiveness activities on Facebook.

result	Examples on simulated images and one real image are presented.

method	This external force, which we call gradient vector flow (GVF), is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image.

background	Problems associated with initialization and poor convergence to concave boundaries, however, have limited their utility.

background	Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries.

objective	This paper develops a new external force for active contours, largely solving both problems.

result	The resultant field has a large capture range and forces active contours into concave regions.

result	Our results show an average increase of F harmonic accuracy score for identifying both negative and positive sentiment of around 6.5% and 4.8% over the baselines of unigrams and part-of-speech features respectively.

objective	In this paper, we introduce a novel approach of adding semantics as additional features into the training set for sentiment analysis.

background	A wide range of features and methods for training sentiment classifiers for Twitter datasets have been researched in recent years with varying results.

background	Sentiment analysis over Twitter offer organisations a fast and effective way to monitor the publics’ feelings towards their brand, business, directors, etc.

method	We apply this approach to predict sentiment for three different Twitter datasets.

method	For each extracted entity (e.g. iPhone) from tweets, we add its semantic concept (e.g. “Apple product”) as an additional feature, and measure the correlation of the representative concept with negative/positive sentiment.

result	We also compare against an approach based on sentiment-bearing topic analysis, and find that semantic features produce better Recall and F score when classifying negative sentiment, and better Precision with lower Recall and F score in positive sentiment classification.

method	To accomplish this, we assembled thirteen culturally diverse global teams from locations in Europe, Mexico, and the United States, assigning each team a project leader and task to complete.

result	This study provides useful insights for managers interested in developing global virtual teams, as well as for academics interested in pursuing virtual team research.

other	8 KAYWORTH AND LEIDNER

result	At the same time, effective leaders are also able to assert their authority without being perceived as overbearing or inflexible.

result	Finally, effective leaders are found to be extremely effective at providing regular, detailed, and prompt communication with their peers and in articulating role relationships (responsibilities) among the virtual team members.

background	The trend toward physically dispersed work groups has necessitated a fresh inquiry into the role and nature of team leadership in virtual settings.

result	The findings suggest that effective team leaders demonstrate the capability to deal with paradox and contradiction by performing multiple leadership roles simultaneously (behavioral complexity).

result	Specifically, we discovered that highly effective virtual team leaders act in a mentoring role and exhibit a high degree of understanding (empathy) toward other team members.

result	Finally, the chapter concludes with a discussion on open issues and future challenges for the class of multi-criteria rating recommenders.

method	A review of current algorithms that use multicriteria ratings for calculating predictions and generating recommendations is provided.

method	First, it defines the recommendation problem as a multi-criteria decision making (MCDM) problem, and reviews MCDM methods and techniques that can support the implementation of multi-criteria recommenders.

objective	This chapter aims to provide an overview of the class of multi-criteria recommender systems.

method	Then, it focuses on the category of multi-criteria rating recommenders – techniques that provide recommendations by modelling a user’s utility for an item as a vector of ratings along several criteria.

method	In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series.

background	In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place.

method	We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem.

background	Probabilistic forecasting, i.e. estimating the probability distribution of a time series’ future given its past, is a key enabler for optimizing business processes.

result	We show through extensive empirical evaluation on several real-world forecasting data sets that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.

method	We compare the different methods across a variety of learning tasks, in order to assess each method's ability to find concise, accurate decision trees.

result	In addition, the experiments confirm that allowing multivariate tests generally improves the accuracy of the resulting decision tree over a univariate tree.

background	Unlike a univariate decision tree, a multivariate decision tree is not restricted to splits of the instance space that are orthogonal to the features' axes.

result	The results demonstrate that some multivariate methods are in general more effective than others (in the context of our experimental assumptions).

method	We present several new methods for forming multivariate decision trees and compare them with several well-known methods.

objective	This article addresses several issues for constructing multivariate decision trees: representing a multivariate test, including symbolic and numeric features, learning the coefficients of a multivariate test, selecting the features to include in a test, and pruning of multivariate decision trees.

method	Comparing the individual company process and the reference process, in-house and external experts can evaluate the gaps by using the decision calculus method.

background	A competitive implementation of customer relationship management (CRM) in retail banking requires business processes aligned to the value contribution.

objective	The objective of this paper is to derive a reference process on macro-level from industry best practice and validate its applicability in a survey.

method	Furthermore, we introduce a new method for evaluating existing business process implementations with respect to the business value, as measured by the value of the customer base as relevant overall key performance measure.

result	This allows a clear recommendation on the prioritization of potential process modifications and quantifies their effect in terms of business value.

background	Existing programs that generalize from examples are characterized in terms of the classes of search strategies that they employ.

background	The problem of concept learning, or forming a general description of a class of objects given a set of examples and non-examples, is viewed here as a search problem.

method	Several classes of search strategies are then analyzed and compared in terms of their relative capabilities and computational complexities.

method	We also use simple heuristics to guide transferring of textures from boundary to the hole.

method	We show that, by using such techniques, inpainting reduces to the problem of learning two image-feature translation functions of much smaller dimensionality.

result	We evaluate our method on several public datasets and show that we not only generate results of comparable or better visual quality, but are orders of magnitude faster than previous state-of-the-art methods.

objective	To this end, we propose a learning-based approach to generate visually coherent completion given a high-resolution image with missing components.

objective	In order to overcome the difficulty to directly learn the distribution of high-dimensional image data, we divide the task into initialization and texture-refinement as two separate steps and model each step with a deep neural network.

background	We study the task of image inpainting, which is to fill in the missing region of an incomplete image with plausible contents.

background	The mutual orthogonality among the received signals is often assumed but cannot be achieved in practice for all Doppler and delay pairs.

method	Based on the expectation maximization algorithm, we propose a method to estimate the target, correlation, and noise parameters.

method	We then use the estimates of these parameters to develop a statistical decision test.

method	Employing the asymptotic statistical characteristics and the numerical performance of the test, we analyze the sensitivity of the MIMO radar with respect to changes in the cross-correlation levels of the measurements.

method	We introduce a data model considering the correlation among the data from different transmitter-receiver pairs as unknown parameters.

objective	We demonstrate the effect of the increase in the correlation among the received signals from different transmitters on the detection performance.

background	We consider the effect of imperfect separability in the received signals on the detection performance of multi-input multi-output (MIMO) radar with widely separated antennas.

background	Plant diseases are important factors as they result in serious reduction in quality and quantity of agriculture products.

background	Therefore, early detection and diagnosis of these diseases are important.

method	To this end, we propose a deep learning-based approach that automates the process of classifying banana leaves diseases.

result	The preliminary results demonstrate the effectiveness of the proposed approach even under challenging conditions such as illumination, complex background, different resolution, size, pose, and orientation of real scene images.

method	In particular, we make use of the LeNet architecture as a convolutional neural network to classify image data sets.

objective	In this work, we aim at containing and preventing the alarming diffusion of such hate campaigns.

method	We first propose a variety of hate categories to distinguish the kind of hate.

background	While favouring communications and easing information sharing, Social Network Sites are also used to launch harmful campaigns against specific groups and individuals.

objective	Using Facebook as a benchmark, we consider the textual content of comments appeared on a set of public Italian pages.

method	We test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition.

method	Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy.

background	Cyberbullism, incitement to self-harm practices, sexual predation are just some of the severe effects of massive online offensives.

result	The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.

method	Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, we design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM).

background	Moreover, attacks can be carried out against groups of victims and can degenerate in physical violence.

other	0957-4174/$ see front matter 2008 Elsevier Ltd. A doi:10.1016/j.eswa.2008.07.006

other	: +3

other	* Corresponding author.

other	Tel.

result	Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.

objective	Our goal is to recover the common set of relevant inputs for each output cluster.

method	Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure.

background	Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso.

method	We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap.

background	We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity.

method	We present an efficient optimization method that can handle a largescale problem.

background	It considers aspects of the relevant legislation and standards when applying them to real world number plates.

method	The varied fixing methodologies and fixing locations are discussed as well as the impact on image capture.

background	This paper considers real world UK number plates and relates these to ANPR.

method	The varied manufacturing techniques and varied specifications of component parts are also noted.

method	We apply our model to real-world data collected from three MOGs that contain in total over ten years of behavioral history for millions of players and matches.

objective	In this work, we focus on understanding social relationships in MOGs.

background	Multiplayer Online Games (MOGs) like Defense of the Ancients and StarCraft II have attracted hundreds of millions of users who communicate, interact, and socialize with each other through gaming.

method	Taking match recommendation as an example application of our model, we propose SAMRA, a Socially Aware Match Recommendation Algorithm that takes social relationships into account.

method	We compare social relationships in MOGs across different game genres and with regular online social networks like Facebook.

result	We show that our model not only improves the precision of traditional link prediction approaches, but also potentially helps players enjoy games to a higher extent.

method	We propose a graph model that is able to capture social relationships of a variety of types and strengths.

background	In MOGs, rich social relationships emerge and can be used to improve gaming services such as match recommendation and game population retention, which are important for the user experience and the commercial value of the companies who run these MOGs.

result	Experiments conducted on large databases of real broadcast documents demonstrate the validity of our approach.

method	Text detection is performed in a two-step approach that combines the speed of a text localization step, enabling text size normalization, with the strength of a machine learning text veri3cation step applied on background independent features.

objective	This paper presents a new method for detecting and recognizing text in complex images and video frames.

other	All rights reserved.

method	Text recognition, applied on the detected text lines, is addressed by a text segmentation step followed by an traditional OCR algorithm within a multi-hypotheses framework relying on multiple segments, language modeling and OCR statistics.

other	Published by Elsevier Ltd.

other	?

other	2003 Pattern Recognition Society.

background	New expressions for one-sided finite-difference approximations are proposed.

other	2006 Elsevier Inc. All rights reserved.

background	The effective local truncation error is shown to be less than for higher-order one-sided finite-difference approximations but the solutions for a test problem are shown to have comparable accuracy for both approximations.

background	In these approximations the odd-order error terms are eliminated while the even-order terms are left to be taken care of by Richardson extrapolation.

background	Little empirical work has directly addressed the sources of competitive advantage of the click and mortar e-commerce approach, despite growing recognition of its importance as a business model.

background	In this paper, we introduce a framework to describe the areas of physical and virtual synergy in click and mortar enterprises, the management actions for achieving synergies and avoiding channel conflicts, and the types of benefits that may be obtained.

background	Case studies of ten US companies, including both business to consumer (B2C) and business to business (B2B) cases are used to illustrate the utility of the

background	Simple questions, which can be answered by the extraction of a single fact, constitute a large part of questions asked on the web but still pose challenges to QA systems, especially when asked against a large knowledge resource.

result	Our approach achieves results competitive with state-of-the-art end-to-end approaches that rely on an attention mechanism.

method	In this work, we follow a quite different approach: We train a neural network for answering simple questions in an end-to-end manner, leaving all decisions to the model.

background	Question Answering (QA) systems over Knowledge Graphs (KG) automatically answer natural language questions using facts contained in a knowledge graph.

background	Existing QA systems usually rely on various components each specialised in solving different sub-tasks of the problem (such as segmentation, entity recognition, disambiguation, and relation classification etc.).

method	It learns to rank subject-predicate pairs to enable the retrieval of relevant facts given a question.

method	The network contains a nested word/character-level question encoder which allows to handle out-of-vocabulary and rare word problems while still being able to exploit word-level semantics.

other	E-mail address: andreas.kamilaris@irta.cat (A. Kamilaris).

other	All rights reserved.

other	T

other	https://doi.org/10.1016/j.compag.2018.02.016 Received 14 July 2017; Received in revised form 7 February 2018; Accepted 10 February 2018 ⁎ Corresponding author.

other	Computers and Electronics in Agriculture 147 (2018) 70–90 0168-1699/ © 2018 Elsevier B.V.

method	The tabular structure is further decomposed into row and column items.

method	The text-lines and words are extracted within text zones and neighboring text-lines are merged to form text-blocks.

method	The classification algorithm labels each zone as text, table, line-drawing, halftone, ruling, or noise.

objective	This paper presents an eficient technique for document page layout structure extraction and classification by analyzing the spatial configuration of the bounding boxes of different entities on the given image.

method	The algorithm segments an image into a list of homogeneous zones.

result	Finally, the document layout hierarchy is produced from these extracted entities.

background	Characteristics of computational intelligence (CI) systems, such as adaptation, fault tolerance, high computational speed and error resilience in the face of noisy information fit the requirements of building a good intrusion detection model.

method	The research contributions in each field are systematically summarized and compared, allowing us to clearly define existing research challenges, and to highlight promising new research directions.

method	The scope of this review will be on core methods of CI, including artificial neural networks, fuzzy systems, evolutionary computation, artificial immune systems, swarm intelligence, and soft computing.

result	The findings of this review should provide useful insights into the current IDS literature and be a good source for anyone who is interested in the application of CI approaches to IDSs or related fields.

background	Intrusion detection based upon computational intelligence is currently attracting considerable interest from the research community.

objective	Here we want to provide an overview of the research progress in applying CI methods to the problem of intrusion detection.

method	In order to achieve high speed performance, the algorithm uses simple block-wise merge and sample operations.

method	For each sub-stream which has a fixed size, we compute and maintain a multi-level summary structure using a novel algorithm.

method	For data streams of size N where N is unknown in advance, our algorithm partitions the stream into sub-streams of exponentially increasing size as they arrive.

result	Overall, our algorithms for fixed-size streams and arbitrary-size streams have a computational cost of O(N log(1/epsivlogepsivN)) and an average per-element update cost of O(log logN) if epsiv is fixed.

objective	We present a fast algorithm for computing approximate quantiles in high speed data streams with deterministic error bounds.

result	This paper presents the latest version of the corpus and performance on the NIST-SRE 2010 extended task.

background	ALIZE is an open-source platform for speaker recognition.

result	Since 2005, the performance of ALIZE has been demonstrated in series of Speaker Recognition Evaluations (SREs) conducted by NIST and has been used by many participants in the last NISTSRE 2012.

method	The toolkit includes a set of high level tools dedicated to speaker recognition based on the latest developments in speaker recognition such as Joint Factor Analysis, Support Vector Machine, i-vector modelling and Probabilistic Linear Discriminant Analysis.

background	The ALIZE library implements a low-level statistical engine based on the well-known Gaussian mixture modelling.

method	Finally, the performance of the proposed principal component logistic regression model is analyzed by developing a simulation study where different methods for selecting the optimum principal components are compared.

other	© 2005 Elsevier B.V.

background	The estimation of the model parameters is not too accurate and their interpretation in terms of odds ratios may be erroneous, when there is multicollinearity (high dependence) among the predictors.

background	The logistic regression model is used to predict a binary response variable in terms of a set of explicative ones.

objective	In order to improve the estimation of the logistic model parameters under multicollinearity and to reduce the dimension of the problem with continuous covariates, it is proposed to use as covariates of the logistic model a reduced set of optimum principal components of the original predictors.

background	Other important problem is the great number of explicative variables usually needed to explain the response.

other	All rights reserved.

background	We assume a real-world setting where the user is not given direct access to a Web search engine's index, i.e., querying is possible only through an interface.

objective	One original contribution of our research is the formalization and theoretical foundation of the problem.

background	Given a set of keywords, we find a maximum Web query (containing the most keywords possible) that respects user-defined bounds on the number of returned hits.

result	The performance gain achieved with our approach is substantial: compared to the uninformed baseline (without co-occurrence information) the expected savings are up to 20% in the number of submitted queries and runtime.

objective	The goal to be optimized is the overall number of submitted Web queries.

objective	But, in particular, we develop a co-occurrence probability informed search strategy for the problem.

background	While several such trackers can be instantiated to track multiple objects independently, this not only neglects that objects should not occupy the same space in 3D, but also fails when objects have highly similar or identical appearances.

objective	In this paper we develop a probabilistic graphical model that accounts for similarity and proximity and leads to robust real-time tracking of multiple objects from RGB-D data, without recourse to bolton collision detection.

background	Most current approaches for 3D object tracking rely on distinctive object appearances.

objective	The goal of this project was to develop a novel methodology to reduce the computation time of VTA estimation.

result	We achieved a considerable reduction in the average time required to estimate the VTA, under both ideal isotropic and realistic anisotropic brain tissue conductive conditions, limiting the loss of accuracy and overcoming other drawbacks entailed by alternative methods.

background	However, the elevated computational time required to estimate the VTA with standard techniques used in biological neural modeling limits its suitability for practical use.

method	It combines a field of multi-compartment axon models coupled to the stimulating electric field with a Gaussian process classifier (GPC); following the premise that computing the VTA from a field of axons is in essence a binary classification problem.

background	The volume of tissue activated (VTA) is a well-established approach to model the direct effects of deep brain stimulation (DBS) on neural tissue and previous studies have pointed to its potential clinical applications.

objective	To that end, we built a Gaussian process emulator.

background	The course taken by the autonomous vehicle had a length of 103 km and covered rural roads, 23 small villages and major cities (e.g. downtown Mannheim and Heidelberg).

background	The historic Bertha Benz Memorial Route is particularly challenging for autonomous driving.

background	125 years after Bertha Benz completed the first overland journey in automotive history, the Mercedes Benz S-Class S 500 INTELLIGENT DRIVE followed the same route from Mannheim to Pforzheim, Germany, in fully autonomous manner.

background	The route posed a large variety of difficult traffic scenarios including intersections with and without traffic lights, roundabouts, and narrow passages with oncoming traffic.

objective	This paper gives an overview of the autonomous vehicle and presents details on vision and radar-based perception, digital road maps and video-based self-localization, as well as motion planning in complex urban scenarios.

background	The autonomous vehicle was equipped with close-to-production sensor hardware and relied solely on vision and radar sensors in combination with accurate digital maps to obtain a comprehensive understanding of complex traffic situations.

objective	We present here a new algorithm for segmentation of intensity images which is robust, rapid, and free of tuning parameters.

method	The method, however, requires the input of a number of seeds, either individual pixels or regions, which will control the formation of regions into which the image will be segmented.

method	In this correspondence, we present the algorithm, discuss briefly its properties, and suggest two ways in which it can be employed, namely, by using manual seed selection or by automated procedures.

background	We find that superimposing CobiT's conceptual model onto audit relevant assessments made by a panel of highly experienced IT auditors confirms the internal consistency between the underlying constructs of CobiT. Furthermore, we find that CobiT's conceptual model predicts auditor behavior in the field related to their seeking help and giving help as evidenced by their postings to a general IT audit listserv.

background	We empirically examine the conceptual model that underlies the CobiT internal control framework as it applies to an audit setting (including operational, compliance, and financial audit settings).

background	One commonly used framework for developing and evaluating technology intensive information systems is CobiT. This framework was originally a benchmark of best control practices developed and maintained by the Information Technology Governance Institute, the umbrella organization to the Information Systems Audit and Control Association.

result	Given the results of this study, we propose future research aimed at developing a general theory of internal control applicable to information technology based on CobiT. © 2007 Elsevier Inc. All rights reserved.

result	The experimental results demonstrated that the Max-Wins-Voting SVM with Gaussian Radial Basis kernel achieves the best classification accuracy of 88.2%.

objective	We propose a novel classification method based on a multi-class kernel support vector machine (kSVM) with the desirable goal of accurate and fast classification of fruits.

method	First, fruit images were acquired by a digital camera, and then the background of each image was removed by a split-and-merge algorithm; Second, the color histogram, texture and shape features of each fruit image were extracted to compose a feature space; Third, principal component analysis (PCA) was used to reduce the dimensions of feature space; Finally, three kinds of multi-class SVMs were constructed, i.e., Winner-Takes-All SVM, Max-Wins-Voting SVM, and Directed Acyclic Graph SVM.

result	For computation time, the Directed Acyclic Graph SVMs performs swiftest.

method	Meanwhile, three kinds of kernels were chosen, i.e., linear kernel, Homogeneous Polynomial kernel, and Gaussian Radial Basis kernel; finally, the SVMs were trained using 5-fold stratified cross validation with the reduced feature vectors as input.

background	Automatic classification of fruits via computer vision is still a complicated task due to the various properties of numerous types of fruits.

background	There has been tremendous progress in developing new methods for modeling and diagnosing reliability at the level of individual transistors, but much less work on propagating these models to higher levels of abstraction to analyze and optimize the reliability of larger circuits.

background	As CMOS technologies have shrunk to tens of nanometers, aging problems have emerged as a major challenge.

result	This talk will provide an introduction to various circuit aging mechanisms and will then discuss research that develops computer-aided design techniques for estimating and enhancing the reliability of large digital circuits, examining solutions that could practically be applied to analyze or improve the lifetime of a design while maintaining consistency to accurate device-level models and the associated physics.

background	Our approach was exemplified in a smart home setting and consists of three main tiers namely: cloud storage, overlay, and smart home.

result	Finally, we present simulation results to highlight that the overheads (in terms of traffic, processing time and energy consumption) introduced by our approach are insignificant relative to its security and privacy gains.

background	Blockchain-based approaches provide decentralized security and privacy, yet they involve significant energy, delay, and computational overhead that is not suitable for most resource-constrained IoT devices.

objective	In this paper we delve deeper and outline the various core components and functions of the smart home tier.

method	Each smart home is equipped with an always online, high resource device, known as “miner” that is responsible for handling all communication within and external to the home.

background	In our previous work, we presented a lightweight instantiation of a BC particularly geared for use in IoT by eliminating the Proof of Work (POW) and the concept of coins.

method	We show that our proposed BC-based smart home framework is secure by thoroughly analysing its security with respect to the fundamental security goals of confidentiality, integrity, and availability.

background	Internet of Things (IoT) security and privacy remain a major challenge, mainly due to the massive scale and distributed nature of IoT networks.

method	The miner also preserves a private and secure BC, used for controlling and auditing communications.

result	While there is a range of emotional intensity that can be expressed by real human faces (from neutral to full intensity), we found that the same range of emotional intensity could be expressed by artificial faces when exaggeration was used.

method	We studied the perceived intensity of emotion and perceived strangeness of faces with varying levels of realism (from schematic to photorealistic) and magnitude of facial expressions (from neutral to extremely exaggerated).

background	Exaggeration of facial expressions is used in animation and robotics to intensify emotions.

result	However, attempts to express emotional intensities outside this range using exaggeration led to strange-looking faces at all levels of realism.

result	We found that less realistic faces required more exaggeration to reach the emotional intensity of a real human face.

objective	The goal of this study was to identify the realism level and magnitude of facial expression that produce the maximum amount of emotional intensity and the minimum amount of perceived strangeness.

background	This phenomenon is known as uncanny valley.

background	However, modifying a human-like face can lead to an unsettling outcome.

method	We report results with state-of-the-art exact and heuristic methods.

objective	We propose a new set of instances ranging from 100 to 1000 customers, designed in order to provide a more comprehensive and balanced experimental setting.

background	The existing sets suffer from at least one of the following drawbacks: (i) became too easy for current algorithms; (ii) are too artificial; (iii) are too homogeneous, not covering the wide range of characteristics found in real applications.

background	The recent research on the CVRP is being slowed down by the lack of a good set of benchmark instances.

background	Many modern robotics applications require robots to function autonomously in dynamic environments including other decision making agents, such as people or other robots.

background	This requires models that take into consideration the other agent's intended actions in one's own planning.

background	This calls for fast and scalable interactive motion planning.

method	We present a real-time motion planning framework that brings together a few key components including intention inference by reasoning counterfactually about potential motion of the other agents as they work towards different goals.

result	We validate this framework with experiments involving multi-robot and human-robot navigation.

result	We further validate the tracker component separately on much larger scale unconstrained pedestrian data sets.

method	This combined approach represents a computationally efficient alternative to previously studied policy learning methods that often require significant offline training or calibration and do not yet scale to densely populated environments.

method	This inference framework is coupled with a novel distributed visual tracking method that provides reliable and robust models for the current belief-state of the monitored environment.

method	By using a light-weight motion model, we achieve efficient iterative planning for fluid motion when avoiding pedestrians, in parallel with goal inference for longer range movement prediction.

background	The drawbacks of crosscutting with respect to architectural views is similar to the drawbacks with respect to code, i.e. hampering reuse, maintenance and evolution of the architecture.

objective	This paper investigates the relations between architectural concerns, architectural drivers and views to identify why crosscutting manifests itself.

method	We propose to extend the architectural description with slices and composition mechanisms to prevent this crosscutting and perform an initial exploration of these concepts in an Online Auction system.

background	Architectural views are at the foundation of software architecture and are used to describe the system from different perspectives.

result	Within this limited setting the first results look promising to better separate concerns that otherwise would crosscut the views.

background	However, some architectural concerns crosscut the decomposition of the architecture in views.

result	The obtained results show that this algorithm is able to deal with arbitrary sets of points, and that the time to compute the polygons increases approximately linearly with the number of points.

objective	This paper describes an algorithm to compute the envelope of a set of points in a plane, which generates convex or non-convex hulls that represent the area occupied by the given points.

method	The proposed algorithm is based on a k-nearest neighbours approach, where the value of k, the only algorithm parameter, is used to control the “smoothness” of the final solution.

background	They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data.

objective	The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular.

result	Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.

background	These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels.

background	Recurrent neural networks are powerful sequence learners.

background	Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context.

method	Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences.

background	However, the experiments by Weber and Balzer which demonstrated the possibilities for increasing computation speeds, decreasing main memory space usage, and easing the task of applications programming by means of special purpose instruction sets implemented in microcode were performed some time ago.

background	Several microprogrammable mini- and microcomputers which provide the user with the means for implementing special purpose instruction sets have been introduced relatively recently.

background	The purposes of this paper are to review the microinstruction sequencing capabilities of several microprogrammed computers; to determine whether these sequencing capabilities permit easy implementation of the control constructs of flowchartable program logic in modular microcode; and to present a set of microinstruction sequencing functions which will support "structured" microprogramming.

