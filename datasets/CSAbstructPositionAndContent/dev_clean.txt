objective	0 In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions.
background	1 The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function.
background	2 This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient.
method	3 To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy.
result	4 We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.

background	1 The vision of next generation 5G wireless communications lies in providing very high data rates (typically of Gbps order), extremely low latency, manifold increase in base station capacity, and significant improvement in users' perceived quality of service (QoS), compared to current 4G LTE networks.
background	2 Ever increasing proliferation of smart devices, introduction of new emerging multimedia applications, together with an exponential rise in wireless data (multimedia) demand and usage is already creating a significant burden on existing cellular networks.
objective	3 5G wireless systems, with improved data rates, capacity, latency, and QoS are expected to be the panacea of most of the current cellular networks' problems.
method	4 In this survey, we make an exhaustive review of wireless evolution toward 5G networks.
method	5 We first discuss the new architectural changes associated with the radio access network (RAN) design, including air interfaces, smart antennas, cloud and heterogeneous RAN.
method	6 Subsequently, we make an in-depth survey of underlying novel mm-wave physical layer technologies, encompassing new channel model estimation, directional antenna design, beamforming algorithms, and massive MIMO technologies.
method	7 Next, the details of MAC layer protocols and multiplexing schemes needed to efficiently support this new physical layer are discussed.
method	8 We also look into the killer applications, considered as the major driving force behind 5G.
method	9 In order to understand the improved user experience, we provide highlights of new QoS, QoE, and SON features associated with the 5G evolution.
method	10 For alleviating the increased network energy consumption and operating expenditure, we make a detail review on energy awareness and cost efficiency.

background	1 While recent literature has partly advanced our understanding of mobile application business models, less attention has been given to identify the most successful business model especially in the game area.
objective	2 In this vein, this paper presents a comparative analysis of six Iranian android games in an app store “Iran Apps” which were top level games in terms of download and rate during a 3-month study.
result	3 The results reveal that freemium and In-app purchase models were more successful, while advertising model can be efficiently used as a complementary model.
result	4 This provides valuable insight into why some mobile applications are more successful than others.

background	1 Instagram is the fastest growing social network site globally.
objective	2 This study investigates motives for its use, and its relationship to contextual age and narcissism.
objective	3 A survey of 239 college students revealed that the main reasons for Instagram use are “Surveillance/Knowledge about others,” “Documentation,” “Coolness,” and “Creativity.
other	4 ”
result	5 The next significant finding was a positive relationship between those who scored high in interpersonal interaction and using Instagram for coolness, creative purposes, and surveillance.
result	6 Another interesting finding shows that there is a positive relationship between high levels of social activity (traveling, going to sporting events, visiting friends, etc.) and being motivated to use Instagram as a means of documentation.
result	7 In reference to narcissism, there was a positive relationship between using Instagram to be cool and for surveillance.
result	8 Theoretical contributions of this study relate to our understanding of uses and gratifications theory.
result	9 This study uncovers new motives for social media use not identified in previous literature.
other	10 © 2015 Elsevier Ltd.

background	1 Previous research has demonstrated that the experience of reading e-books is not equivalent to reading textbooks.
background	2 This study examines factors influencing preference for e-books as well as reported use of e-book content.
background	3 Although the present student cohort is the most technologically savvy to ever enter universities, students do not prefer e-books over textbooks regardless of their gender, computer use or comfort with computers.
background	4 No significant correlations existed between the number of e-books previously used and overall preference of e-books: Participants who had previously used an e-book still preferred print texts for learning.
background	5 Despite the ability to easily access supplemental content through e-books via hyperlinks and other features, students were more likely to use special features in print books than in e-books.
other	6 2010 Elsevier Ltd.
other	7 All rights reserved.

background	1 Digital market has never been so unstable due to more and more demanding users and new disruptive competitors.
objective	2 CEOs from most of industries investigate digitalization opportunities.
method	3 Through a Systematic Literature Review, we found that digital transformation is more than just a technological shift.
method	4 According to this study, these transformations have had an impact on the business models, the operational processes and the end-users experience.
method	5 Considering the richness of this topic, we had proposed a research agenda of digital transformation in a managerial perspective.

background	1 Spiking Neuron Networks (SNNs) are often referred to as the 3rd generation of neural networks.
background	2 Highly inspired from natural computing in the brain and recent advances in neurosciences, they derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike firing.
background	3 SNNs overcome the computational power of neural networks made of threshold or sigmoidal units.
method	4 Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation.
objective	5 Today, the main challenge is to discover efficient learning rules that might take advantage of the specific features of SNNs while keeping the nice properties (general-purpose, easy-to-use, available simulators, etc.
other	6 ) of traditional connectionist models.
background	7 This chapter relates the history of the “spiking neuron” in Section 1 and summarizes the most currently-in-use models of neurons and synaptic plasticity in Section 2.
result	8 The computational power of SNNs is addressed in Section 3 and the problem of learning in networks of spiking neurons is tackled in Section 4, with insights into the tracks currently explored for solving it.
result	9 Finally, Section 5 discusses application domains, implementation issues and proposes several simulation frameworks.
background	10 1 Professor at Universit de Lyon Laboratoire de Recherche en Informatique INRIA CNRS bat.

background	1 We describe ITS4, a tool for statically scanning security-critical C source code for vulnerabilities.
background	2 Compared to other approaches, our scanning technique stakes out a new middle ground between accuracy and eficiency.
method	3 This method is eficient enough to offer real-time feedback to developers during coding while producing few false negatives.
method	4 Unlike other techniques, our method is also simple enough to scan C + + code despite the complexities inherent in the language.
result	5 Using ITS4 we found new remotelyexploitable vulnerabilities in a widely distributed software package as well as in a major piece of e-commerce software.
other	6 The ITS4 source distribution is available at h t tp : //www.rstcorp.
other	7 com/its4.

background	1 Scholars in many disciplines have considered the antecedents and consequences of various forms of trust.
objective	2 This paper generates 11 propositions exploring the relationship between Human Resource Information Systems (HRIS) and the trust an individual places in the inanimate technology (technology trust) and models the effect of those relationships on HRIS implementation success.
method	3 Specifically, organizational, technological, and user factors are considered and modeled to generate a set of testable propositions that can subsequently be investigated in various organizational settings.
method	4 Eleven propositions are offered suggesting that organizational trust, pooled interdependence, organizational community, organizational culture, technology adoption, technology utility, technology usability, socialization, sensitivity to privacy, and predisposition to trust influence an individual’s level of trust in the HRIS technology (technology trust) and ultimately the success of an HRIS implementation process.
result	5 A summary of the relationships between the key constructs in the model and recommendations for future research are provided.

background	1 Credit card plays a very important rule in today's economy.
background	2 It becomes an unavoidable part of household, business and global activities.
background	3 Although using credit cards provides enormous benefits when used carefully and responsibly,significant credit and financial damagesmay be causedby fraudulent activities.
background	4 Many techniques have been proposed to confront thegrowthin credit card fraud.
background	5 However, all of these techniques have the same goal of avoiding the credit card fraud; each one has its own drawbacks, advantages and characteristics.
objective	6 In this paper, after investigating difficultiesof credit card fraud detection, we seek to review the state of the art in credit card fraud detection techniques, datasets and evaluation criteria.
method	7 The advantages and disadvantages of fraud detection methods are enumerated and compared.
method	8 Furthermore, a classification of mentioned techniques into two main fraud detection approaches, namely, misuses (supervised) and anomaly detection (unsupervised) is presented.
method	9 Again, a classification of techniques is proposed based on capability to process the numerical and categorical datasets.
method	10 Different datasets used in literatureare then described and grouped into real and synthesized data and the effective and common attributesare extracted for further usage.

background	1 This paper proposes a new algorithm for K-medoids clustering which runs like the K-means algorithm and tests several methods for selecting initial medoids.
background	2 The proposed algorithm calculates the distance matrix once and uses it for finding new medoids at every iterative step.
background	3 To evaluate the proposed algorithm, we use some real and artificial data sets and compare with the results of other algorithms in terms of the adjusted Rand index.
method	4 Experimental results show that the proposed algorithm takes a significantly reduced time in computation with comparable performance against the partitioning around medoids.
other	5 2008 Elsevier Ltd.
other	6 All rights reserved.

objective	1 In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN).
method	2 We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image.
method	3 Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space.
method	4 The mapped vector is then passed through the generative model to predict the missing content.
result	5 The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80% corruption and large blocky corruption.
result	6 Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods.

background	1 We present and explore the effectiveness of several variations on the All-Moves-As-First (AMAF) heuristic in Monte-Carlo Go.
result	2 Our results show that: • Random play-outs provide more information about the goodness of moves made earlier in the play-out.
background	3 • AMAF updates are not just a way to quickly initialize counts, they are useful after every play-out.
background	4 • Updates even more aggressive than AMAF can be even

background	1 Distant supervision for relation extraction is an efficient method to scale relation extraction to very large corpora which contains thousands of relations.
background	2 However, the existing approaches have flaws on selecting valid instances and lack of background knowledge about the entities.
method	3 In this paper, we propose a sentence-level attention model to select the valid instances, which makes full use of the supervision information from knowledge bases.
method	4 And we extract entity descriptions from Freebase and Wikipedia pages to supplement background knowledge for our task.
result	5 The background knowledge not only provides more information for predicting relations, but also brings better entity representations for the attention module.
result	6 We conduct three experiments on a widely used dataset and the experimental results show that our approach outperforms all the baseline systems significantly.

background	1 Increasing use of the World Wide Web as a B2C commercial tool raises interest in understanding the key issues in building relationships with customers on the Internet.
background	2 Trust is believed to be the key to these relationships.
background	3 Given the differences between a virtual and a conventional marketplace, antecedents and consequences of trust merit re-examination.
objective	4 This research identifies a number of key factors related to trust in the B2C context and proposes a framework based on a series of underpinning relationships among these factors.
objective	5 The findings in this research suggest that people are more likely to purchase from the web if they perceive a higher degree of trust in e-commerce and have more experience in using the web.
result	6 Customer’s trust levels are likely to be influenced by the level of perceived market orientation, site quality, technical trustworthiness, and user’s web experience.
method	7 People with a higher level of perceived site quality seem to have a higher level of perceived market orientation and trustworthiness towards e-commerce.
method	8 Furthermore, people with a higher level of trust in e-commerce are more likely to participate in e-commerce.
method	9 Positive ‘word of mouth’, money back warranty and partnerships with well-known business partners, rank as the top three effective risk reduction tactics.
result	10 These findings complement the previous findings on e-commerce and shed light on how to establish a trust relationship on the World Wide Web.

background	1 Most of a cell’s functional processes involve interactions among proteins, and a key challenge in proteomics is to better understand these complex interaction graphs at a systems level.
background	2 Because of their importance in development and disease, protein-protein interactions (PPIs) have been the subject of intense research in recent years.
background	3 In addition, a greater understanding of PPIs can be achieved through the detailed investigation of the protein domain interactions which mediate PPIs.
background	4 In this chapter, we describe recent efforts to predict interactions between proteins and between protein domains.
method	5 We also describe methods that attempt to use protein interaction data to infer protein function.
method	6 Protein-protein interactions directly contribute to protein functions, and implications about functions can often be made via PPI studies.
background	7 These inferences are based on the premise that the function of a protein may be discovered by studying its interaction with one or more proteins of known functions.
result	8 The second part of this chapter reviews recent computational approaches to predict protein functions from PPI networks.

background	1 Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.
objective	2 In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.
objective	3 The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.
method	4 We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.
method	5 Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.
result	6 We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.

background	1 This paper presents a new algorithm to implement causal ordering.
background	2 Causal ordering was first proposed in the ISIS system developed at Cornell University.
method	3 The interest of causal ordering in a distr ibuted system is that it is cheaper to realize than total ordering.
method	4 The implementation of causal ordering proposed in this paper uses logical clocks of Mat te rn-Fidge (which define a partial order between events in a distr ibuted system) and presents two advantages over the implementation in ISIS: (1) the information added to messages to ensure causal ordering is bounded by the number of sites in the system, and (2) no special protocol is needed to dispose of this added information when it has become useless.
result	5 The implementation of ISIS presents however advantages in the case of site failures.

objective	1 In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed to scale simple OLTP-style application loads over many servers.
method	2 Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses.
method	3 We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions.
result	4 These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.

background	1 The wealth of social information presented on Facebook is astounding.
background	2 While these affordances allow users to keep up-to-date, they also produce a basis for social comparison and envy on an unprecedented scale.
background	3 Even though envy may endanger users’ life satisfaction and lead to platform avoidance, no study exists uncovering this dynamics.
background	4 To close this gap, we build on responses of 584 Facebook users collected as part of two independent studies.
background	5 In study 1, we explore the scale, scope, and nature of envy incidents triggered by Facebook.
objective	6 In study 2, the role of envy feelings is examined as a mediator between intensity of passive following on Facebook and users’ life satisfaction.
method	7 Confirming full mediation, we demonstrate that passive following exacerbates envy feelings, which decrease life satisfaction.
result	8 From a provider’s perspective, our findings signal that users frequently perceive Facebook as a stressful environment, which may, in the long-run, endanger platform sustainability.

background	1 Speech separation is the task of separating target speech from background interference.
result	2 Traditionally, speech separation is studied as a signal processing problem.
background	3 A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data.
objective	4 Over the past decade, many supervised separation algorithms have been put forward.
method	5 In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance.
result	6 This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years.
method	7 We first introduce the background of speech separation and the formulation of supervised separation.
method	8 Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features.
objective	9 Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement speech-nonspeech separation, speaker separation multitalker separation, and speech dereverberation, as well as multimicrophone techniques.
method	10 The important issue of generalization, unique to supervised learning, is discussed.

background	1 This paper presents a new feature extraction algorithm for the challenging problem of the classification of myoelectric signals for prostheses control.
background	2 The algorithm employs the orientation between a set of descriptors of muscular activities and a nonlinearly mapped version of them.
background	3 It incorporates information about the Electromyogram (EMG) signal power spectrum characteristics derived from each analysis window while correlating that with the descriptors of previous windows for robust activity recognition.
objective	4 The proposed idea can be summarized in the following three steps: 1) extract power spectrum moments from the current analysis window and its nonlinearly scaled version in time-domain through Fourier transform relations, 2) compute the orientation between the two sets of moments, and 3) apply data fusion on the resulting orientation features for the current and previous time windows and use the result as the final feature set.
result	5 EMG data collected from nine transradial amputees performing six classes of movements with different force levels is used to validate the proposed features.
result	6 When compared to other well-known EMG feature extraction methods, the proposed features produced an improvement of at least 4%.

background	1 Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite.
method	2 We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author.
method	3 We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm.
method	4 We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task.
result	5 Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.

background	1 This paper presents a consensus model in group decision making under linguistic assessments.
background	2 It is based on the use of linguistic preferences to provide individuals' opinions, and on the use of fuzzy majority of consensus, represented by means of a linguistic quantifier.
method	3 Several linguistic consensus degrees and linguistic distances are defined, acting on three levels.
method	4 The consensus degrees indicate how far a group of individuals is from the maximum consensus, and linguistic distances indicate how far each individual is from current consensus labels over the preferences.
result	5 This consensus model allows to incorporate more human consistency in decision support systems.

background	1 Mobile business is a young promising industry created by the emergence of wireless data networks.
background	2 Similar to other emerging industries, it is characterized by a large number of uncertainties at different levels, in particular concerning technology, business strategy and consumer demand.
objective	3 This paper focuses on the strategic uncertainties, where a large number of actors are trying a number of strategic approaches to position themselves in the most favourable position in the value system.
method	4 This paper intends to apply a business model analysis methodology in order to better understand the strategic approaches of these actors.
objective	5 We argue that successful business models are likely to be the ones that best address the economic peculiarities underlying this industry, like mobility, network effects and natural monopolies.

objective	1 This paper addresses the problem of generating possible object locations for use in object recognition.
method	2 We introduce selective search which combines the strength of both an exhaustive search and segmentation.
method	3 Like segmentation, we use the image structure to guide our sampling process.
method	4 Like exhaustive search, we aim to capture all possible object locations.
method	5 Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible.
result	6 Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations.
result	7 The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition.
method	8 In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition.
other	9 The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).

background	1 Words have only one purpose in a technical context–the transmission of information.
background	2 When they fail to do that, they lead to confusion and misunderstanding.
method	3 "Distributed data processing" and "distributed processing" are two phrases which illustrate that axiom.
background	4 Like many other words in the lexicon of the computer professional, these have become cliches through over-use, losing much of their original meaning in the process.
objective	5 This paper is an attempt to reverse that trend.

background	1 Generative Adversarial Networks have emerged as an effective technique for estimating data distributions.
objective	2 The basic setup consists of two deep networks playing against each other in a zero-sum game setting.
objective	3 However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible.
method	4 The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature.
method	5 In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs.
result	6 In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games.
result	7 Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.

background	1 Blendshapes”, a simple linear model of facial expression, is the prevalent approach to realistic facial animation.
background	2 It has driven animated characters in Hollywood films, and is a standard feature of commercial animation packages.
background	3 The blendshape approach originated in industry, and became a subject of academic research relatively recently.
method	4 This course describes the published state of the art in this area, covering both literature from the graphics research community, and developments published in industry forums.
method	5 We show that, despite the simplicity of the blendshape approach, there remain open problems associated with this fundamental technique.

objective	1 This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking.
method	2 In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking.
method	3 We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs.
method	4 We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.

background	1 Social media is bringing great challenges and wonderful opportunities for organizational learning.
method	2 With support of social media, organizations may facilitate the knowledge management process within firms (e.g., knowledge sharing), then to encourage employees to promote collaborative learning behaviors from e-learning to social learning.
method	3 There is a significant trend in the recent studies is increasing number of publications on social media supported knowledge management (SMKM).
background	4 However, previous SMKM studies have not been depicted well by combining work of both researchers in social media study and ones in KM (which supports organizational learning) study.
objective	5 By using CiteSpace, this paper mapped important references that lead trends of SMKM development, authors contributing greatly to this field and hot topics of all the related articles.
method	6 The way that SMKM study developed was analyzed according to the visualization of references and topics.
method	7 Furthermore, the two most important groups – topics from SM and those from KM study were studied respectively to compare their development in order to show the fusion, the separation and other relationship.
method	8 Finally, hottest trends and topics in these years and recent future were discussed to provide help for future work.
other	9 2014 Elsevier Ltd. All rights reserved.

background	1 Scalable and effective exploration remains a key challenge in reinforcement learning (RL).
background	2 While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios.
background	3 As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls.
objective	4 This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent’s belief of environment dynamics.
method	5 We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces.
method	6 VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms.
result	7 We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.

background	1 A common way of dynamically scheduling jobs in a manufacturing system is by implementing dispatching rules.
method	2 The issues with this method are that the performance of these rules depends on the state the system is in at each moment and also that no “ideal” single rule exists for all the possible states that the system may be in.
method	3 Therefore, it would be interesting to use the most appropriate dispatching rule for each instance.
method	4 To achieve this goal, a scheduling approach that uses machine learning can be used.
method	5 Analyzing the previous performance of the system (training examples) by means of this technique, knowledge is obtained that can be used to decide which is the most appropriate dispatching rule at each moment in time.
result	6 In this paper, a literature review of the main machine learning based scheduling approaches from the last decade is presented.

background	1 The linear combination of forecasts is a procedure that has improved the forecasting accuracy for different time series.
background	2 In this procedure, each method being combined is associated to a numerical weight that indicates the contribution of the method in the combined forecast.
method	3 We present the use of machine learning techniques to define the weights for the linear combination of forecasts.
method	4 In this paper, a machine learning technique uses features of the series at hand to define the adequate weights for a pre-defined number of forecasting methods.
result	5 In order to evaluate this solution, we implemented a prototype that uses a MLP network to combine two widespread methods.
result	6 The experiments performed revealed significantly accurate forecasts.

background	1 This paper presents a survey on the current state-of-the-art in Wireless Sensor Network (WSN) Operating Systems (OSs).
background	2 In recent years, WSNs have received tremendous attention in the research community, with applications in battlefields, industrial process monitoring, home automation, and environmental monitoring, to name but a few.
background	3 A WSN is a highly dynamic network because nodes die due to severe environmental conditions and battery power depletion.
background	4 Furthermore, a WSN is composed of miniaturized motes equipped with scarce resources e.g., limited memory and computational abilities.
background	5 WSNs invariably operate in an unattended mode and in many scenarios it is impossible to replace sensor motes after deployment, therefore a fundamental objective is to optimize the sensor motes' life time.
background	6 These characteristics of WSNs impose additional challenges on OS design for WSN, and consequently, OS design for WSN deviates from traditional OS design.
objective	7 The purpose of this survey is to highlight major concerns pertaining to OS design in WSNs and to point out strengths and weaknesses of contemporary OSs for WSNs, keeping in mind the requirements of emerging WSN applications.
method	8 The state-of-the-art in operating systems for WSNs has been examined in terms of the OS Architecture, Programming Model, Scheduling, Memory Management and Protection, Communication Protocols, Resource Sharing, Support for Real-Time Applications, and additional features.
result	9 These features are surveyed for both real-time and non-real-time WSN operating systems.

background	1 Biham and Kocher demonstrated that the PKZIP stream cipher was weak and presented an attack requiring thirteen bytes of plaintext.
background	2 The deflate algorithm “zippers” now use to compress the plaintext before encryption makes it difficult to get known plaintext.
method	3 We consider the problem of reducing the amount of known plaintext by finding other ways to filter key guesses.
method	4 In most cases we can reduce the amount of known plaintext from the archived file to two or three bytes, depending on the zipper used and the number of files in the archive.
method	5 For the most popular zippers on the Internet, there is a fast attack that does not require any information about the files in the archive; instead, it gets doubly-encrypted plaintext by exploiting a weakness in the pseudorandom-number generator.

background	1 The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited.
background	2 Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users’ characteristics and profile information.
method	3 In this paper we show that taking user context into account improves the detection of cyberbullying.

background	1 The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness.
background	2 They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture.
objective	3 They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality.
method	4 The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative.
method	5 A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end.
result	6 The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.

background	1 A new, simple, and symmetric algorithm can be implemented that results in higher levels of detail in solid objects than previously possible with autostereograms.
background	2 In a stereoscope, an optical instrument similar to binoculars, each eye views a different picture and thereby receives the specific image that would have arisen naturally.
background	3 An early suggestion for a color stereo computer display involved a rotating filter wheel held in front of the eyes.
method	4 In contrast, this article describes a method for viewing on paper or on an ordinary computer screen without special equipment, although it is limited to the display of 3D monochromatic objects.
method	5 (The image can be colored, say, for artistic reasons, but the method we describe does not allow colors to be allocated in a way that corresponds to an arbitrary coloring of the solid object depicted.)
result	6 The image can easily be constructed by computer from any 3D scene or solid object description.<<ETX>>

background	1 Downloading the book in this website lists can give you more advantages.
background	2 It will show you the best book collections and completed collections.
background	3 So many books can be found in this website.
background	4 So, this is not only this ways of knowing in hci.
method	5 However, this book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts.
method	6 This is simple, read the soft file of the book and you get it.

background	1 This paper presents a simulated memristor crossbar implementation of a deep Convolutional Neural Network (CNN).
background	2 In the past few years deep neural networks implemented on GPU clusters have become the state of the art in image classification.
background	3 They provide excellent classification ability at the cost of a more complex data manipulation process.
method	4 However once these systems are trained, we show that the analog crossbar circuits in this paper can highly parallelize the recognition phase of a CNN algorithm.
method	5 One of the drawbacks of using memristors to carry out computations is that the data stored will likely have less precision when compared to typical 32-bit floating point memory.
method	6 However, we show the proposed system is capable of operating with zero loss in classification accuracy if the memristors utilized are able to store at least 16 unique values (essentially acting as 4-bit devices).
result	7 To the best of our knowledge, this is the first paper that presents a memristor based circuit for implementing CNN recognition.
result	8 This is also the first paper that provides a circuit for precise memristor based analog convolution.

method	1 We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN.
method	2 As the DNN performs a mapping from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers.
method	3 When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model, (2) the student DNN outperforms the original DNN, and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.

background	1 We propose the use of dimensionality reduction as a defense against evasion attacks on ML classifiers.
background	2 We present and investigate a strategy for incorporating dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase.
result	3 We empirically evaluate and demonstrate the feasibility of dimensionality reduction of data as a defense mechanism against evasion attacks using multiple real-world datasets.
result	4 Our key findings are that the defenses are (i) effective against strategic evasion attacks in the literature, increasing the resources required by an adversary for a successful attack by a factor of about two, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification, and human activity classification.

objective	1 In this paper we critically review task analysis models and techniques.
result	2 These approaches to task analysis are discussed in order to develop a richer picture of human activity, while analyzing their limitations, general weaknesses, and possibilities for improvement.
method	3 We consider their ability to determine the appropriate set of atomic actions in a task, their effect on workers’ motivational needs, their support of users’ cognitive and sociocultural processes, and their effectiveness in supporting interface design.
result	4 We note that the major approaches have focused on very different levels of analysis, and call for greater integration of these different levels in task analysis theory.

background	1 The exponential growth and availability of data in all forms is the main booster to the continuing evolution in the communications industry.
background	2 The popularization of traffic-intensive applications including high definition video, 3-D visualization, augmented reality, wearable devices, and cloud computing defines a new era of mobile communications.
background	3 The immense amount of traffic generated by today's customers requires a paradigm shift in all aspects of mobile networks.
background	4 Ultradense network (UDN) is one of the leading ideas in this racetrack.
background	5 In UDNs, the access nodes and/or the number of communication links per unit area are densified.
objective	6 In this paper, we provide a survey-style introduction to dense small cell networks.
method	7 Moreover, we summarize and compare some of the recent achievements and research findings.
method	8 We discuss the modeling techniques and the performance metrics widely used to model problems in UDN.
method	9 Also, we present the enabling technologies for network densification in order to understand the state-of-the-art.
method	10 We consider many research directions in this survey, namely, user association, interference management, energy efficiency, spectrum sharing, resource management, scheduling, backhauling, propagation modeling, and the economics of UDN deployment.

background	1 Lane detection is to detect lanes on the road and provide the accurate location and shape of each lane.
background	2 It severs as one of the key techniques to enable modern assisted and autonomous driving systems.
method	3 However, several unique properties of lanes challenge the detection methods.
method	4 The lack of distinctive features makes lane detection algorithms tend to be confused by other objects with similar local appearance.
method	5 Moreover, the inconsistent number of lanes on a road as well as diverse lane line patterns, e.g. solid, broken, single, double, merging, and splitting lines further hamper the performance.
method	6 In this paper, we propose a deep neural network based method, named LaneNet, to break down the lane detection into two stages: lane edge proposal and lane line localization.
method	7 Stage one uses a lane edge proposal network for pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based on lane edge proposals.
method	8 Please note that the goal of our LaneNet is built to detect lane line only, which introduces more difficulties on suppressing the false detections on the similar lane marks on the road like arrows and characters.
method	9 Despite all the difficulties, our lane detection is shown to be robust to both highway and urban road scenarios method without relying on any assumptions on the lane number or the lane line patterns.
result	10 The high running speed and low computational cost endow our LaneNet the capability of being deployed on vehicle-based systems.

background	1 Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years.
background	2 A number of GAN variants have been proposed and have been utilized in many applications.
background	3 Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task.
background	4 While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison.
background	5 As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field.
objective	6 In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models.
result	7 I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.

background	1 Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as “the curse of dimensionality”.
objective	2 This paper presents a deep learning-based approach that can handle general high-dimensional parabolic PDEs.
background	3 To this end, the PDEs are reformulated as a control theory problem and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function.
result	4 Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and speed.
result	5 This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.

background	1 The results of application of multichannel Kalman filtering to reduction of uncorrelated noise in magnetotelluric recordings are discussed in this article.
background	2 Magnetotelluric method of Earth structure recognition is shortly presented together with the its most popular measurement method called the remote reference method.
method	3 The basic theory of nonstationar, discrete Kalman filter and its implementation to multichannel magnetotelluric data recorded in multi-site experiment are also discussed with details.
result	4 The practical examples of Kalman filter application to the real 2D and 3D data illustrate the merits of presented technique.

background	1 We consider the effect of imperfect separability in the received signals on the detection performance of multi-input multi-output (MIMO) radar with widely separated antennas.
background	2 The mutual orthogonality among the received signals is often assumed but cannot be achieved in practice for all Doppler and delay pairs.
method	3 We introduce a data model considering the correlation among the data from different transmitter-receiver pairs as unknown parameters.
method	4 Based on the expectation maximization algorithm, we propose a method to estimate the target, correlation, and noise parameters.
method	5 We then use the estimates of these parameters to develop a statistical decision test.
method	6 Employing the asymptotic statistical characteristics and the numerical performance of the test, we analyze the sensitivity of the MIMO radar with respect to changes in the cross-correlation levels of the measurements.
objective	7 We demonstrate the effect of the increase in the correlation among the received signals from different transmitters on the detection performance.

background	1 Interest in human-robot coexistence, in which humans and robots share a common work volume, is increasing in manufacturing environments.
background	2 Efficient work coordination requires both awareness of the human pose and a plan of action for both human and robot agents in order to compute robot motion trajectories that synchronize naturally with human motion.
objective	3 In this paper, we present a data-driven approach that synthesizes anticipatory knowledge of both human motions and subsequent action steps in order to predict in real-time the intended target of a human performing a reaching motion.
method	4 Motion-level anticipatory models are constructed using multiple demonstrations of human reaching motions.
method	5 We produce a library of motions from human demonstrations, based on a statistical representation of the degrees of freedom of the human arm, using time series analysis, wherein each time step is encoded as a multivariate Gaussian distribution.
method	6 We demonstrate the benefits of this approach through offline statistical analysis of human motion data.
result	7 The results indicate a considerable improvement over prior techniques in early prediction, achieving 70% or higher correct classification on average for the first third of the trajectory (<; 500msec).
method	8 We also indicate proof-of-concept through the demonstration of a human-robot cooperative manipulation task performed with a PR2 robot.
background	9 Finally, we analyze the quality of task-level anticipatory knowledge required to improve prediction performance early in the human motion trajectory.

background	1 Predicting stock market movements is a well-known problem of interest.
background	2 Now-a-days social media is perfectly representing the public sentiment and opinion about current events.
background	3 Especially, Twitter has attracted a lot of attention from researchers for studying the public sentiments.
background	4 Stock market prediction on the basis of public sentiments expressed on Twitter has been an intriguing field of research.
background	5 Previous studies have concluded that the aggregate public mood collected from Twitter may well be correlated with Dow Jones Industrial Average Index (DJIA).
background	6 The thesis of this work is to observe how well the changes in stock prices of a company, the rises and falls, are correlated with the public opinions being expressed in tweets about that company.
objective	7 Understanding author's opinion from a piece of text is the objective of sentiment analysis.
method	8 The present paper have employed two different textual representations, Word2vec and N-gram, for analyzing the public sentiments in tweets.
objective	9 In this paper, we have applied sentiment analysis and supervised machine learning principles to the tweets extracted from Twitter and analyze the correlation between stock market movements of a company and sentiments in tweets.
method	10 In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase.

background	1 Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era.
background	2 In the past Richard Watson was the accepting senior editor for this paper.
method	3 MISQ Review articles survey, conceptualize, and synthesize prior MIS research and set directions for future research.
other	4 For more details see http://www.misq.org/misreview/announce.html few years, however, there has been a growing interest in treating knowledge as a significant organizational resource.
result	5 Consistent with the interest in organizational knowledge and knowledge management (KM), IS researchers have begun promoting a class of information systems, referred to as knowledge management systems (KMS).
method	6 The objective of KMS is to support creation, transfer, and application of knowledge in organizations.
result	7 Knowledge and knowledge management are complex and multi-faceted concepts.
result	8 Thus, effective development and implementation of KMS requires a foundation in several rich

background	1 Even the support vector machine (SVM) has been proposed to provide a good generalization performance, the classification result of the practically implemented SVM is often far from the theoretically expected level because their implementations are based on the approximated algorithms due to the high complexity of time and space.
method	2 To improve the limited classification performance of the real SVM, we propose to use the SVM ensembles with bagging (bootstrap aggregating).
method	3 Each individual SVM is trained independently using the randomly chosen training samples via a bootstrap technique.
method	4 Then, they are aggregated into to make a collective decision in several ways such as the majority voting, the LSE(least squares estimation)-based weighting, and the double-layer hierarchical combining.
result	5 Various simulation results for the IRIS data classification and the hand-written digit recognitionshow that the proposed SVM ensembles with bagging outperforms a single SVM in terms of classification accuracy greatly.

background	1 Most pseudorandom number generators (PRNGs) scale poorly to massively parallel high-performance computation because they are designed as sequentially dependent state transformations.
background	2 We demonstrate that independent, keyed transformations of counters produce a large alternative class of PRNGs with excellent statistical properties (long period, no discernable structure or correlation).
method	3 These counter-based PRNGs are ideally suited to modern multi-core CPUs, GPUs, clusters, and special-purpose hardware because they vectorize and parallelize well, and require little or no memory for state.
method	4 We introduce several counter-based PRNGs: some based on cryptographic standards (AES, Threefish) and some completely new (Philox).
result	5 All our PRNGs pass rigorous statistical tests (including TestU01's BigCrush) and produce at least 264 unique parallel streams of random numbers, each with period 2128 or more.
result	6 In addition to essentially unlimited parallel scalability, our PRNGs offer excellent single-chip performance: Philox is faster than the CURAND library on a single NVIDIA GPU.

background	1 In recent years, there has been an explosion of interest in mining time series databases.
background	2 As with most computer science problems, representation of the data is the key to efficient and effective solutions.
background	3 One of the most commonly used representations is piecewise linear approximation.
method	4 This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time series data.
method	5 A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times.
objective	6 In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques.
result	7 We show that all these algorithms have fatal flaws from a data mining perspective.
result	8 We introduce a novel algorithm that we empirically show to be superior to all others in the literature.

background	1 The energy consumption rate for sensors in a wireless sensor network vary greatly depending on the protocols the sensors use for communication.
objective	2 The gossip-based sleep protocol (GSP) (X. Hou et al., 2004) is an example of a protocol that implements routing and some MAC functions in an effort to conserve energy.
background	3 Simulations show that GSP can conserve energy.
background	4 We expand on this effort by building a prototype system and measuring energy consumption rates.
background	5 GSP was implemented on the Mica2 platform and measurements were conducted to determine energy consumption.
method	6 The measurements were then used to build an energy consumption model for GSP

background	1 The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain.
background	2 Deep learning uses multiple layers to represent the abstractions of data to build computational models.
background	3 Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing.
background	4 However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective.
background	5 The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level.
background	6 Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth.
objective	7 This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications.
objective	8 It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.

background	1 We present a novel clustering algorithm for tagging a face dataset (e. g., a personal photo album).
result	2 The core of the algorithm is a new dissimilarity, called Rank-Order distance, which measures the dissimilarity between two faces using their neighboring information in the dataset.
method	3 The Rank-Order distance is motivated by an observation that faces of the same person usually share their top neighbors.
method	4 Specifically, for each face, we generate a ranking order list by sorting all other faces in the dataset by absolute distance (e. g., L1 or L2 distance between extracted face recognition features).
method	5 Then, the Rank-Order distance of two faces is calculated using their ranking orders.
method	6 Using the new distance, a Rank-Order distance based clustering algorithm is designed to iteratively group all faces into a small number of clusters for effective tagging.
result	7 The proposed algorithm outperforms competitive clustering algorithms in term of both precision/recall and efficiency.

background	1 A large number of absolute pose algorithms have been presented in the literature.
background	2 Common performance criteria are computational complexity, geometric optimality, global optimality, structural degeneracies, and the number of solutions.
background	3 The ability to handle minimal sets of correspondences, resulting solution multiplicity, and generalized cameras are further desirable properties.
method	4 This paper presents the first PnP solution that unifies all the above desirable properties within a single algorithm.
method	5 We compare our result to state-of-the-art minimal, non-minimal, central, and non-central PnP algorithms, and demonstrate universal applicability, competitive noise resilience, and superior computational efficiency.
result	6 Our algorithm is called Unified PnP (UPnP).

background	1 We introduce a novel approach to automatically recover 3D human pose from a single image.
background	2 Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose.
method	3 Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly.
method	4 In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems.
method	5 For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms.
result	6 Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.

background	1 Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues.
background	2 Researchers in many di erent elds have shown great interest in data mining.
background	3 Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities.
objective	4 In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently.
method	5 A classi cation of the available data mining techniques is provided and a comparative study of such techniques is presented.
result	6 Index Terms | Data mining, knowledge discovery, association rules, classi cation, data clustering, pattern matching algorithms, data generalization and characterization, data cubes, multiple-dimensional databases.
other	7 J. Han was supported in part by the research grant NSERC-A3723 from the Natural Sciences and Engineering Research Council of Canada, the research grant NCE:IRIS/Precarn-HMI5 from the Networks of Centres of Excellence of Canada, and research grants from MPR Teltech Ltd. and Hughes Research Laboratories.

background	1 A range of algebras between lattices and Boolean algebras generalise the notion of a complement.
method	2 We develop a hierarchy of these pseudo-complemented algebras that includes Stone algebras.
method	3 Independently of this theory we study filters based on partial orders.
method	4 Both theories are combined to prove Chen and Grätzer’s construction theorem for Stone algebras.
result	5 The latter involves extensive reasoning about algebraic structures in addition to reasoning in algebraic structures.

background	1 A document image is composed of a variety of physical entities or regions such as text blocks, lines, words, figures, tables, and background.
background	2 We could also assign functional or logical labels such as sentences, titles, captions, author names, and addresses to some of these regions.
method	3 The process of document structure and layout analysis tries to decompose a given document image into its component regions and understand their functional roles and relationships.
method	4 The processing is carried out in multiple steps, such as preprocessing, page decomposition, structure understanding, etc.
objective	5 We will look into each of these steps in detail in the following sections.
method	6 Document images are often generated from physical documents by digitization using scanners or digital cameras.
result	7 Many documents, such as newspapers, magazines and brochures, contain very complex layout due to the placement of figures, titles, and captions, complex backgrounds, artistic text formatting, etc.
other	8 (see Figure 1).
result	9 A human reader uses a variety of additional cues such as context, conventions and information about language/script, along with a complex reasoning process to decipher the contents of a document.
result	10 Automatic analysis of an arbitrary document with complex layout is an extremely difficult task and is beyond the capabilities of the state-of-the-art document structure and layout analysis systems.

background	1 Modern NAND flash memory chips provide high density by storing two bits of data in each flash cell, called a multi-level cell (MLC).
background	2 An MLC partitions the threshold voltage range of a flash cell into four voltage states.
background	3 When a flash cell is programmed, a high voltage is applied to the cell.
background	4 Due to parasitic capacitance coupling between flash cells that are physically close to each other, flash cell programming can lead to cell-to-cell program interference, which introduces errors into neighboring flash cells.
method	5 In order to reduce the impact of cell-to-cell interference on the reliability of MLC NAND flash memory, flash manufacturers adopt a two-step programming method, which programs the MLC in two separate steps.
method	6 First, the flash memory partially programs the least significant bit of the MLC to some intermediate threshold voltage.
result	7 Second, it programs the most significant bit to bring the MLC up to its full voltage state.
result	8 In this paper, we demonstrate that two-step programming exposes new reliability and security vulnerabilities.
method	9 We experimentally characterize the effects of two-step programming using contemporary 1X-nm (i.e., 15–19nm) flash memory chips.
result	10 We find that a partially-programmed flash cell (i.e., a cell where the second programming step has not yet been performed) is much more vulnerable to cell-to-cell interference and read disturb than a fully-programmed cell.

background	1 In many practical learning scenarios, there is a small amount of labeled data along with a large pool of unlabeled data.
background	2 Many supervised learning algorithms have been developed and extensively studied.
method	3 We present a new \co-training" strategy for using un-labeled data to improve the performance of standard supervised learning algorithms.
method	4 Unlike much of the prior work, such as the co-training procedure of Blum and Mitchell (1998), we do not assume there are two redundant views both of which are suucient for classiication.
method	5 The only requirement our co-training strategy places on each supervised learning algorithm is that its hypothesis partitions the example space into a set of equivalence classes (e.g. for a decision tree each leaf deenes an equivalence class).
result	6 We evaluate our co-training strategy via experiments using data from the UCI repository.

background	1 Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.
background	2 Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.
background	3 However, we found that this loss function may lead to the vanishing gradients problem during the learning process.
method	4 To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.
method	5 We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence.
background	6 There are two benefits of LSGANs over regular GANs.
background	7 First, LSGANs are able to generate higher quality images than regular GANs.
background	8 Second, LSGANs perform more stable during the learning process.
result	9 We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.
result	10 We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.

background	1 Our daily lives have been immersed in widespread location-based social networks (LBSNs).
background	2 As an open platform, LBSNs typically allow all kinds of users to register accounts.
background	3 Malicious attackers can easily join and post misleading information, often with the intention of influencing users' decisions in urban computing environments.
method	4 To provide reliable information and improve the experience for legitimate users, we design and implement DeepScan, a malicious account detection system for LBSNs.
method	5 Different from existing approaches, DeepScan leverages emerging deep learning technologies to learn users' dynamic behavior.
method	6 In particular, we introduce the long short-term memory (LSTM) neural network to conduct time series analysis of user activities.
method	7 DeepScan combines newly introduced time series features and a set of conventional features extracted from user activities, and exploits a supervised machine-learning-based model for detection.
result	8 Using real traces collected from Dianping, a representative LBSN, we demonstrate that DeepScan can achieve excellent prediction performance with an F1-score of 0.964.
result	9 We also find that the time series features play a critical role in the detection system.

background	1 Introduction: While increased time spent on social media (TSSM) has been associated with depression and anxiety, the independent role of using multiple social media (SM) platforms is unclear.
method	2 Methods: We surveyed a nationally-representative sample of 1787 U.S. young adults ages 19e32.
method	3 Depression and anxiety symptoms were measured using the Patient-Reported Outcomes Measurement Information System (PROMIS).
result	4 We assessed use of multiple SM platforms with an adapted Pew Internet Research scale.
method	5 We used ordered logistic regression models to assess associations between use of multiple SM platforms and mental health outcomes while controlling for eight covariates, including overall TSSM.
result	6 Results: Compared to those who used 0e2 social media platforms, participants who used 7e11 social media platforms had substantially higher odds of having increased levels of both depression (Adjusted Odds Ratio [AOR] 1⁄4 3.0, 95% CI 1⁄4 1.9e4.8) and anxiety symptoms (AOR 1⁄4 3.2, 95% CI 1⁄4 2.0e5.1).
result	7 Associations were linear (p < 0.001 for all) and robust to all sensitivity analyses.
result	8 Conclusions: Use of multiple SM platforms is independently associated with symptoms of depression and anxiety, even when controlling for overall TSSM.
result	9 These associations are strong enough that it may be valuable for clinicians to ask individuals with depression and anxiety about multiple platform use and to counsel regarding this potential contributing factor.
other	10 © 2016 Published by Elsevier Ltd.

background	1 Model interpretation is one of the key aspects of the model evaluation process.
background	2 The explanation of the relationship between model variables and outputs is easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance.
background	3 For “black box” models, such as random forest, this information is hidden inside the model structure.
background	4 This work presents an approach for computing feature contributions for random forest classification models.
objective	5 It allows for the determination of the influence of each variable on the model prediction for an individual instance.
method	6 Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology.
result	7 The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models.

background	1 The Internet of Things is one of the most promising technological developments in information technology.
background	2 It promises huge financial and nonfinancial benefits across supply chains, in product life cycle and customer relationship applications as well as in smart environments.
background	3 However, the adoption process of the Internet of Things has been slower than expected.
background	4 One of the main reasons for this is the missing profitability for each individual stakeholder.
background	5 Costs and benefits are not equally distributed.
method	6 Cost benefit sharing models have been proposed to overcome this problem and to enable new areas of application.
method	7 However, these cost benefit sharing approaches are complex, time consuming, and have failed to achieve broad usage.
method	8 In this chapter, an alternative concept, suggesting flexible pricing and trading of information, is proposed.
method	9 On the basis of a beverage supply chain scenario, a prototype installation, based on an open source billing solution and the Electronic Product Code Information Service (EPCIS), is shown as a proof of concept and an introduction to different pricing options.
method	10 This approach allows a more flexible and scalable solution for cost benefit sharing and may enable new business models for the Internet of Things.

background	1 The after-sales activities are nowadays acknowledged as a relevant source of revenue, profit and competitive advantage in most manufacturing industries.
method	2 Top and middle management, therefore, should focus on the definition of a structured business performance measurement system for the after-sales business.
objective	3 In addition, since many actors are involved along the after-sale service supply chain, an integrated and multi-attribute set of measures needs to be designed consistently at every level of the supply chain.
background	4 Nonetheless, little attention was devoted by scientific and managerial literature to this topic.
method	5 The paper aims at filling this gap, and proposes an integrated framework for the after-sales network performance measurement, and provides an empirical application to two automotive case companies and their official service network.
result	6 The cases show that performance measurement systems of different supply chain actors should be aligned in order to achieve strategic consistency.
result	7 In particular, the performance of different actors at the process level of the framework concurs in determining the after-sales service overall performance towards the final customer.
result	8 In addition, linkages at other levels (mainly the business and activity ones) may be needed or helpful in ensuring consistency between strategic and operational objectives, inside the organisations and thus for the whole supply chain.
other	9 # 2007 Elsevier B.V. All rights reserved.

objective	1 This paper proposes to use autoencoders with nonlinear dimensionality reduction in the anomaly detection task.
method	2 The authors apply dimensionality reduction by using an autoencoder onto both artificial data and real data, and compare it with linear PCA and kernel PCA to clarify its property.
method	3 The artificial data is generated from Lorenz system, and the real data is the spacecrafts' telemetry data.
method	4 This paper demonstrates that autoencoders are able to detect subtle anomalies which linear PCA fails.
method	5 Also, autoencoders can increase their accuracy by extending them to denoising autoenconders.
method	6 Moreover, autoencoders can be useful as nonlinear techniques without complex computation as kernel PCA requires.
result	7 Finaly, the authors examine the learned features in the hidden layer of autoencoders, and present that autoencoders learn the normal state properly and activate differently with anomalous input.

background	1 Reinforcement learning is an unsupervised machine learning method in the area of Artificial Intelligence.
background	2 It presents well performance in simulation of the thinking ability of human.
background	3 However, it needs a trial-and-error process to achieve the goal.
background	4 In the research field of game AI, it is a good approach to allow the non-player-characters (NPCs) of digital games to become more humanity.
objective	5 In this paper, we try to build a Tank-battle computer game and use the methodology of reinforcement learning for the NPCs (tanks).
objective	6 The goal of this paper is to make this game become more interesting from the enhanced interactions with these intelligent NPCs.

background	1 Ties often have a strength naturally associated with them that differentiate them from each other.
background	2 Tie strength has been operationalized as weights.
objective	3 A few network measures have been proposed for weighted networks, including three common measures of node centrality: degree, closeness, and betweenness.
objective	4 However, these generalizations have solely focused on tie weights, and not on the number of ties, which was the central component of the original measures.
objective	5 This paper proposes generalizations that combine both these aspects.
method	6 We illustrate the benefits of this approach by applying one of them to Freeman’s EIES dataset.

background	1 Online taxicab platforms like DiDi and Uber have impacted hundreds of millions of users on their choices of traveling, but how do users feel about the ride-sharing services, and how to improve their experience?
background	2 While current ride-sharing services have collected massive travel data, it remains challenging to develop data-driven techniques for modeling and predicting user ride experience.
objective	3 In this work, we aim to accurately predict passenger satisfaction over their rides and understand the key factors that lead to good/bad experiences.
method	4 Based on in-depth analysis of large-scale travel data from a popular taxicab platform in China, we develop PHINE (Pattern-aware Heterogeneous Information Network Embedding) for data-driven user experience modeling.
method	5 Our PHINE framework is novel in that it is composed of spatial-temporal node binding and grouping for addressing the inherent data variation, and pattern preservation based joint training for modeling the interactions among drivers, passengers, locations, and time.
result	6 Extensive experiments on 12 real-world travel datasets demonstrate the effectiveness of PHINE over strong baseline methods.
result	7 We have deployed PHINE in the DiDi Big Data Center, delivering high-quality predictions for passenger satisfaction on a daily basis.

background	1 3D sensors such as LIDARs, stereo cameras, time-of-flight cameras, and the Microsoft Kinect are increasingly found in a wide range of applications, including gaming, personal robotics, and space exploration.
background	2 In some cases, pattern recognition algorithms for processing depth images can be tested using actual sensors observing real-world objects.
background	3 In many situations, however, it is common to test new algorithms using computer-generated synthetic images, as such simulations tend to be faster, more flexible, and less expensive than hardware tests.
background	4 Computer generation of images is especially useful for Monte Carlo-type analyses or for situations where obtaining real sensor data for preliminary testing is difficult (e.g., space applications).
method	5 We present GLIDAR, an OpenGL and GL Shading Language-based sensor simulator, capable of imaging nearly any static three-dimensional model.
method	6 GLIDAR allows basic object manipulations, or may be connected to a physics simulator for more advanced behaviors.
method	7 It permits publishing to a TCP socket at high frame-rates or can save to PCD (point cloud data) files.
result	8 The software is written in C++, and is released under the open source BSD license.

background	1 This paper attempts to examine the effects of virtual team dimensions on social identities of its members.
method	2 A review of the literature shows that the geographically dispersed, culturally diverse as well as temporary dimensions of virtual teams do not match with their stability as members have different ethnic, social, or cultural backgrounds.
background	3 Sources like culture, place, and time seem to continuously acquire social identities.
method	4 Due to the importance of social identity, an attempt has been made to examine its influence on organizational variables (i.e. job satisfaction, job involvement, job commitment, and organizational citizenship behavior).
method	5 Questionnaire-based data have been accomplished from 149 members of 44 teams.
result	6 The hypothesized relationships among the proposed variables are tested via a structural equation model (SEM).
result	7 Results show that the geographically disperse and culturally diverse variables are negatively related to the social identity as against those of temporary and organizational variables which are related positively.

background	1 The leapfrog method is popular because of its good stability when solving partial differential equations with oscillatory solutions.
other	2 ‘‘
method	3 It has the disadvantage that the solution at odd time steps tends to drift farther and farther from the solution for even time steps, so it is common to stop the integration every twenty time steps or so and reinitialize with the first order forward Euler method . .
other	4 .
other	5 ”.
method	6 We prove that restarting in this way results in a method that is not stable.
method	7 We further show that if the step size is not too big, perturbations grow so slowly that the computations are stable enough for practical purposes.
result	8 The leapfrog method is not dissipative, but we show that restarting results in a method with a useful amount of dissipation.
result	9 We also show that Gragg’s smoothing scheme improves the stability of the method.
other	10 2008 Elsevier Inc. All rights reserved.

background	1 Automatic player detection, labeling and tracking in broadcast soccer video are significant while quite challenging tasks.
objective	2 In this paper, we present a solution to perform automatic multiple player detection, unsupervised labeling and efficient tracking.
background	3 Players’ position and scale are determined by a boosting based detector.
method	4 Players’ appearance models are unsupervised learned from hundreds of samples automatically collected by detection.
method	5 Thereafter, these models can be utilized for player labeling (Team A, Team B and Referee).
method	6 Player tracking is achieved by Markov Chain Monte Carlo (MCMC) data association.
method	7 Some data driven dynamics are proposed to improve the Markov chain’s efficiency.
result	8 The testing results on FIFA World Cup 2006 video demonstrate that our method can reach high detection and labeling precision, and reliably tracking in cases of scenes such as multiple player occlusion, moderate camera motion and pose variation.

background	1 Principal component analysis (PCA) is a popular tool for linear dimensionality reduction and feature extraction.
background	2 Kernel PCA is the nonlinear form of PCA, which better exploits the complicated spatial structure of high-dimensional features.
objective	3 In this paper, we first review the basic ideas of PCA and kernel PCA.
method	4 Then we focus on the reconstruction of pre-images for kernel PCA.
method	5 We also give an introduction on how PCA is used in active shape models (ASMs), and discuss how kernel PCA can be applied to improve traditional ASMs.
method	6 Then we show some experimental results to compare the performance of kernel PCA and standard PCA for classification problems.
method	7 We also implement the kernel PCA-based ASMs, and use it to construct human face models.

background	1 While favouring communications and easing information sharing, Social Network Sites are also used to launch harmful campaigns against specific groups and individuals.
background	2 Cyberbullism, incitement to self-harm practices, sexual predation are just some of the severe effects of massive online offensives.
background	3 Moreover, attacks can be carried out against groups of victims and can degenerate in physical violence.
objective	4 In this work, we aim at containing and preventing the alarming diffusion of such hate campaigns.
objective	5 Using Facebook as a benchmark, we consider the textual content of comments appeared on a set of public Italian pages.
method	6 We first propose a variety of hate categories to distinguish the kind of hate.
method	7 Crawled comments are then annotated by up to five distinct human annotators, according to the defined taxonomy.
method	8 Leveraging morpho-syntactical features, sentiment polarity and word embedding lexicons, we design and implement two classifiers for the Italian language, based on different learning algorithms: the first based on Support Vector Machines (SVM) and the second on a particular Recurrent Neural Network named Long Short Term Memory (LSTM).
method	9 We test these two learning algorithms in order to verify their classification performances on the task of hate speech recognition.
result	10 The results show the effectiveness of the two classification approaches tested over the first manually annotated Italian Hate Speech Corpus of social media text.

background	1 Marketing experts consider the mobile device as an extremely promising marketing tool as it supports them to cope with their major challenge: getting time and attention from customers.
background	2 Current mobile marketing research mostly covers success factors and acceptance analysis.
background	3 Categorization, when addressed, lacks in appropriate foundation and is not linked to objectives at all.
objective	4 In this article we examine 55 case studies in order to identify relevant characteristics of mobile marketing campaigns.
result	5 The outcome of the paper is the derivation of four mobile marketing standard types and an examination of campaign objectives that can be addressed by mobile marketing.
result	6 The proposed scheme allows to unambiguously characterize any given mobile marketing campaign and to identify the respective objectives.

background	1 In recent years two main platforms emerged as powerful key players in the domain of parallel computing: GPUs and FPGAs.
background	2 Many researches investigate interaction and benefits of coupling them with a general purpose processor (CPU), but very few, and only very recently, integrate the two in the same computational system.
background	3 Even less research are focusing on direct interaction of the two platforms [1].
objective	4 This paper presents an open source framework enabling easy integration of GPU and FPGA resources; Our work provides direct data transfer between the two platforms with minimal CPU coordination at high data rate and low latency.
objective	5 Finally, at the best of our knowledge, this is the first proposition of an open source implementation of a system including an FPGA and a GPU that provides code for both sides.
method	6 Notwithstanding the generality of the presented framework, we present in this paper an actual implementation consisting of a single GPU board and a FPGA board connected through a PCIe link.
result	7 Measures on this implementation demonstrate achieved data rate that are close to the theoretical maximum.

background	1 Instagram is a popular social networking application, which allows photo-sharing and applying different photo filters to adjust the appearance of a picture.
background	2 By applying photo filters, users are able to create a style that they want to express to their audience.
objective	3 In this study we tried to infer personality traits from the way users take pictures and apply filters to them.
method	4 To investigate this relationship, we conducted an online survey where we asked participants to fill in a personality questionnaire, and grant us access to their Instagram account through the Instagram API.
result	5 Among 113 participants and 22,398 extracted Instagram pictures, we found distinct picture features (e.g., hue, brightness, saturation) that are related to personality traits.
result	6 Our findings suggest a relationship between personality traits and the way users want to make their pictures look.
result	7 This allow for new ways to extract personality traits from social media trails, and new ways to facilitate personalized systems.

background	1 In this work we investigate building indoor location based applications for a mobile augmented reality system.
background	2 We believe that augmented reality is a natural interface to visualize spacial information such as position or direction of locations and objects for location based applications that process and present information based on the user’s position in the real world.
method	3 To enable such applications we construct an indoor tracking system that covers a substantial part of a building.
objective	4 It is based on visual tracking of fiducial markers enhanced with an inertial sensor for fast rotational updates.
objective	5 To scale such a system to a whole building we introduce a space partitioning scheme to reuse fiducial markers throughout the environment.
result	6 Finally we demonstrate two location based applications built upon this facility, an indoor navigation aid and a library search applica-

objective	1 Through an empirical study among holiday travellers, residing in the Former Soviet Union Republics, this paper presents a comprehensive view of role and impact of social media on the whole holiday travel planning process: Before, during and after the trip, providing insights on usage levels, scope of use, level of influence and trust.
result	2 Findings suggest that social media are predominantly used after holidays for experience sharing.
method	3 It is also shown that there is a strong correlation between perceived level of influence from social media and changes made in holiday plans prior to final decisions.
result	4 Moreover, it is revealed that user-generated content is perceived as more trustworthy when compared to official tourism websites, travel agents and mass media advertising.

background	1 This paper provides new results for the tracking control of a quadrotor unmanned aerial vehicle (UAV).
background	2 The UAV has four input degrees of freedom, namely the magnitudes of the four rotor thrusts, that are used to control the six translational and rotational degrees of freedom, and to achieve asymptotic tracking of four outputs, namely, three position variables for the vehicle center of mass and the direction of one vehicle body-fixed axis.
background	3 A globally defined model of the quadrotor UAV rigid body dynamics is introduced as a basis for the analysis.
method	4 A nonlinear tracking controller is developed on the special Euclidean group SE(3) and it is shown to have desirable closed loop properties that are almost global.
result	5 Several numerical examples, including an example in which the quadrotor recovers from being initially upside down, illustrate the versatility of the controller.

background	1 We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity.
objective	2 Our goal is to recover the common set of relevant inputs for each output cluster.
background	3 Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso.
method	4 Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure.
method	5 We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap.
method	6 We present an efficient optimization method that can handle a largescale problem.
result	7 Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.

background	1 While software is so important for all facets of the modern world, software development itself is not a perfect process.
method	2 Agile software engineering methods have recently emerged as a new and different way of developing software as compared to the traditional methodologies.
result	3 However, their success has mostly been anecdotal, and research in this subject is still scant in the academic circles.
method	4 This research study was a survey study on the critical success factors of Agile software development projects using quantitative approach.
method	5 Based on existing literature, a preliminary list of potential critical success factors of Agile projects were identified and compiled.
objective	6 Subsequently, reliability analysis and factor analysis were conducted to consolidate this preliminary list into a final set of 12 possible critical success factors for each of the four project success categories – Quality, Scope, Time, and Cost.
method	7 A survey was conducted among Agile professionals, gathering survey data from 109 Agile projects from 25 countries across the world.
result	8 Multiple regression techniques were used, both at the full regression model and at the optimized regression model via the stepwise screening procedure.
result	9 The results revealed that only 10 out of 48 hypotheses were supported, identifying three critical success factors for Agile software development projects: (a) Delivery Strategy, (b) Agile Software Engineering Techniques, and (c) Team Capability.
result	10 Limitations of the study are discussed together with interpretations for practitioners.

background	1 In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans.
objective	2 We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input.
method	3 We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).

background	1 Autonomous vehicles capable of navigating unpredictable real-world environments with little human feedback are a reality today.
background	2 Such systems rely heavily on onboard sensors such as cameras, radar/LIDAR, and GPS as well as capabilities such as 3G/4G connectivity and V2V/V2I communication to make real-time maneuvering decisions.
background	3 Autonomous vehicle control imposes very strict requirements on the security of the communication channels used by the vehicle to exchange information as well as the control logic that performs complex driving tasks such as adapting vehicle velocity or changing lanes.
objective	4 This study presents a first look at the effects of security attacks on the communication channel as well as sensor tampering of a connected vehicle stream equipped to achieve CACC.
result	5 Our simulation results show that an insider attack can cause significant instability in the CACC vehicle stream.
result	6 We also illustrate how different countermeasures, such as downgrading to ACC mode, could potentially be used to improve the security and safety of the connected vehicle streams.

background	1 This paper presents a wall-climbing robot which adopts passive suction cups as the attaching components.
background	2 Using only one motor, this robot can not only move on a wall but also attach suction cups to the wall and remove them from the wall.
background	3 Passive suction cups do not consume additional energy to keep adhesion.
objective	4 Therefore, the proposed robot can realize the climbing motion on a wall with relatively low energy consumption.
method	5 The prototype has been designed, fabricated and tested.
result	6 The experiments showed that the proposed robot could attach and remove suction cups passively.
result	7 However, the robot could not move up the wall well and fell down often.
result	8 In order to solve this problem, the load of each suction cup when attached to a vertical wall is analyzed.
result	9 As a result, it is shown that a moment generated by both of the gravity and the attaching force of suction cups turns the robot down from the wall.
result	10 Then a new model which improves the falling problems is thus designed.

background	1 There is an ongoing debate over the activities of brands and companies in social media.
background	2 Some researchers believe social media provide a unique opportunity for brands to foster their relationships with customers, while others believe the contrary.
objective	3 Taking the perspective of the brand community building plus the brand trust and loyalty literatures, our goal is to show how brand communities based on social media influence elements of the customer centric model (i.e., the relationships among focal customer and brand, product, company, and other customers) and brand loyalty.
method	4 A survey-based empirical study with 441 respondents was conducted.
result	5 The results of structural equation modeling show that brand communities established on social media have positive effects on customer/product, customer/brand, customer/company and customer/other customers relationships, which in turn have positive effects on brand trust, and trust has positive effects on brand loyalty.
method	6 We find that brand trust has a fully mediating role in converting the effects of enhanced relationships in brand community to brand loyalty.
result	7 The implications for marketing practice and future research are discussed.
other	8 © 2012 Elsevier Ltd.
other	9 All rights reserved.

background	1 Imagine that you get such certain awesome experience and knowledge by only reading a book.
other	2 How can?
objective	3 It seems to be greater when a book can be the best thing to discover.
result	4 Books now will appear in printed and soft file collection.
method	5 One of them is this book business process management concepts languages architectures.
other	6 It is so usual with the printed books.
result	7 However, many people sometimes have no space to bring the book for them; this is why they can't read the book wherever they want.

background	1 Process discovery—discovering a process model from example behavior recorded in an event log—is one of the most challenging tasks in process mining.
background	2 The primary reason is that conventional modeling languages (e.g., Petri nets, BPMN, EPCs, and ULM ADs) have difficulties representing the observed behavior properly and/or succinctly.
background	3 Moreover, discovered process models tend to have deadlocks and livelocks.
objective	4 Therefore, we advocate a new representation more suitable for process discovery: causal nets.
method	5 Causal nets are related to the representations used by several process discovery techniques (e.g., heuristic mining, fuzzy mining, and genetic mining).
method	6 However, unlike existing approaches, we provide declarative semantics more suitable for process mining.
method	7 To clarify these semantics and to illustrate the non-local nature of this new representation, we relate causal nets to Petri nets.

objective	1 The main objective of this research was to identify factors that affected worker productivity, occupational health and safety in selected industries in a developing country.
background	2 Fifty production managers participated in the study.
background	3 Fifty-four percent of the managers reported hot environmental conditions, 28% a noisy environment, and 26% a lack of resources and facilities.
method	4 Managers received worker complaints of fatigue, back pain, upper-body pain, hand and wrist pain and headaches.
result	5 Management (88%) acknowledged not having knowledge or access to ergonomics information.
result	6 Ninety-four percent of the companies did not carry out ergonomic assessments.
result	7 A significant correlation ðp , 0:01Þ was found among productivity indicators and health and organizational attributes.
result	8 Lack of skills in ergonomics and training, communication and resources are believed to be some of the factors contributing to the poor ergonomic conditions and consequent loss of worker productivity and reduced health and safety in these industries.
other	9 q 2003 Elsevier Ltd. All rights reserved.

background	1 Resting state functional connectivity reveals intrinsic, spontaneous networks that elucidate the functional architecture of the human brain.
background	2 However, valid statistical analysis used to identify such networks must address sources of noise in order to avoid possible confounds such as spurious correlations based on non-neuronal sources.
method	3 We have developed a functional connectivity toolbox Conn ( www.nitrc.org/projects/conn ) that implements the component-based noise correction method (CompCor) strategy for physiological and other noise source reduction, additional removal of movement, and temporal covariates, temporal filtering and windowing of the residual blood oxygen level-dependent (BOLD) contrast signal, first-level estimation of multiple standard functional connectivity magnetic resonance imaging (fcMRI) measures, and second-level random-effect analysis for resting state as well as task-related data.
method	4 Compared to methods that rely on global signal regression, the CompCor noise reduction method allows for interpretation of anticorrelations as there is no regression of the global signal.
method	5 The toolbox implements fcMRI measures, such as estimation of seed-to-voxel and region of interest (ROI)-to-ROI functional correlations, as well as semipartial correlation and bivariate/multivariate regression analysis for multiple ROI sources, graph theoretical analysis, and novel voxel-to-voxel analysis of functional connectivity.
method	6 We describe the methods implemented in the Conn toolbox for the analysis of fcMRI data, together with examples of use and interscan reliability estimates of all the implemented fcMRI measures.
result	7 The results indicate that the CompCor method increases the sensitivity and selectivity of fcMRI analysis, and show a high degree of interscan reliability for many fcMRI measures.

background	1 Despite their massivesize, successful deep artificial neural networkscan exhibit a remarkably small differencebetween training and test performance.
background	2 Conventional wisdom attributessmall generalization error either to propertiesof themodel family, or to the regularization techniquesused during training.
objective	3 Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice.
method	4 Specifically, our experimentsestablish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data.
method	5 This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise.
result	6 We corroborate these experimental findings with a theoretical construction showing that simpledepth two neural networksalready haveperfect finitesampleexpressivity assoon as thenumber of parameters exceeds thenumber of datapointsas it usually does in practice.
result	7 We interpret our experimental findingsby comparison with traditional models.

background	1 Due to the increasing threat from malicious software (malware), monitoring of vulnerable systems is becoming increasingly important.
background	2 The need to log and analyze activity encompasses networks, individual computers, as well as mobile devices.
background	3 While there are various automatic approaches and techniques available to detect, identify, or capture malware, the actual analysis of the ever-increasing number of suspicious samples is a time-consuming process for malware analysts.
method	4 The use of visualization and highly interactive visual analytics systems can help to support this analysis process with respect to investigation, comparison, and summarization of malware samples.
background	5 Currently, there is no survey available that reviews available visualization systems supporting this important and emerging field.
objective	6 We provide a systematic overview and categorization of malware visualization systems from the perspective of visual analytics.
result	7 Additionally, we identify and evaluate data providers and commercial tools that produce meaningful input data for the reviewed malware visualization systems.
result	8 This helps to reveal data types that are currently underrepresented, enabling new research opportunities in the visualization community.

background	1 Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision.
background	2 Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry.
objective	3 We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data.
method	4 Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised.
objective	5 By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.

background	1 We have analyzed the properties of the HSV (Hue, Saturation and Value) color space with emphasis on the visual perception of the variation in Hue, Saturation and Intensity values of an image pixel.
method	2 We extract pixel features by either choosing the Hue or the Intensity as the dominant property based on the Saturation value of a pixel.
method	3 The feature extraction method has been applied for both image segmentation as well as histogram generation applications – two distinct approaches to content based image retrieval (CBIR).
method	4 Segmentation using this method shows better identification of objects in an image.
method	5 The histogram retains a uniform color transition that enables us to do a window-based smoothing during retrieval.
result	6 The results have been compared with those generated using the RGB color space.

background	1 We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes.
objective	2 The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent.
result	3 Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.

background	1 Social media technologies collapse multiple audiences into single contexts, making it difficult for people to use the same techniques online that they do to handle multiplicity in face-to-face conversation.
background	2 This article investigates how content producers navigate ‘imagined audiences’ on Twitter.
method	3 We talked with participants who have different types of followings to understand their techniques, including targeting different audiences, concealing subjects, and maintaining authenticity.
background	4 Some techniques of audience management resemble the practices of ‘micro-celebrity’ and personal branding, both strategic self-commodification.
result	5 Our model of the networked audience assumes a manyto-many communication through which individuals conceptualize an imagined audience evoked through their tweets.

objective	1 The accurate modeling of micro-grid access to power system planning and design stage needs is the primary problem to solve.
method	2 This paper modeled the micro grid photovoltaic power generation system ,including silicon solar cell, photovoltaic inverters, battery energy storage system, and the micro power distribution system .The use of power system analysis software (DIGSILENT) of actual power system simulation, the simulation results verify the model's correctness.
result	3 In the power grid fault disturbance, the light intensity of disturbance and the load disturbances, the simulation results show that the optical storage combined with micro network has fast dynamic response characteristics, and its network of grid-connected voltage influenced by the changes of the light and load is little, while more affected by the network fault influence.

background	1 Accurate state estimation for a mobile robot often requires the fusion of data from multiple sensors.
background	2 Software that performs sensor fusion should therefore support the inclusion of a wide array of heterogeneous sensors.
method	3 This paper presents a software package, robot_localization, for the Robot Operating System (ROS).
method	4 The package currently contains an implementation of an extended Kalman filter (EKF).
result	5 It can support an unlimited number of inputs from multiple sensor types, and allows users to customize which sensor data fields are fused with the current state estimate.
result	6 In this work, we motivate our design decisions, discuss implementation details, and provide results from real-world tests.
other	7 Keywords—sensor fusion; extended Kalman filter; localization; Robot Operating System

background	1 The ICP algorithm is accurate and fast for registration between two point sets in a same scale, but it doesn't handle the case with different scales.
objective	2 This paper instead introduces a novel approach named the scaling iterative closest point (SICP) algorithm which integrates a scale matrix with boundaries into the original ICP algorithm for scaling registration.
method	3 This method uses a simple iterative algorithm with the SVD algorithm and the properties of parabola incorporated to compute the translation, rotation and scale transformations at each iterative step, and its convergence is rapid with only a few iterations.
method	4 The SICP algorithm is independent of shape representation and feature extraction; thereby it is general for scaling registration.
result	5 Experimental results demonstrate its robustness and fast speed compared with the standard ICP algorithm.

background	1 Fractional calculus has recently attracted much attention in the literature.
background	2 In particular, fractional derivatives are widely discussed and applied in many areas.
background	3 However, it is still hard to develop numerical methods for fractional calculus.
objective	4 In this paper, based on Fourier series and Taylor series technique, we provide some numerical methods for computing and simulating fractional derivatives by using Matlab.
method	5 Some numerical examples are also presented.

background	1 Most of current computer-based facial expression analysis methods focus on the recognition of perfectly posed expressions, and hence are incapable of handling the individuals with expression impairments.
background	2 In particular, patients with schizophrenia usually have impaired expressions in the form of "flat" or "inappropriate" affects, which make the quantification of their facial expressions a challenging problem.
method	3 This paper presents methods to quantify the group differences between patients with schizophrenia and healthy controls, by extracting specialized features and analyzing group differences on a feature manifold.
method	4 The features include 2D and 3D geometric features, and the moment invariants combining both 3D geometry and 2D textures.
result	5 Facial expression recognition experiments on actors demonstrate that our combined features can better characterize facial expressions than either 2D geometric or texture features.
result	6 The features are then embedded into an ISOMAP manifold to quantify the group differences between controls and patients.
result	7 Experiments show that our results are strongly supported by the human rating results and clinical findings, thus providing a framework that is able to quantify the abnormality in patients with schizophrenia.

background	1 Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data.
background	2 In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units.
background	3 Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks.
background	4 However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited.
method	5 This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems.
result	6 An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.

background	1 Top-performing deep architectures are trained on massive amounts of labeled data.
background	2 In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available.
background	3 Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).
method	4 As the training progresses, the approach promotes the emergence of “deep” features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains.
result	5 We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer.
method	6 The resulting augmented architecture can be trained using standard backpropagation.
result	7 Overall, the approach can be implemented with little effort using any of the deep-learning packages.
result	8 The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-ofthe-art on Office datasets.

background	1 Distributed Processing Systems are the ones that include multiple devices (which could be of many types, such as PC computers, mobile devices etc.)
background	2 that have computational and communication capabilities.
background	3 Their computational power is jointly used for collaborative processing of variety of tasks – and this processing is realized in distributed manner.
background	4 UAV Unmanned Aerial Vehicles (also called drones) gain significant attention over recent years.
method	5 They have been employed to realize multiple tasks such as surveillance or environmental monitoring.
result	6 First implementations were based on single UAV, later the potential of multiple UAVs collaborating in a team was noticed.
result	7 Many applications were implemented in distributed manner, using multiple collaborative UAVs and the distributed processing systems principles.
method	8 In this paper we survey the applications implemented over cooperative teams of UAVs that operate as distributed processing systems.

background	1 Real-world case studies are important to complement the academic skills and knowledge acquired by computer science students.
background	2 In this paper we relate our experiences with a course specifically designed to address this issue.
background	3 The problem to be addressed is the replacement of a Hospital Information System in a large regional hospital.
method	4 The case mimics as close as possible the project as it really took place.
objective	5 The objectives of the course are threefold: to train management and communication skills, to integrate and apply knowledge gained at different previous courses, and to learn by experience the difference between a real-world problem and a textbook problem.
result	6 Students’ evaluations show that the objectives of the course are met and that it is regarded as very useful.
result	7 We found that the three objectives mutually reinforce each other, which is a decisive factor for the success of the course.

background	1 The Internet of Things (IoT) represents a modern approach where boundaries between real and digital domains are progressively eliminated by changing over consistently every physical device to smart object ready to provide valuable services.
background	2 These services provide a vital role in different life domains but at the same time create new challenges particularly in security and privacy.
background	3 Authentication and access control models are considered as the essential elements to address these security and privacy challenges.
background	4 Risk-based access control model is one of the dynamic access control models that provides more flexibility in accessing system resources.
method	5 This model performs a risk analysis to estimate the security risk associated with each access request and uses the estimated risk to make the access decision.
method	6 One of the essential elements in this model is the risk estimation process.
method	7 Estimating risk is a complex operation that requires the consideration of a variety of factors in the access control environment.
result	8 Moreover, the interpretation and estimation of the risk might vary depending on the working domain.
other	9 This paper presents a review of different risk estimation techniques.
other	10 Existing risk-based access control models are discussed and compared in terms of the risk estimation technique, risk factors, and the evaluation domain.

background	1 Decision Tree Classifiers (DTC's) are used successfully in many diverse areas such as radar signal classification, character recognition, remote sensing, medical diagnosis, expert systems, and speech recognition, to name only a few.
background	2 Perhaps, the most important feature of DTC's is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution which is often easier to interpret.
objective	3 This paper presents a survey of current methods for DTC designs and the various existing issues.
result	4 After considering potential advantages of DTC's over single stage classifiers, subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed_ Finally, several remarks are made concerning possible future research directions.

objective	1 This paper introduces in details a genetic algorithm-called BASIC, which is designed to take advantage of well known genetic schemes so as to be able to deal with numerous optimization problems.
method	2 BASIC GA follows all common steps of the genetic algorithms.
method	3 It involves real representation schemes for both real and integer variables.
method	4 Three biased selection schemes for reproduction; four for recombination and three for mutation are applied in it and a new selection scheme for replacement is approached.
method	5 BASIC GA can be easy adjusted to the concrete problems by fitting its global and local parameters.
result	6 It provides an opportunity to the genetic operators to be extended with new schemes.
result	7 A range of various optimization problems has been solved to test its capability.
method	8 To handle all sorts of constraints the static and dynamic penalty f ©

objective	1 This paper describes a visual odometry algorithm for estimating frame-to-frame camera motion from successive stereo image pairs.
method	2 The algorithm differs from most visual odometry algorithms in two key respects: (1) it makes no prior assumptions about camera motion, and (2) it operates on dense disparity images computed by a separate stereo algorithm.
method	3 This algorithm has been tested on many platforms, including wheeled and legged vehicles, and has proven to be fast, accurate and robust.
result	4 For example, after 4000 frames and 400 m of travel, position errors are typically less than 1 m (0.25% of distance traveled).
result	5 Processing time is approximately 20 ms on a 512times384 image.
result	6 This paper includes a detailed description of the algorithm and experimental evaluation on a variety of platforms and terrain types.

background	1 The phenomenon of digital transformation received some attention in previous literature concerning industries such as media, entertainment and publishing.
background	2 However, there is a lack of understanding about digital transformation of primarily physical industries, whose products cannot be completely digitized, e.g., automotive industry.
method	3 We conducted a rigorous content analysis of substantial secondary data from industry magazines aiming to generate insights to this phenomenon in the automotive industry.
background	4 We examined the impact of major digital trends on dominant business models.
method	5 Our findings indicate that trends related to social media, mobile, big data and cloud computing are driving automobile manufactures to extend, revise, terminate, and create business models.
method	6 By doing so, they contribute to the constitution of a digital layer upon the physical mobility infrastructure.
method	7 Despite its strong foundation in the physical world, the industry is undergoing important structural changes due to the ongoing digitalization of consumer lives and business.

background	1 Computer laboratory is a facility where students access to the hardware and software necessary to fulfil the requirements in the course.
background	2 Computer laboratories are also used to train and expose students on computer programming, simulation and other subjects.
objective	3 The design and layout of the laboratory is an important concern to ensure that the facility provides maximum benefits to students.
objective	4 This paper examine the layout design of Industrial Application Computer Laboratory, in Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia.
objective	5 The objective of this paper is to propose new layout designs for the facility and identify the best layout that suits the purpose of the laboratory.
result	6 The results show that some modification is required to improve the current layout and fulfil a better environment for teaching and learning process in the facility.

background	1 Smartphones have exploded in popularity in recent years, becoming ever more sophisticated and capable.
background	2 As a result, developers worldwide are building increasingly complex applications that require ever increasing amounts of computational power and energy.
objective	3 In this paper we propose ThinkAir, a framework that makes it simple for developers to migrate their smartphone applications to the cloud.
objective	4 ThinkAir exploits the concept of smartphone virtualization in the cloud and provides method-level computation offloading.
objective	5 Advancing on previous work, it focuses on the elasticity and scalability of the cloud and enhances the power of mobile cloud computing by parallelizing method execution using multiple virtual machine (VM) images.
method	6 We implement ThinkAir and evaluate it with a range of benchmarks starting from simple micro-benchmarks to more complex applications.
method	7 First, we show that the execution time and energy consumption decrease two orders of magnitude for a N-queens puzzle application and one order of magnitude for a face detection and a virus scan application.
method	8 We then show that a parallelizable application can invoke multiple VMs to execute in the cloud in a seamless and on-demand manner such as to achieve greater reduction on execution time and energy consumption.
method	9 We finally use a memory-hungry image combiner tool to demonstrate that applications can dynamically request VMs with more computational power in order to meet their computational requirements.

background	1 Human being is the most intelligent animal in this world.
background	2 Intuitively, optimization algorithm inspired by human being creative problem solving process should be superior to the optimization algorithms inspired by collective behavior of insects like ants, bee, etc.
objective	3 In this paper, we introduce a novel brain storm optimization algorithm, which was inspired by the human brainstorming process.
method	4 Two benchmark functions were tested to validate the effectiveness and usefulness of the proposed algorithm.

objective	1 The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune.
objective	2 We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors.
objective	3 Our motivation is the same as it has always been - detection datasets contain an overwhelming number of easy examples and a small number of hard examples.
objective	4 Automatic selection of these hard examples can make training more effective and efficient.
result	5 OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use.
result	6 But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012.
result	7 Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset.
result	8 Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.

background	1 We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis.
background	2 This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology.
method	3 We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.

background	1 This paper describes a simple, efficient algorithm to locate all occurrences of any of a finite number of keywords in a string of text.
objective	2 The algorithm consists of constructing a finite state pattern matching machine from the keywords and then using the pattern matching machine to process the text string in a single pass.
method	3 Construction of the pattern matching machine takes time proportional to the sum of the lengths of the keywords.
background	4 The number of state transitions made by the pattern matching machine in processing the text string is independent of the number of keywords.
result	5 The algorithm has been used to improve the speed of a library bibliographic search program by a factor of 5 to 10.

objective	1 This paper discusses the need to consider possible risks to ensure business survival and business continuity before the implementation of new technologies, with specific interest to wireless networks and wireless devices.
background	2 Information Technology is fast becoming essential within business processes and is considered a technical issue yet neglected at board level.
background	3 In recent years, there has been an elevated awareness in Information Security and Corporate Governance.
background	4 Organisations, and their board of directors, have become increasingly aware that securing information is vital for the organisation in both financial terms and corporate identity.
background	5 Wireless Networks, and the use of mobile devices, are bringing the world a new means of communication and day-to-day business activities.
background	6 The implementation of these new Wireless devices also brings about new security threats to Information assets.
result	7 This paper will determine the risks involved with new technologies and motivate the importance of understanding these risks before its implementation by emphasising the important role Corporate and IT Governance play.

background	1 Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images.
objective	2 To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently.
objective	3 The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree.
method	4 One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions.
result	5 However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency.
method	6 To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles.
result	7 A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle.
result	8 Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions.
result	9 This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing.
result	10 We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

background	1 Underwater range scanning techniques are starting to gain interest in underwater exploration, providing new tools to represent the seafloor.
background	2 These scans (often) acquired by underwater robots usually result in an unstructured point cloud, but given the common downward-looking or forward-looking configuration of these sensors with respect to the scene, the problem of recovering a piecewise linear approximation representing the scene is normally solved by approximating these 3D points using a heightmap (2.5D).
background	3 Nevertheless, this representation is not able to correctly represent complex structures, especially those presenting arbitrary concavities normally exhibited in underwater objects.
method	4 We present a method devoted to full 3D surface reconstruction that does not assume any specific sensor configuration.
method	5 The method presented is robust to common defects in raw scanned data such as outliers and noise often present in extreme environments such as underwater, both for sonar and optical surveys.
method	6 Moreover, the proposed method does not need a manual preprocessing step.
method	7 It is also generic as it does not need any information other than the points themselves to work.
result	8 This property leads to its wide application to any kind of range scanning technologies and we demonstrate its versatility by using it on synthetic data, controlled laser scans, and multibeam sonar surveys.
method	9 Finally, and given the unbeatable level of detail that optical methods can provide, we analyze the application of this method on optical datasets related to biology, geology

background	1 Invasive species are one of the main drivers of biodiversity loss.
background	2 In the past decade, the development of environmental spectroscopy, both field spectrometers and airborne imaging spectrometers, has allowed progress in identifying individual species from remote sensing data.
objective	3 However, use of environmental spectroscopy for species identification needs understanding at a more fundamental level, especially the development of generalized methodologies and rules for detection and mapping, which is an area of active research today.
result	4 These issues will be explored using examples from a wide range of habitats and site conditions, towards the development of a robust methodology to identify native and non-native species.

background	1 Enforcing security in Internet of Things environments has been identified as one of the top barriers for realizing the vision of smart, energy-efficient homes and buildings.
objective	2 In this context, understanding the risks related to the use and potential misuse of information about homes, partners, and end-users, as well as, forming methods for integrating security-enhancing measures in the design is not straightforward and thus requires substantial investigation.
method	3 A risk analysis applied on a smart home automation system developed in a research project involving leading industrial actors has been conducted.
method	4 Out of 32 examined risks, 9 were classified as low and 4 as high, i.e., most of the identified risks were deemed as moderate.
method	5 The risks classified as high were either related to the human factor or to the software components of the system.
result	6 The results indicate that with the implementation of standard security features, new, as well as, current risks can be minimized to acceptable levels albeit that the most serious risks, i.e., those derived from the human factor, need more careful consideration, as they are inherently complex to handle.
result	7 A discussion of the implications of the risk analysis results points to the need for a more general model of security and privacy included in the design phase of smart homes.
result	8 With such a model of security and privacy in design in place, it will contribute to enforcing system security and enhancing user privacy in smart homes, and thus helping to further realize the potential in such IoT environments.

background	1 The performance of pattern classifiers depends on the separability of the classes in the feature space - a property related to the quality of the descriptors - and the choice of informative training samples for user labeling - a procedure that usually requires active learning.
objective	2 This work is devoted to improve the quality of the descriptors when samples are superpixels from remote sensing images.
method	3 We introduce a new scheme for superpixel description based on Bag of visual Words, which includes information from adjacent superpixels, and validate it by using two remote sensing images and several region descriptors as baselines.

background	1 Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models.
background	2 In the past, GPUs enabled these breakthroughs because of their greater computational speed.
background	3 In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices.
result	4 As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL).
objective	5 Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks.
result	6 We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated.
result	7 Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.

background	1 The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning.
background	2 Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity.
objective	3 This motivates us to investigate their effectiveness on face recognition.
objective	4 This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition.
method	5 These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition.
method	6 Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training.
result	7 An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively.
result	8 A further discussion of LFW face verification result is given in the end.

background	1 We describe a system for transforming an off-the-shelf flatbed scanner into a $200 scan backend for large format cameras.
background	2 While we describe both software and hardware aspects, the focus of the paper is on software issues such as color calibration and removal of scanner artifacts.
method	3 With current scanner technology, the resulting camera system is capable of taking black&white, color, or near-infrared photographs with up to 490 million pixels.
result	4 Our analysis shows that we achieve actual optical resolutions close to the theoretical maximum, and that color reproduction is comparable to commercial camera systems.
result	5 We believe that the camera system described here has many potential applications in image-based modeling and rendering, cultural heritage projects, and professional digital photography.

background	1 We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene.
background	2 Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to structure from motion approaches.
background	3 The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework.
method	4 Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation.
method	5 Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware.
result	6 This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas.
result	7 We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera

background	1 Ease of use and usefulness are believed to be fundamental in determining the acceptance and use of various, corporate ITs.
background	2 These beliefs, however, may not explain the user's behavior toward newly emerging ITs, such as the World-Wide-Web (WWW).
objective	3 In this study, we introduce playfulness as a new factor that re ̄ects the user's intrinsic belief in WWW acceptance.
method	4 Using it as an intrinsic motivation factor, we extend and empirically validate the Technology Acceptance Model (TAM) for the WWW context.
other	5 # 2001 Elsevier Science B.V. All rights reserved.

background	1 Call Centers are important channels of communication within the consumer relationship and a point of integration between suppliers and their customers.
background	2 Correctly sizing the capacity of a given Call Center can bring benefits not only in terms of improved customer service (efficacy), but also in terms of reduced operating costs (efficiency).
method	3 However, specifying the capacity of a Call Center is not a trivial task, but one that demands a significant knowledge of mathematics, in particular of analytical models.
result	4 This paper presents the Erlang B, Erlang C and Simulation models followed by a comparison based on a case study, in order to identify the advantages of using simulation.

background	1 We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem.
result	2 SCITAIL is the first entailment set that is created solely from natural sentences that already exist independently “in the wild” rather than sentences authored specifically for the entailment task.
method	3 Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus.
result	4 These sentences are often linguistically challenging.
method	5 This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult.
result	6 The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on SCITAIL, especially in comparison to a simple majority class baseline.
result	7 As a step forward, we demonstrate that one can improve accuracy on SCITAIL by 5% using a new neural model that exploits linguistic structure.

background	1 Chinese noun classifiers are an indispensible part of the Chinese language, but are difficult for non-native speakers to use correctly.
background	2 Chinese language teachers often face challenges in finding an effective way to teach classifiers, as the rules for defining which nouns can be associated with which classifiers are not straightforward.
background	3 Many theoretical studies have explored the nature of Chinese classifiers, but few studies take an empirical approach to the investigation of effective teaching and learning methods of classifiers.
background	4 Learners often find that existing dictionaries either do not have classifiers as lexical entries, or give very brief explanations that are hardly helpful.
background	5 This paper presents the progress of an ongoing project on the construction of an e-dictionary of Chinese classifiers.
objective	6 The objective of the project is to provide a platform for Chinese language learners to explore and learn classifier uses in a bottom-up fashion.
method	7 The current work is on the design of an e-learning tool database and its connection to the e-dictionary database.
method	8 Descriptions of the design and the functions of the e-learning tool are provided in the paper.

background	1 We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs).
background	2 We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only.
background	3 In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input.
method	4 Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture.
result	5 The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.

background	1 This paper considers the task of matching images and sentences.
background	2 The challenge consists in discriminatively embedding the two modalities onto a shared visual-textual space.
background	3 Existing work in this field largely uses Recurrent Neural Networks (RNN) for text feature learning and employs off-the-shelf Convolutional Neural Networks (CNN) for image feature extraction.
background	4 Our system, in comparison, differs in two key aspects.
method	5 Firstly, we build a convolutional network amenable for fine-tuning the visual and textual representations, where the entire network only contains four components, i.e., convolution layer, pooling layer, rectified linear unit function (ReLU), and batch normalisation.
method	6 Endto-end learning allows the system to directly learn from the data and fully utilise the supervisions.
method	7 Secondly, we propose instance loss according to viewing each multimodal data pair as a class.
method	8 This works with a large margin objective to learn the inter-modal correspondence between images and their textual descriptions.
result	9 Experiments on two generic retrieval datasets (Flickr30k and MSCOCO) demonstrate that our method yields competitive accuracy compared to state-of-the-art methods.
result	10 Moreover, in language person retrieval, we improve the state of the art by a large margin.

background	1 ROC analysis is increasingly being recognised as an important tool for evaluation and comparison of classifiers when the operating characteristics (i.e. class distribution and cost parameters) are not known at training time.
background	2 Usually, each classifier is characterised by its estimated true and false positive rates and is represented by a single point in the ROC diagram.
objective	3 In this paper, we show how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves.
objective	4 In this setting, rather than estimating the accuracy of a single tree, it makes more sense to use the area under the ROC curve (AUC) as a quality metric.
objective	5 We also propose a novel splitting criterion which chooses the split with the highest local AUC.
background	6 To the best of our knowledge, this is the first probabilistic splitting criterion that is not based on weighted average impurity.
result	7 We present experiments suggesting that the AUC splitting criterion leads to trees with equal or better AUC value, without sacrificing accuracy if a single labelling is chosen.

background	1 As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied.
objective	2 We propose a memory-centric deep learning system that can transparently expand the memory capacity accessible to the accelerators while also providing fast inter-device communication for parallel training.
objective	3 Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion.
result	4 Compared to conventional systems, our proposal achieves an average <inline-formula> <tex-math notation="LaTeX">$2.1\times$</tex-math><alternatives><inline-graphic xlink:href="rhu-ieq1-2823302.gif"/> </alternatives></inline-formula> speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.

background	1 In today's applications data is produced at unprecedented rates.
background	2 While the capacity to collect and store new data grows rapidly, the ability to analyze these data volumes increases at much lower pace.
background	3 This gap leads to new challenges in the analysis process, since analysts, decision makers, engineers, or emergency response teams depend on information "concealed" in the data.
background	4 The emerging field of visual analytics focuses on handling massive, heterogenous, and dynamic volumes of information through integration of human judgement by means of visual representations and interaction techniques in the analysis process.
background	5 Furthermore, it is the combination of related research areas including visualization, data mining, and statistics that turns visual analytics into a promising field of research.
objective	6 This paper aims at providing an overview of visual analytics, its scope and concepts, and details the most important technical research challenges in the field

objective	1 The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations.
objective	2 These results depend crucially on the choice of effective termweighting systems.
objective	3 This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.
other	4 1.
background	5 AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users’ information queries.
result	6 Typically, certain words extracted from the texts of documents and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections.
method	7 In either case, the documents would be represented by term vectors of the form D= (ti,tj,...
method	8 ytp) (1) where each tk identifies a content term assigned to some sample document D. Analogously, the information requests, or queries, would be represented either in vector form, or in the form of Boolean statements.
other	9 Thus, a typical query Q might be formulated as Q = (qa,qbr.. .
other	10 ,4r) (2)

background	1 Particle swarm optimization is a heuristic global optimization method and also an optimization algorithm, which is based on swarm intelligence.
background	2 It comes from the research on the bird and fish flock movement behavior.
background	3 The algorithm is widely used and rapidly developed for its easy implementation and few particles required to be tuned.
method	4 The main idea of the principle of PSO is presented; the advantages and the shortcomings are summarized.
objective	5 At last this paper presents some kinds of improved versions of PSO and research situation, and the future research issues are also given.

background	1 Object detection – the computer vision task dealing with detecting instances of objects of a certain class (e.g ., ’car’, ’plane’, etc.)
background	2 in images – attracted a lot of attention from the community during the last 5 years.
background	3 This strong interest can be explained not only by the importance this task has for many applications but also by the phenomenal advances in this area since the arrival of deep convolutional neural networks (DCNN).
objective	4 This article reviews the recent literature on object detection with deep CNN, in a comprehensive way, and provides an in-depth view of these recent advances.
result	5 The survey covers not only the typical architectures (SSD, YOLO, Faster-RCNN) but also discusses the challenges currently met by the community and goes on to show how the problem of object detection can be extended.
objective	6 This survey also reviews the public datasets and associated state-of-the-art algorithms.

background	1 The famous Towers of Hanoi puzzle consists of 3 pegs (A, B, C) on one of which (A) are stacked n rings of different sizes, each ring resting on a larger ring.
objective	2 The objective is to move the n rings one by one until they are all stacked on another peg (B) in such a way that no ring is ever placed on a smaller ring; the other peg (C) can be used as workspace.
method	3 The problem has tong been a favourite iir programming courses as one which admits a concise recursive solution.
method	4 This solution hinges on the observation that, when the largest ring is moved from A to B, the n 1 remaining rings must all be on peg C. This immediately leads to the recursive procedure

background	1 A growing number of information technology systems and services are being developed to change users’ attitudes or behavior or both.
background	2 Despite the fact that attitudinal theories from social psychology have been quite extensively applied to the study of user intentions and behavior, these theories have been developed for predicting user acceptance of the information technology rather than for providing systematic analysis and design methods for developing persuasive software solutions.
objective	3 This article is conceptual and theory-creating by its nature, suggesting a framework for Persuasive Systems Design (PSD).
method	4 It discusses the process of designing and evaluating persuasive systems and describes what kind of content and software functionality may be found in the final product.
method	5 It also highlights seven underlying postulates behind persuasive systems and ways to analyze the persuasion context (the intent, the event, and the strategy).
method	6 The article further lists 28 design principles for persuasive system content and functionality, describing example software requirements and implementations.
other	7 Some of the design principles are novel.
result	8 Moreover, a new categorization of these principles is proposed, consisting of the primary task, dialogue, system credibility, and social support categories.

method	1 We introduce a new model for replication in distributed systems.
background	2 The primary motivation for replication lies in fault tolerance.
method	3 Although there are different kinds of replication approaches, our model combines the advantages of modular redundancy and primary-stand-by approaches to give more flexibility with respect to system configuration.
method	4 To implement such a model, we select the IBM PC-net with MS-DOS environment as our base.
background	5 Transparency as well as fault-tolerance file access are the highlights of our system design.
method	6 To fulfil these requirements, we incorporate the idea of directory-oriented replication and extended prefix tab/es in the system design.
method	7 The implementation consists of a command shell, a DOS manager, and a recovery manager.
result	8 Through this design, we can simulate a UNIX-like distributed file system whose function is compatible with MS-DOS.

objective	1 Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines.
background	2 GPs have received growing attention in the machine learning community over the past decade.
objective	3 The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning.
objective	4 The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.
method	5 The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms.
method	6 A wide variety of covariance (kernel) functions are presented and their properties discussed.
method	7 Model selection is discussed both from a Bayesian and classical perspective.
method	8 Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others.
method	9 Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed.
method	10 The book contains illustrative examples and exercises.

background	1 This paper presents an extended Kalman filter for real-time estimation of rigid body orientation using the newly developed MARG (Magnetic, Angular Rate, and Gravity) sensors.
background	2 Each MARG sensor contains a three-axis magnetometer, a three-axis angular rate sensor, and a three-axis accelerometer.
objective	3 The filter represents rotations using quaternions rather than Euler angles, which eliminates the long-standing problem of singularities associated with attitude estimation.
objective	4 A process model for rigid body angular motions and angular rate measurements is defined.
method	5 The process model converts angular rates into quaternion rates, which are integrated to obtain quaternions.
method	6 The Gauss-Newton iteration algorithm is utilized to find the best quaternion that relates the measured accelerations and earth magnetic field in the body coordinate frame to calculated values in the earth coordinate frame.
background	7 The best quaternion is used as part of the measurements for the Kalman filter.
result	8 As a result of this approach, the measurement equations of the Kalman filter become linear, and the computational requirements are significantly reduced, making it possible to estimate orientation in real time.
result	9 Extensive testing of the filter with synthetic data and actual sensor data proved it to be satisfactory.
result	10 Test cases included the presence of large initial errors as well as high noise levels.

background	1 IoT (Internet of Things) has a large portion of our life.
background	2 This is manifested by the large number of connected devices.
background	3 With the exponential growth of IoT devices, IoT security is becoming important.
background	4 In particular, Smart Door Lock system is extremely important because it is closely related to the safety of the user.
background	5 However, the data sent and received of existing Smart Door Lock system is vulnerable to forgery and hacking.
objective	6 To improve these security issues, we propose a Smart Door Lock system based on blockchain.
result	7 Also, this provides data integrity and non-repudiation.
method	8 Lastly, we propose an algorithm that the Smart Door Lock system judge some situations around itself and operates based on data sent from sensors.

background	1 In this paper, we show that the LVQ learning algorithm converges to locally asymptotic stable equilibria of an ordinary differential equation.
background	2 We show that the learning algorithm performs stochastic approximation.
background	3 Convergence of the Voronoi vectors is guaranteed under the appropriate conditions on the underlying statistics of the classification problem.
result	4 We also present a modification to the learning algorithm which we argue results in convergence of the LVQ for a larger set of initial conditions.
result	5 Finally, we show that LVQ is a general histogram classifier and that its risk converges to the Bayesian optimal risk as the appropriate parameters go to infinity with the number of past observations.

objective	1 This paper summarizes the current state of the art and recent trends in software engineering economics.
objective	2 It provides an overview of economic analysis techniques and their applicability to software engineering and management.
method	3 It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.

background	1 We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes.
method	2 Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data.
method	3 The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper.
result	4 We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.

background	1 The e-government field, like most young fields, lacks a strong body of well-developed theory.
background	2 One strategy for coping with theoretical immaturity is to import and adapt theories from other, more mature fields.
objective	3 This study reviews Stakeholder Theory (ST) and investigates its potential in relation to e-Government.
method	4 Originally a management theory, stakeholder theory advocates addressing the concerns of all stakeholders in a firm, as opposed to concentration on the interests of senior managers and stockholders.
method	5 Apart from its original profit focus, there is no serious conceptual mismatch between stakeholder theory and government’s objective of providing policy and services for citizens and organizations – society’s stakeholders.
result	6 Potential problems with adapting a management theory to a government setting are discussed.
result	7 The paper further discusses how information technology impacts a stakeholder model of governance.
result	8 Finally, the paper makes recommendations for future work in adapting ST to the e-government context.

background	1 High level data quality and the management of ensuring data quality is one of the key success factors for Data Warehousing projects.
background	2 The following article describes an approach for Data Quality Management, which is based on theories as well as practical experiences.
objective	3 Starting from effects of insufficient data quality in practice, a definition for information, data and data quality will be worked out.
objective	4 Based on the concept of total data quality management the Data Quality Management (DQM) for Data-Warehouse-System will be described.
objective	5 As key part DQM an approach for operative DQM (planing and measuring data quality) will be illustrated and explained.
result	6 Finally, based on the research results further conclusions are summarised.

background	1 This paper gives an overview of shape dissimilarity measure properties, such as metric and robustness properties, and of retrieval performance measures.
method	2 Fifteen shape similarity measures are shortly described and compared.
method	3 Their retrieval results on the MPEG-7 Core Experiment CE-Shape-1 test set as reported in the literature and obtained by a reimplementation are compared and discussed.

background	1 Inventory systems with uncertainty go hand in hand with the determination of a safety stock level.
background	2 The decision on the safety stock level is based on a performance measure, for example the expected shortage per replenishment period or the probability of a stock-out per replenishment period.
method	3 The performance measure assumes complete knowledge of the probability distribution during lead time, which might not be available.
objective	4 In case of incomplete information regarding the lead-time distribution of demand, no single figure for the safety stock can de determined in order to satisfy a performance measure.
objective	5 However, an optimisation model may be formulated in order to determine a safety stock level which guarantees the performance measure under the worst case of lead-time demand, of which the distribution is known in an incomplete way.
result	6 It is shown that this optimisation problem can be formulated as a linear programming problem.
other	7 2011 Elsevier Ltd. All rights reserved.

background	1 User-generated reviews on the Web contain sentiments about detailed aspects of products and services.
background	2 However, most of the reviews are plain text and thus require much effort to obtain information about relevant details.
method	3 In this paper, we tackle the problem of automatically discovering what aspects are evaluated in reviews and how sentiments for different aspects are expressed.
method	4 We first propose Sentence-LDA (SLDA), a probabilistic generative model that assumes all words in a single sentence are generated from one aspect.
method	5 We then extend SLDA to Aspect and Sentiment Unification Model (ASUM), which incorporates aspect and sentiment together to model sentiments toward different aspects.
method	6 ASUM discovers pairs of {aspect, sentiment} which we call senti-aspects.
method	7 We applied SLDA and ASUM to reviews of electronic devices and restaurants.
result	8 The results show that the aspects discovered by SLDA match evaluative details of the reviews, and the senti-aspects found by ASUM capture important aspects that are closely coupled with a sentiment.
result	9 The results of sentiment classification show that ASUM outperforms other generative models and comes close to supervised classification methods.
result	10 One important advantage of ASUM is that it does not require any sentiment labels of the reviews, which are often expensive to obtain.

background	1 The paper presents the results of a robust Model Predictive Control (MPC) development for the diesel engine air path.
objective	2 The objective is to regulate the intake manifold pressure (MAP) and exhaust gas recirculation (EGR) rate to the specified set-points by coordinated control of the variable geometry turbine (VGT), EGR valve, and EGR throttle.
method	3 The approach uses tube-MPC to robustly enforce state constraints in the presence of set-bounded engine speed and fueling disturbances.
method	4 Furthermore, a rate-based formulation along with various approximations is applied to reduce the conservativeness and computational complexity of tube-MPC.
result	5 The ability of the controller to handle input and output constraints is demonstrated in both nonlinear model simulations and experimental results.

background	1 Countless learning tasks require dealing with sequential data.
background	2 Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences.
background	3 In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences.
background	4 Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities.
background	5 Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes.
background	6 Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window.
objective	7 Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them.
background	8 In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition.
background	9 In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models.
other	10 When appropriate, we reconcile conflicting notation and nomenclature.

background	1 The rapid adoption of heterogeneous computing has driven the integration of Field Programmable Gate Arrays (FPGAs) into cloud datacenters and flexible System-on-Chips (SoCs).
objective	2 This paper shows that the integrated FPGA introduces a new security vulnerability by enabling software-based power side-channel attacks without physical proximity to a target system.
method	3 We first demonstrate that an on-chip power monitor can be built on a modern FPGA using ring oscillators (ROs), and characterize its ability to observe the power consumption of other modules on the FPGA or the SoC. Then, we show that the RO-based FPGA power monitor can be used for a successful power analysis attack on an RSA cryptomodule on the same FPGA.
result	4 Additionally, we show that the FPGA-based power monitor can observe the power consumption of a CPU on the same SoC, and demonstrate that the FPGA-to-CPU power side-channel attack can break timing-channel protection for a RSA program running on a CPU.
result	5 This work introduces and demonstrates remote power side-channel attacks using an FPGA, showing that the common assumption that power side-channel attacks require specialized equipment and physical access to the victim hardware is not true for systems with an integrated FPGA.

background	1 In recent decades, the ad hoc network for vehicles has been a core network technology to provide comfort and security to drivers in vehicle environments.
background	2 However, emerging applications and services require major changes in underlying network models and computing that require new road network planning.
background	3 Meanwhile, blockchain widely known as one of the disruptive technologies has emerged in recent years, is experiencing rapid development and has the potential to revolutionize intelligent transport systems.
method	4 Blockchain can be used to build an intelligent, secure, distributed and autonomous transport system.
background	5 It allows better utilization of the infrastructure and resources of intelligent transport systems, particularly effective for crowdsourcing technology.
objective	6 In this paper, we proposes a vehicle network architecture based on blockchain in the smart city (Block-VN).
objective	7 Block-VN is a reliable and secure architecture that operates in a distributed way to build the new distributed transport management system.
objective	8 We are considering a new network system of vehicles, Block-VN, above them.
method	9 In addition, we examine how the network of vehicles evolves with paradigms focused on networking and vehicular information.
method	10 Finally, we discuss service scenarios and design principles for Block-VN.

background	1 Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing.
background	2 More recently, neural network models started to be applied also to textual natural language signals, again with very promising results.
method	3 This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques.
objective	4 The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.

background	1 Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction.
background	2 K-means clustering is a commonly used data clustering for performing unsupervised learning tasks.
method	3 Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering.
method	4 New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix.
method	5 These results indicate that unsupervised dimension reduction is closely related to unsupervised learning.
result	6 Several implications are discussed.
result	7 On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noise-reduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data.
result	8 On learning, the result suggests effective techniques for K-means data clustering.
result	9 DNA gene expression and Internet newsgroups are analyzed to illustrate our results.
result	10 Experiments indicate that the new bounds are within 0.5-1.5% of the optimal values.

objective	1 A recommender system aims to recommend items that a user is interested in among many items.
background	2 The need for the recommender system has been expanded by the information explosion.
method	3 Various approaches have been suggested for providing meaningful recommendations to users.
method	4 One of the proposed approaches is to consider a recommender system as aMarkov decision process (MDP) problem and try to solve it using reinforcement learning (RL).
method	5 However, existing RL-based methods have an obvious drawback.
result	6 To solve an MDP in a recommender system, they encountered a problem with the large number of discrete actions that bring RL to a larger class of problems.
method	7 In this paper, we propose a novel RL-based recommender system.
method	8 We formulate a recommender system as a gridworld game by using a biclustering technique that can reduce the state and action space significantly.
method	9 Using biclustering not only reduces space but also improves the recommendation quality effectively handling the cold-start problem.
result	10 In addition, our approach can provide users with some explanation why the system recommends certain items.

background	1 In this chapter methods of handling missing attribute values in data mining are described.
background	2 These methods are categorized into sequential and parallel.
method	3 In sequential methods, missing attribute values are replaced by known values first, as a preprocessing, then the knowledge is acquired for a data set with all known attribute values.
method	4 In parallel methods, there is no preprocessing, i.e., knowledge is acquired directly from the original data sets.
method	5 In this chapter the main emphasis is put on rule induction.
method	6 Methods of handling attribute values for decision tree generation are only briefly summarized.

background	1 We study the problem of object recognition for categories for which we have no training examples, a task also called zero--data or zero-shot learning.
background	2 This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them.
objective	3 To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape.
method	4 Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task.
method	5 Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase.
result	6 In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes.
result	7 Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes.

background	1 We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos.
background	2 Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions.
background	3 To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality.
method	4 We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows.
method	5 We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations.
method	6 We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D.
method	7 Our framework is generic to any input modality, i.e., RGB, depth, and RGB-D videos.

background	1 We consider the problem of dynamic spectrum access for network utility maximization in multichannel wireless networks.
background	2 The shared bandwidth is divided into K orthogonal channels, and the users access the spectrum using a random access protocol.
background	3 In the beginning of each time slot, each user selects a channel and transmits a packet with a certain attempt probability.
background	4 After each time slot, each user that has transmitted a packet receives a local observation indicating whether its packet was successfully delivered or not (i.e., ACK signal).
objective	5 The objective is to find a multi-user strategy that maximizes a certain network utility in a distributed manner without online coordination or message exchanges between users.
objective	6 Obtaining an optimal solution for the spectrum access problem is computationally expensive in general due to the large state space and partial observability of the states.
method	7 To tackle this problem, we develop a distributed dynamic spectrum access algorithm based on deep multi-user reinforcement leaning.
method	8 Specifically, at each time slot, each user maps its current state to spectrum access actions based on a trained deep-Q network used to maximize the objective function.
result	9 Experimental results have demonstrated that users are capable to learn good policies that achieve strong performance in this challenging partially observable setting only from their ACK signals, without online coordination, message exchanges between users, or carrier sensing.

background	1 We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location.
method	2 We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space.
method	3 In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution.
background	4 This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside.
method	5 This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours.
method	6 We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces.
result	7 We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.

background	1 The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis.
background	2 We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones.
background	3 Generative approaches have thus far been either inflexible, inefficient or non-scalable.
method	4 We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.

background	1 'No-shows' or missed appointments result in under-utilized clinic capacity.
objective	2 We develop a logistic regression model using electronic medical records to estimate patients' no-show probabilities and illustrate the use of the estimates in creating clinic schedules that maximize clinic capacity utilization while maintaining small patient waiting times and clinic overtime costs.
method	3 This study used information on scheduled outpatient appointments collected over a three-year period at a Veterans Affairs medical center.
method	4 The call-in process for 400 clinic days was simulated and for each day two schedules were created: the traditional method that assigned one patient per appointment slot, and the proposed method that scheduled patients according to their no-show probability to balance patient waiting, overtime and revenue.
method	5 Combining patient no-show models with advanced scheduling methods would allow more patients to be seen a day while improving clinic efficiency.
result	6 Clinics should consider the benefits of implementing scheduling software that includes these methods relative to the cost of no-shows.

background	1 We present a simple, highly modularized network architecture for image classification.
background	2 Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology.
objective	3 Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set.
method	4 This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.
method	5 On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy.
method	6 Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity.
result	7 Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place.
result	8 We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.
result	9 The code and models are publicly available online.

background	1 The proliferation of cyber-physical systems introduces the fourth stage of industrialization, commonly known as Industry 4.0.
background	2 The vertical integration of various components inside a factory to implement a flexible and reconfigurable manufacturing system, i.e., smart factory, is one of the key features of Industry 4.0.
background	3 In this paper, we present a smart factory framework that incorporates industrial network, cloud, and supervisory control terminals with smart shop-floor objects such as machines, conveyers, and products.
method	4 Then, we provide a classification of the smart objects into various types of agents and define a coordinator in the cloud.
method	5 The autonomous decision and distributed cooperation between agents lead to high flexibility.
method	6 Moreover, this kind of self-organized system leverages the feedback and coordination by the central coordinator in order to achieve high efficiency.
method	7 Thus, the smart factory is characterized by a self-organized multi-agent system assisted with big data based feedback and coordination.
method	8 Based on this model, we propose an intelligent negotiation mechanism for agents to cooperate with each other.
method	9 Furthermore, the study illustrates that complementary strategies can be designed to prevent deadlocks by improving the agents’ decision making and the coordinator’s behavior.
result	10 The simulation results assess the effectiveness of the proposed negotiation mechanism and deadlock prevention strategies.

background	1 State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.
background	2 In this paper, we introduce two new neural architectures—one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers.
method	3 Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora.
objective	4 Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.
other	5 1

background	1 An understanding of quality attributes is relevant for the software organization to deliver high software reliability.
background	2 An empirical assessment of metrics to predict the quality attributes is essential in order to gain insight about the quality of software in the early phases of software development and to ensure corrective actions.
objective	3 In this paper, we predict a model to estimate fault proneness using Object Oriented CK metrics and QMOOD metrics.
method	4 We apply one statistical method and six machine learning methods to predict the models.
method	5 The proposed models are validated using dataset collected from Open Source software.
method	6 The results are analyzed using Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis.
result	7 The results show that the model predicted using the random forest and bagging methods outperformed all the other models.
result	8 Hence, based on these results it is reasonable to claim that quality models have a significant relevance with Object Oriented metrics and that machine learning methods have a comparable performance with statistical methods Keywords—Empirical Validation, Object Oriented, Receiver Operating Characteristics, Statistical Methods, Machine Learning, Fault Prediction

objective	1 The goal of this work is to develop a soft robotic manipulation system that is capable of autonomous, dynamic, and safe interactions with humans and its environment.
background	2 First, we develop a dynamic model for a multi-body fluidic elastomer manipulator that is composed entirely from soft rubber and subject to the self-loading effects of gravity.
method	3 Then, we present a strategy for independently identifying all unknown components of the system: the soft manipulator, its distributed fluidic elastomer actuators, as well as drive cylinders that supply fluid energy.
method	4 Next, using this model and trajectory optimization techniques we find locally optimal open-loop policies that allow the system to perform dynamic maneuvers we call grabs.
result	5 In 37 experimental trials with a physical prototype, we successfully perform a grab 92% of the time.
result	6 By studying such an extreme example of a soft robot, we can begin to solve hard problems inhibiting the mainstream use of soft machines.

background	1 Localization is one important part of Internet of Things(IoT) where the Location of Everything (LoE) system plays a important role to improve most services in IoT area.
background	2 On the other hand, data mining techniques are essential analyses when we have big data from IoT platforms.
background	3 Indeed, integration of location-based methods and data mining analysis process can make a smart system service for IoT scenarios and applications.
method	4 For this purpose, we design a smart shopping platform including four components, location of everything component, data collection component, data filtering/analysing component and data mining component.
method	5 Then a novel accurate localization scheme named “location orbital” is developed that estimates the current location of mobile objects (users or everything) based on both current and the previous locations.
method	6 Finally, an implementation of the experiment in a shopping mall is conducted to practically examine performance evaluation of the location-based scheme.
result	7 The experimental results show that the proposed scheme could achieve significant higher precision than other localization techniques.

background	1 Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image.
method	2 This paper describes some easily computable textural features based on graytone spatial dependancies, and illustrates their application in categoryidentification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories.
objective	3 We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule).
objective	4 In each experiment the data set was divided into two parts, a training set and a test set.
background	5 Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery.
result	6 These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.

background	1 How many moves does it take to solve Rubik’s Cube?
method	2 Positions are known that require 20 moves, and it has already been shown that there are no positions that require 27 or more moves; this is a surprisingly large gap.
method	3 This paper describes a program that is able to find solutions of length 20 or less at a rate of more than 16 million positions a second.
objective	4 We use this program, along with some new ideas and incremental improvements in other techniques, to show that there is no position that requires 26 moves.

background	1 The movement to real-time is the latest development in business intelligence (BI) and data warehousing.
background	2 Real-time data warehousing provides the data that is required to implement realtime BI.
background	3 By moving to real-time, firms can use BI to affect current decision making and business processes.
background	4 This capability is especially important for customer-facing applications, such as those found in call centers and check-in processes, and helps firms become more customer-centric.
method	5 Terms such as the “real-time enterprise” and the “zero latency organization” are often used to describe firms that use real-time BI.

objective	1 This paper describes an efficient system for entering data into a pen-enabled computer, particularly handheld devices.
objective	2 While keyboard input typically is faster than handwriting input, this is not true for the small PDA interfaces.
method	3 For this reason, we designed and developed a prototype that uses chatroom abbreviations and shorthand symbols to increase the speed of data entry for these devices.
method	4 This system was also developed as a prototype system that would enable persons with speech impairments to rapidly convert hand-drawn symbols on a pen-enabled device into speech output.
result	5 We created a library of chatroom abbreviations and shorthand symbols, and developed a k-nn classification system to recognize the symbols.
result	6 Experimental results show the effectiveness of the system in terms of speed and accuracy.

background	1 We explore deep reinforcement learning methods for multi-agent domains.
method	2 We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows.
method	3 We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination.
method	4 Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies.
method	5 We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.

background	1 Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor.
method	2 In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving.
objective	3 We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving.
method	4 Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously.
method	5 Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction.
method	6 To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments.
result	7 We also train a model for car distance estimation on the KITTI dataset.
result	8 Results show that our direct perception approach can generalize well to real driving images.
result	9 Source code and data are available on our project website.

background	1 The design of a low-frequency high-input-impedance amplifier having probably the lowest noise ever reported is presented.
background	2 The amplifier's frequency range is from about 0.07 Hz to about 110 kHz at the -3-dB level.
background	3 The equivalent input noise voltage spectral density is about 5.6, 1.4, 0.6, and 0.5 nV/radicHz at frequencies 0.1, 1, 10, and 1000 Hz, respectively.
method	4 Gain of the amplifier is about 83 dB. Noise analysis is made for active-type, capacitive-type, and low impedance signal sources.
result	5 The contribution from different noise sources in the amplifier and JFET to the overall noise is shown.

background	1 In this paper we derive a linear-time, constant-space algorithm to construct a binary heap whose inorder traversal equals a given sequence.
background	2 We do so in two steps.
method	3 First, we invert a program that computes the inorder traversal of a binary heap, using the proof rules for program inversion by W. Chen and J.T. Udding.
method	4 This results in a linear-time solution in terms of binary trees.
result	5 Subsequently, we data-refine this program to a constant-space solution in terms of linked structures.

background	1 As business competition becomes global, international information systems (IIS) management presents a signi®cant challenge to multinational corporations and their af®liates.
background	2 However, very few empirical studies have been conducted to investigate the management of IIS and the issues that confront the IS executives of such corporations.
objective	3 In this study, we perform a three-round Delphi study to identify, rank and evaluate the twenty most signi®cant IIS issues of af®liates.
method	4 The ratings of these issues suggest that technology infrastructure concerns, rather than planning and management concerns, have a larger impact on the IS operations of foreign af®liates.
method	5 This study also reports on statistical analyses to differentiate the impact of different industries, respondents, IS structures and international involvement of af®liates on IIS issues ratings.
result	6 The results indicate that respondents of IS and non-IS executives and af®liates of different international involvement levels have different views on the ratings of IIS issues.
result	7 Our study also con®rms that IIS issues can signi®cantly impact the strategic, tactical and operational IS decisions of af®liates.
result	8 These ®ndings allow some important implications to be drawn for both practitioners and researchers dealing with IIS issues.
other	9 # 2001 Elsevier Science B.V. All rights reserved.

background	1 IT architecture is often assumed to follow business strategy, to align IT with the business’s strategic objectives.
background	2 Increasingly, though, many business strategies depend on specific underlying IT capabilities.
background	3 To develop a synergy between business strategy and IT architecture, firms must develop organizational competencies in IT architecture.
objective	4 My research has identified four IT architectural stages, each with its own requisite competencies.
method	5 The “application silo architecture stage” consists of IT architectures of individual applications.
method	6 The “standardized technology architecture stage” has an enterprise-wide IT architecture that provides efficiencies through technology standardization.
result	7 The “rationalized data architecture stage” extends the enterprise-wide IT standards to data and processes.
result	8 And the “modular architecture stage” builds onto enterprise-wide global standards with loosely coupled IT components to preserve the global standards while enabling local differences.
result	9 Each stage demands different organizational competencies to implement the architecture and prepare the firm to move to the next stage.

background	1 With the advances in the development of mobile payments, a huge amount of payment data are collected by banks.
background	2 User payment data offer a good dataset to depict customer behavior patterns.
background	3 A comprehensive understanding of customers' purchase behavior is crucial to developing good marketing strategies, which may trigger much greater purchase amounts.
method	4 For example, by exploring customer behavior patterns, given a target store, a set of potential customers is able to be identified.
background	5 In other words, personalized campaigns at the right time and in the right place can be treated as the last stage of consumption.
objective	6 Here we propose a probability graphical model that exploits the payment data to discover customer purchase behavior in the spatial, temporal, payment amount and product category aspects, named STPC-PGM.
result	7 As a result, the mobility behavior of an individual user could be predicted with a probabilistic graphical model that accounts for all aspects of each customer's relationship with the payment platform.
method	8 To achieve real time advertising, we then develop an online framework that efficiently computes the prediction results.
result	9 Our experiment results show that STPC-PGM is effective in discovering customers' profiling features, and outperforms the state-of-the-art methods in purchase behavior prediction.
result	10 In addition, the prediction results are being deployed in the marketing of real-world credit card users, and have presented a significant growth in the advertising conversion rate.

background	1 Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training.
background	2 The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source.
background	3 By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources.
background	4 KBANN(Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques.
method	5 It maps problem-specific “domain theories”, represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation.
method	6 KBANN is evaluated by extensive empirical tests on two problems from molecular biology.
result	7 Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists.

background	1 Gamification evolved to one of the most important trends in technology and therefore gains more and more practical and scientific notice.
background	2 Yet academia lacks a comprehensive overview of research, even though a review of prior, relevant literature is essential for advancing knowledge in a field.
objective	3 Therefore a novel classification framework for Gamification in Information Systems with the intention to provide a structured, summarized as well as organized overview was constructed to close this gap of research.
method	4 A literature review on Gamification in quality outlets combined with a Grounded Theory approach served as a starting point.
method	5 As a result this paper provides a foundation for current and future research to advance the knowledge on Gamification.
result	6 Moreover it offers a structure for Gamification research which was not available previously.
result	7 Findings from the literature review were mapped to the classification framework and analyzed.
result	8 Derived from the classification framework and its outcome future research outlets were identified.

objective	1 We present a method that learns word embedding for Twitter sentiment classification in this paper.
background	2 Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.
background	3 This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.
method	4 We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.
method	5 Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions.
method	6 To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.
result	7 Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.

background	1 Mobile technology is a continuously growing domain and research activities regarding its use are quite intensive.
background	2 A questionnaire regarding the use of mobile devices was developed and distributed to 416 students in a Greek University.
background	3 There were completed 384 questionnaires.
background	4 The results revealed that students use their mobiles mostly for phone calls and SMS (short message service).
background	5 They also tend to use their mobiles to take photos and activate the reminder.
result	6 However, they do not deal with many of the devices’ operations.
method	7 They use their mobiles to communicate (telephone, SMS, email) mostly with their boy/girlfriend, then with their friends.
method	8 They use their mobiles mostly at home, then at the University.
background	9 Also, they consider health issues as the main reason to limit the use of their mobiles.
result	10 Finally, there was not a statistically significant relationship between genders and their preferences.

background	1 We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning.
objective	2 The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples.
method	3 In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task.
method	4 In effect, our method trains the model to be easy to fine-tune.
result	5 We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.

background	1 There is an increased awareness of the roles that e nterprise architecture (EA) and enterprise systems (ES) play in today ’s organizations.
background	2 EA and ES usage maturity models are used to assess how well c ompanies are capable of deploying these two concepts while striving to achi eve strategic corporate goals.
background	3 The existence of various architecture and ES usage models raises questions about how they both refer to each other, e.g. if a higher level of architecture maturity implies a higher ES usage level.
objective	4 This paper compares these two types of models by using literature survey results and case-study experiences.
result	5 We conclude that (i) EA and ES usage maturity model s agree on a number of critical success factors and (ii) in a company with a mature architecture function, one is likely to observe, at the early stages of ES initiatives, certain practices associated with a higher level of ES usage ma turity.

background	1 Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure.
background	2 The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis.
method	3 Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof).
method	4 The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach.
result	5 Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.

background	1 Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics.
background	2 This is why pixel-space video prediction is viewed as a promising avenue for unsupervised feature learning.
objective	3 In this work, we train a convolutional network to generate future frames given an input sequence.
method	4 To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function.
result	5 We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.

background	1 The amount of data stored by banks is rapidly increasing and provides the opportunity for banks to conduct predictive analytics and enhance its businesses.
background	2 However, data scientists are facing large challenges, handling the massive amount of data efficiently and generating insights with real business value.
objective	3 In this paper, the Intelligent Customer Analytics for Recognition and Exploration (iCARE) framework is presented to analyze banking customer behaviors from banking big data, through analytical modeling methodologies and techniques designed for a key business scenario.
method	4 Combining IBM software platforms and big data processing power with customized data analytical models, the iCARE solution provides deeper customer insights to satisfy a bank’s specific business need and data environment.
method	5 The advantages of the iCARE framework have been confirmed in a real case study of a bank in southeast China.
result	6 In this case, iCARE helps generate insights for active customers based on their transaction behavior, using close to 20 terabytes of data.

other	1 ral he tuffect as 3; a Abstract.
objective	2 We conduct an exhaustive survey of image thresholding methods, categorize them, express their formulas under a uniform notation, and finally carry their performance comparison.
method	3 The thresholding methods are categorized according to the information they are exploiting, such as histogram shape, measurement space clustering, entropy, object attributes, spatial correlation, and local gray-level surface.
method	4 40 selected thresholding methods from various categories are compared in the context of nondestructive testing applications as well as for document images.
method	5 The comparison is based on the combined performance measures.
method	6 We identify the thresholding algorithms that perform uniformly better over nondestructive testing and document image applications.
method	7 © 2004 SPIE and IS&T. [DOI: 10.1117/1.1631316]

objective	1 We show how to outsource data annotation to Amazon Mechanical Turk.
result	2 Doing so has produced annotations in quite large numbers relatively cheaply.
result	3 The quality is good, and can be checked and controlled.
result	4 Annotations are produced quickly.
result	5 We describe results for several different annotation problems.
result	6 We describe some strategies for determining when the task is well specified and properly priced.

background	1 From scientific research to commercial applications, eye tracking is an important tool across many domains.
background	2 Despite its range of applications, eye tracking has yet to become a pervasive technology.
background	3 We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices.
method	4 We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2:5M frames.
method	5 Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device.
method	6 Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively.
method	7 With calibration, this is reduced to 1.34cm and 2.12cm.
result	8 Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results.
other	9 The code, data, and models are available at http://gazecapture.csail.mit.edu.

background	1 This paper presents the design and preliminary investigation of a fully 3D printed soft robotic hand exoskeleton, Print-it-Yourself (PIY) Glove for stroke patients.
method	2 The PIY Glove is fabricated with Fused Deposition Modeling (FDM) using consumer based 3D printing technology to lower fabrication costs and allow patients to 3D print a rehabilitative and assistive device at home.
method	3 PIY Glove uses a novel, fold based design of 3D printed soft actuators to achieve bending motion in the fingers.
method	4 Fabrication guidelines of the PIY Glove are laid out and characterization of the glove in terms of its range of motion and grip force are also presented.
method	5 An EMG control system that achieves control of the PIY Glove is also described.
method	6 This work paves the way for the implementation of printable pneumatics in real-world applications, particularly robot-assisted hand therapy.

background	1 A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA).
background	2 The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature.
background	3 Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically.
objective	4 The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively.
background	5 The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively.
result	6 The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature.
result	7 The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems.
result	8 The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well.
result	9 Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html .

objective	1 We propose a new approach for general object tracking with fully convolutional neural network.
method	2 Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet.
result	3 The discoveries motivate the design of our tracking system.
result	4 It is found that convolutional layers in different levels characterize the target from different perspectives.
result	5 A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance.
result	6 Both layers are jointly used with a switch mechanism during tracking.
background	7 It is also found that for a tracking target, only a subset of neurons are relevant.
method	8 A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy.
result	9 Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.

background	1 Recognising the need for the development of research capacity and changing learning paradigms that include online and collaborative approaches, an ontology of research methodology needs to be developed to allow for the shared creation of knowledge in this domain.
objective	2 An ontology engineering approach is followed in developing a conceptual model of the domain using UML, with a focus on studies in the computing disciplines.
objective	3 A research scheme that is made up of a philosophical world view, a research design, and research methods is proposed.
method	4 Appropriate relations between these are identified, as well as attributes of the various concepts in the conceptual model.
result	5 A focus group consisting of senior researchers in the field of computing was utilised to validate the model.

background	1 The representation of a knowledge graph (KG) in a latent space recently has attracted more and more attention.
background	2 To this end, some proposed models (e.g., TransE) embed entities and relations of a KG into a "point" vector space by optimizing a global loss function which ensures the scores of positive triplets are higher than negative ones.
background	3 We notice that these models always regard all entities and relations in a same manner and ignore their (un)certainties.
method	4 In fact, different entities and relations may contain different certainties, which makes identical certainty insufficient for modeling.
method	5 Therefore, this paper switches to density-based embedding and propose KG2E for explicitly modeling the certainty of entities and relations, which learn the representations of KGs in the space of multi-dimensional Gaussian distributions.
result	6 Each entity/relation is represented by a Gaussian distribution, where the mean denotes its position and the covariance (currently with diagonal covariance) can properly represent its certainty.
method	7 In addition, compared with the symmetric measures used in point-based methods, we employ the KL-divergence for scoring triplets, which is a natural asymmetry function for effectively modeling multiple types of relations.
result	8 We have conducted extensive experiments on link prediction and triplet classification with multiple benchmark datasets (WordNet and Freebase).
result	9 Our experimental results demonstrate that our method can effectively model the (un)certainties of entities and relations in a KG, and it significantly outperforms state-of-the-art methods (including TransH and TransR).

background	1 This study explored the impact of the Internet on our reading behaviour.
background	2 Using an exploratory survey, it examined the online and offline reading behaviour of individuals, and determined the underlying patterns, the differences between online and offline reading, and the impacts of the online environment on individuals’ reading behaviour.
method	3 The findings indicated that there were definite differences between people’s online and offline reading behaviours.
method	4 In general, online reading has had a negative impact on people’s cognition.
result	5 Concentration, comprehension, absorption and recall rates were all much lower while reading online than offline.

objective	1 This study examines how leadership characteristics in new product development teams affect the learning, knowledge application, and subsequently the performance of these teams.
method	2 Using data from a study of 229 members from 52 high-tech new product projects, we empirically demonstrate that team learning has a strong positive effect on the innovativeness and speed to market of the new products.
method	3 Moreover, a democratic leadership style, initiation of goal structure by the team leader, and his or her position within the organization were positively related to team learning.
result	4 Managerial implications of these results are discussed.
result	5 Subject Areas: Cross-Functional Product Development Teams, knowledge Management, Learning, New Product Development, and Team Leadership.

background	1 In this paper, an electrocardiographic (ECG) signal processing IC, which is used for portable biomedical application, was designed using continuous-time technique.
background	2 The circuit consists of an instrumentation amplifier (INA) with driven-right-leg circuit (DRL), a 5th order Gm -C low pass filter (Gm-C LPF) operating in sub-threshold mode, and amplifiers.
method	3 DRL circuit is used to detect small amplitude signal in the presence of large common-mode voltage from the human body.
result	4 The CMRR of the INA is 78 dB and the Gm-C LPF has a cutoff frequency of 18 Hz.
result	5 As a result of using the DRL, a small signal can be detected in the presence of large common-mode differential.
method	6 The circuit consumes 1.23 mW when operating from with a supply voltage of plusmn1.5-V and occupies a core area of 0.94 mm2.
result	7 The circuit was designed in a 0.35mum CMOS process and simulation results have successfully demonstrated the functionalities

background	1 Cloud systems require elastic resource allocation to minimize resource provisioning costs while meeting service level objectives (SLOs).
background	2 In this paper, we present a novel PRedictive Elastic reSource Scaling (PRESS) scheme for cloud systems.
method	3 PRESS unobtrusively extracts fine-grained dynamic patterns in application resource demands and adjust their resource allocations automatically.
method	4 Our approach leverages light-weight signal processing and statistical learning algorithms to achieve online predictions of dynamic application resource requirements.
method	5 We have implemented the PRESS system on Xen and tested it using RUBiS and an application load trace from Google.
result	6 Our experiments show that we can achieve good resource prediction accuracy with less than 5% over-estimation error and near zero under-estimation error, and elastic resource scaling can both significantly reduce resource waste and SLO violations.

background	1 An increasing number of people are using the Internet, in many instances unaware of the information being collected about them.
background	2 In contrast, other people concerned about the privacy and security issues are limiting their use of the Internet, abstaining from purchasing products online.
background	3 Businesses should be aware that consumers are looking for privacy protection and a privacy statement can help to ease consumers' concerns.
objective	4 New Zealand based web sites are expected to have privacy statements on their web sites under the New Zealand Privacy Act 1993.
objective	5 The incidence of the information gathered from New Zealand web sites and their use of privacy statements is examined here.
background	6 In particular, web sites utilizing cookies and statements about them are scanned.
objective	7 Global consistency on Internet privacy protection is important to boost the growth of electronic commerce.
result	8 To protect consumers in a globally consistent manner, legislation, self-regulation, technical solutions and combination solutions are different ways that can be implemented.

method	1 The paper describes a method of fully automatic 3D-reconstruction of a mouse brain from a sequence of histological coronal 2D slices.
method	2 The model is constructed via non-linear transformations between the neighboring slices and further morphing.
method	3 We also use rigid-body transforms in the preprocessing stage to align the slices.
objective	4 Afterwards, the obtained 3D-model is used to generate virtual 2D-images of the brain in arbitrary section-plane.
method	5 We use this approach to construct a highresolution anatomic 3D-model of a mouse brain using well-known Allen Brain Atlas which is publicly available.

background	1 Given a set of keywords, we find a maximum Web query (containing the most keywords possible) that respects user-defined bounds on the number of returned hits.
background	2 We assume a real-world setting where the user is not given direct access to a Web search engine's index, i.e., querying is possible only through an interface.
objective	3 The goal to be optimized is the overall number of submitted Web queries.
objective	4 One original contribution of our research is the formalization and theoretical foundation of the problem.
objective	5 But, in particular, we develop a co-occurrence probability informed search strategy for the problem.
result	6 The performance gain achieved with our approach is substantial: compared to the uninformed baseline (without co-occurrence information) the expected savings are up to 20% in the number of submitted queries and runtime.

objective	1 This paper presents a new phase-unwrapping (PU) algorithm for SAR interferometry that makes use of a particle filter (PF) to perform simultaneously noise filtering and PU.
objective	2 The formulation of this technique provides independence from noise statistics and is not constrained by the nonlinearity of the problem.
method	3 In addition, an enhanced variant of this method combining a PF with artificial-intelligence search strategies and an omnidirectional local phase estimator, based on the mode of the power spectral density, is also presented.
result	4 Results obtained with synthetic and real data show a significant improvement with respect to other conventional unwrapping algorithms in some situations.

background	1 Over the last decade the importance of network games has seen a tremendous growth.
background	2 A large part includes the size reduction of the handheld devices.
background	3 Mobile gaming in a wireless environment and the availability to play games at any place is receiving major importance.
background	4 Thus more and more games are released in this section (including a huge number of different mobile phone games).
background	5 Thus, the mobile market offers a wide variety of devices, such as the new handhelds like Nintendo DS and Sony PSP.
background	6 With the increase in opportunities one must first look at the user behavior to understand how to improve current problems.
objective	7 This paper gives an introduction into the differences of current mobile gaming platforms and their capabilities.
method	8 Furthermore it features a user survey about individual preferences and social coefficients with unexpected results.
method	9 The current survey system features a database to handle the huge amount of answers (the predecessor used a polling system).
result	10 Concluding the results of the previous inquiry, this paper contains a lobby tool based on J2ME and C# to increase the matching mechanisms in a local environment.

objective	1 We propose a flexible new technique to easily calibrate a camera.
objective	2 It is well suited for use without specialized knowledge of 3D geometry or computer vision.
method	3 The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations.
method	4 Either the camera or the planar pattern can be freely moved.
method	5 The motion need not be known.
method	6 Radial lens distortion is modeled.
method	7 The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion.
result	8 Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained.
result	9 Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible.
result	10 It advances 3D computer vision one step from laboratory environments to real world use.

background	1 The end of Moores law and Dennard scaling has led to the end of rapid improvement in general-purpose program performance.
background	2 Machine learning (ML), and in particular deep learning, is an attractive alternative for architects to explore.
background	3 It has recently revolutionized vision, speech, language understanding, and many other fields, and it promises to help with the grand challenges facing our society.
method	4 The computation at its core is low-precision linear algebra.
method	5 Thus, ML is both broad enough to apply to many domains and narrow enough to benefit from domain-specific architectures, such as Googles Tensor Processing Unit (TPU).
method	6 Moreover, the growth in demand for ML computing exceeds Moores law at its peak, just as it is fading.
method	7 Hence, ML experts and computer architects must work together to design the computing systems required to deliver on the potential of ML.
result	8 This article offers motivation, suggestions, and warnings to computer architects on how to best contribute to the ML revolution.

background	1 Wireless sensor networks (WSNs) constitute a new pervasive and ubiquitous technology.
background	2 They have been successfully used in various application areas and in future computing environments, WSNs will play an increasingly important role.
background	3 However, programming sensor networks and applications to be deployed in them is extremely challenging.
background	4 It has traditionally been an error-prone task since it requires programming individual nodes, using low-level programming issues and interfacing with the hardware and the network.
background	5 This aspect is currently changing as different high-level programming abstractions and middleware solutions are coming into the arena.
other	6 Nevertheless, many research challenges are still open.
objective	7 This paper presents a survey of the current state-of-the-art in the field, establishing a classification and highlighting some likely research challenges and future directions.

background	1 Unternehmen sind zunehmend gezwungen, ihre Geschäftsmodelle anzupassen oder sogar neu zu erfinden,
background	2 um mit technologischen Entwicklungen und sich verändernden Kundenbedürfnissen Schritt halten zu können.
background	3 Ein konkurrenzfähiges Geschäftsmodell ist für Unternehmen von existenzieller Bedeutung.
method	4 In der wissenschaftlichen Literatur existieren mit der Business Model Canvas und dem St. Galler Business Model Navigator zwei umfangreiche Frameworks zur Beschreibung und Entwicklung von Geschäftsmodellen.
method	5 Es ist jedoch unklar, inwiefern diese oder andere strukturierte Vorgehensweisen in der Praxis tatsächlich zum Einsatz kommen und wie hilfreich sie sind.
result	6 Anhand von zwölf Experteninterviews konnte u.a.
method	7 festgestellt werden, dass die Weiterentwicklung des Geschäftsmodells eines Unternehmens zumeist nicht als Prozess verstetigt ist, sondern als Reaktion auf
other	8 eine negative Geschäftsentwicklung erfolgt.
background	9 Strukturierende Frameworks aus der einschlägigen Literatur kommen nur selten zum Einsatz und werden teilweise explizit abgelehnt.
method	10 Es existieren jedoch in Literatur und Praxis vielfältige Methoden zur kreativen

background	1 Most people use Facebook on a daily basis; few are aware of the consequences.
result	2 Based on a 1-week experiment with 1,095 participants in late 2015 in Denmark, this study provides causal evidence that Facebook use affects our well-being negatively.
method	3 By comparing the treatment group (participants who took a break from Facebook) with the control group (participants who kept using Facebook), it was demonstrated that taking a break from Facebook has positive effects on the two dimensions of well-being: our life satisfaction increases and our emotions become more positive.
result	4 Furthermore, it was demonstrated that these effects were significantly greater for heavy Facebook users, passive Facebook users, and users who tend to envy others on Facebook.

objective	1 Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values.
objective	2 In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a.
result	3 , informative missingness.
method	4 There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance.
method	5 In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts.
method	6 GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network.
method	7 It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results.
result	8 Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.

objective	1 The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform.
objective	2 To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems.
method	3 A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms.
method	4 We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures.
method	5 On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device.
result	6 On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.

background	1 Gamification, an emerging idea for using game design elements and principles to make everyday tasks more engaging, is permeating many different types of information systems.
background	2 Excitement surrounding gamification results from its many potential organizational benefits.
background	3 However, few research and design guidelines exist regarding gamified information systems.
objective	4 We therefore write this commentary to call upon information systems scholars to investigate the design and use of gamified information systems from a variety of disciplinary perspectives and theories, including behavioral economics, psychology, social psychology, information systems, etc.
method	5 We first explicate the idea of gamified information systems, provide real-world examples of successful and unsuccessful systems, and, based on a synthesis of the available literature, present a taxonomy of gamification design elements.
method	6 We then develop a framework for research and design: its main theme is to create meaningful engagement for users; that is, gamified information systems should be designed to address the dual goals of instrumental and experiential outcomes.
method	7 Using this framework, we develop a set of design principles and research questions, using a running case to illustrate some of our ideas.
result	8 We conclude with a summary of opportunities for IS researchers to extend our knowledge of gamified information systems, and, at the same time, advance existing theories.

background	1 In the Spring of 2010, ACUTA conducted a survey of its institutional members regarding their institutions’ use of social networking sites.
background	2 This literature review was compiled in conjunction with the survey research.
method	3 The first section describes resources that were recommended by ACUTA members who serve either on the ACUTA Higher Education Advisory Panel or the ACUTA Social Networking, New Media, and Web Resources Subcommittee.
method	4 The other sections of the review identify scholarly publications and presentations that were identified through online library searches about social media or social networks in higher education.
result	5 Abstracts were drawn from the sources indicated at the end of each abstract.

background	1 The growing power of bloggers to influence their connected network has emerged as a new communication venue for brands.
objective	2 This study elaborates upon the role of bloggers in brand communication, and reveals how brands can engage with bloggers, currently considered as online opinion leaders, from the perspective of the two-step flow theory.
method	3 Following clarification of the aims of the study, we report on in-depth interviews with 17 brand and digital agency representatives, selected because they regard communication with bloggers as an important strategy in increasing the influence of their brands among online communities.
method	4 This exploratory study reflects current blogger communication implementations, and concludes with a discussion of seven major issues arising from the literature review and interviews (definition of bloggers, blogger selection criteria, digital integration, power of bloggers, long-term relationship building with bloggers, measurement, and budgetary issues in blogger communication).
objective	5 These areas represent relatively unexplored areas of blogger engagement from both an academic and managerial perspective.
method	6 Based on the findings of the interviews, we propose a model which traces the influencer role of bloggers from the two-step flow theory perspective.
method	7 This model is named as the brand communication through digital influencers model.
other	8 © 2014 Elsevier Ltd.
other	9 All rights reserved.

background	1 Previous work has used monolingual parallel corpora to extract and generate paraphrases.
method	2 We show that this task can be done using bilingual parallel corpora, a much more commonly available resource.
method	3 Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.
method	4 We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.
result	5 We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.

background	1 Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care.
background	2 Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured.
method	3 Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them.
result	4 There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge.
method	5 The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data.
method	6 In this article, we review the recent literature on applying deep learning technologies to advance the health care domain.
method	7 Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health.
result	8 However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists.
method	9 We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.

background	1 GCC 4.5.0 introduces support for link time optimization (LTO).
background	2 The LTO infrastructure is designed to allow parallel linking of large applications using a special mode, WHOPR.
objective	3 In this paper we present an overview of the design and implementation of WHOPR and present results of its behavior when optimizing large applications.
method	4 We compare WHOPR’s compile time, memory usage, and code quality to the results of the classical file-byfile optimization model, focusing on its effects on GCC itself and the Firefox web browser.
result	5 We examine critical issues which arise only when considering large applications, such as startup time and code size growth.

background	1 In this study we investigate how social media shape the networked public sphere and facilitate communication between communities with different political orientations.
result	2 We examine two networks of political communication on Twitter, comprised of more than 250,000 tweets from the six weeks leading up to the 2010 U.S. congressional midterm elections.
method	3 Using a combination of network clustering algorithms and manually-annotated data we demonstrate that the network of political retweets exhibits a highly segregated partisan structure, with extremely limited connectivity between leftand right-leaning users.
method	4 Surprisingly this is not the case for the user-to-user mention network, which is dominated by a single politically heterogeneous cluster of users in which ideologically-opposed individuals interact at a much higher rate compared to the network of retweets.
background	5 To explain the distinct topologies of the retweet and mention networks we conjecture that politically motivated individuals provoke interaction by injecting partisan content into information streams whose primary audience consists of ideologically-opposed users.
result	6 We conclude with statistical evidence in support of this hypothesis.

objective	1 This paper describes the configuration of a floor-tile installation robot for commercial buildings.
objective	2 The research is motivated by the need to reduce the installation time and cost while guaranteeing consistent quality.
objective	3 In order to compete with human installation, a time of 24 seconds per installed tile has to be matched.
method	4 The technical solution that is deemed feasible and capable of reducing this time to about 10 seconds, is an autonomous, electrically-powered mobile robot with omni-directional locomotive capability, and stereo cameras and light-striper for sensing.
method	5 High resolution imaging is needed to identify tile seams and edges, assess the quality of automatic installation, and locate where the next tile should be placed.
method	6 A mechanically compliant placement device would place the tile quickly and accurately without damaging the placed and surrounding tiles, emulating a human capability.
method	7 Vinyl/ceramic tiles, adhesives, and grout are carried onboard the robot and replenished by the operator.
method	8 Navigation and positioning are performed through a laserbased triangulation system, and by detecting, counting and dead-reckoning off of tiles placed on the floor.
result	9 Tile and installation quality are continuously monitored and errors corrected for, based on an overall layout map.

background	1 This technical note describes a new baseline for the Natural Questions (Kwiatkowski et al., 2019).
background	2 Our model is based on BERT (Devlin et al., 2018) and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30% and 50% relative for the long and short answer tasks respectively.
objective	3 This baseline has been submitted to the official NQ leaderboard and we plan to opensource the code for it in the near future.

background	1 A memory-based learning system is an extended memory management system that decomposes the input space either statically or dynamically into subregions for the purpose of storing and retrieving functional information.
background	2 The main generalization techniques employed by memory-based learning systems are the nearest-neighbor search, space decomposition techniques, and clustering.
background	3 Research on memory-based learning is still in its early stage.
background	4 In particular, there are very few rigorous theoretical results regarding memory requirement, sample size, expected performance, and computational complexity.
objective	5 In this paper, we propose a model for memory-based learning and use it to analyze several methods— ε-covering, hashing, clustering, tree-structured clustering, and receptive-fields—for learning smooth functions.
method	6 The sample size and system complexity are derived for each method.
method	7 Our model is built upon the generalized PAC learning model of Haussler
result	8 (Haussler, 1989) and is closely related to the method of vector quantization in data compression.
result	9 Our main result is that we can build memory-based learning systems using new clustering algorithms (Lin & Vitter, 1992a) to PAC-learn in polynomial time using only polynomial storage in typical situations.

background	1 The use of the Internet by older adults is growing at a substantial rate.
background	2 They are becoming an increasingly important potential market for electronic commerce.
background	3 However, previous researchers and practitioners have focused mainly on the youth market and paid less attention to issues related to the online behaviors of older consumers.
result	4 To bridge the gap, the purpose of this study is to increase a better understanding of the drivers and barriers affecting older consumers’ intention to shop online.
objective	5 To this end, this study is developed by integrating the Unified Theory of Acceptance and Use of Technology (UTAUT) and innovation resistance theory.
result	6 By comparing younger consumers with their older counterparts, in terms of gender the findings indicate that the major factors driving older adults toward online shopping are performance expectation and social influence which is the same with younger.
result	7 On the other hand, the major barriers include value, risk, and tradition which is different from younger.
result	8 Consequently, it is notable that older adults show no gender differences in regards to the drivers and barriers.
result	9 2014 Elsevier Ltd. All rights reserved.

background	1 This study captures the state of current satellite transponder technology, specifically, solid-state power amplifiers (SSPAs) and traveling wave tube amplifiers (TWTAs), and describes expected future advances, including GaN SSPAs.
background	2 The findings of five previous SSPA and TWTA studies, including the 1991 European Space and Technology Center study, the 1993 National Aeronautics and Space Administration study, and three Boeing studies conducted in 2005, 2008, and 2013, are tabulated and summarized.
background	3 The results of these studies are then compared with new analyses of two validated sources of amplifier data: a commercially licensed database, Seradata’s Spacetrak, and a publicly available database, Gunter’s Space Page.
result	4 The new analyses consider a total of 18,902 amplifiers (6428 TWTAs, 2158 SSPAs, and 10,316 unspecified amplifiers) onboard 565 communications satellites launched from 1982 to 2016.
result	5 This new study contains the largest number of satellites and amplifiers to date and compares output power, redundancy, and bandwidth capabilities.
result	6 We find an increase in output power from the 1993 study of >200% for Ku-band TWTAs and C-band SSPAs, and >1000% increase for C-band TWTAs.
result	7 The ratio of operational to redundant amplifiers is 10 times higher for TWTAs than SSPAs, and the majority of amplifiers over the past 30 years operate with bandwidth less than 100MHz.
result	8 A second analysis is conducted using failure records and telemetry of 16 geostationary satellites equipped with 659 amplifiers: 535 SSPAs and 124 TWTAs.
result	9 We find that <2% of TWTAs and 5% of SSPAs experience anomalies.
objective	10 Overall, this research was performed to update and clarify how the power and bandwidth needs and redundancy trends of the SatCom community have evolved over the past 30 years.

background	1 Now, we come to offer you the right catalogues of book to open.
background	2 multisensor data fusion a review of the state of the art is one of the literary work in this world in suitable to be reading material.
background	3 That's not only this book gives reference, but also it will show you the amazing benefits of reading a book.
method	4 Developing your countless minds is needed; moreover you are kind of people with great curiosity.
result	5 So, the book is very appropriate for you.

objective	1 Knowledge graph completion aims to perform link prediction between entities.
background	2 In this paper, we consider the approach of knowledge graph embeddings.
background	3 Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity.
background	4 We note that these models simply put both entities and relations within the same semantic space.
background	5 In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling.
method	6 In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces.
method	7 Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities.
result	8 In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction.
result	9 Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.

background	1 Several real world prediction problems involve forecasting rare values of a target variable.
background	2 When this variable is nominal we have a problem of class imbalance that was already studied thoroughly within machine learning.
background	3 For regression tasks, where the target variable is continuous, few works exist addressing this type of problem.
result	4 Still, important application areas involve forecasting rare extreme values of a continuous target variable.
other	5 This paper describes a contribution to this type of tasks.
other	6 Namely, we propose to address such tasks by sampling approaches.
result	7 These approaches change the distribution of the given training data set to decrease the problem of imbalance between the rare target cases and the most frequent ones.
result	8 We present a modification of the well-known Smote algorithm that allows its use on these regression tasks.
result	9 In an extensive set of experiments we provide empirical evidence for the superiority of our proposals for these particular regression tasks.
result	10 The proposed SmoteR method can be used with any existing regression algorithm turning it into a general tool for addressing problems of forecasting rare extreme values of a continuous target variable.

background	1 Manufacturing organizations are able to accumulate large amounts of plant floor production and environmental data due to advances in data collection, communications technology, and use of standards.
other	2 The challenge has shifted from collecting a sufficient amount of data to analyzing and making decisions based on the huge amount of data available.
result	3 Data analytics (DA) can help understand and gain insights from the big data and in turn help advance towards the vision of smart manufacturing.
method	4 Modeling and simulation have been used by manufacturers to analyze their operations and support decision making.
method	5 This paper proposes multiple methods in which simulation can serve as a DA application or support other DA applications in manufacturing environment to address big data issues.
method	6 An example case is discussed to demonstrate one use of simulation.
result	7 In the presented case, a virtual representation of machining operations is used to generate the data required to evaluate manufacturing data analytics applications.

other	1 0747-5632/$ see front matter
other	2 2011 Elsevier Ltd. A doi:10.1016/j.chb.2011.08.023 ⇑ Corresponding author.
other	3 Tel.
other	4 : +1 610 519 7940; fax
background	5 E-mail address: rebecca.brand@villanova.edu (R.J. Attractive people are considered by others to have many positive qualities and in the case of social skills and intelligence, these attributions are often true.
background	6 In internet dating, individuals with attractive profile photos are viewed more favorably overall, but no research has yet established whether they indeed have more positive qualities.
method	7 We addressed this issue by having 50 women independently rate 100 photos and free-written texts taken from males’ profiles on a popular dating website.
method	8 Photos rated as physically attractive had profile texts that were rated as more attractive, even though photos and texts were rated by different judges.
method	9 Perceived confidence seemed to play a mediating role, suggesting that attractive men write appealing texts because they are aware of their high mate value.
other	10 Thus, contrary to popular belief, the internet does not seem to ‘‘level the playing field.

background	1 Heuristic evaluation is an informal method of usability analysis where a number of evaluators are presented with an interface design and asked to comment on it.
background	2 Four experiments showed that individual evaluators were mostly quite bad at doing such heuristic evaluations and that they only found between 20 and 51% of the usability problems in the interfaces they evaluated.
result	3 On the other hand, we could aggregate the evaluations from several evaluators to a single evaluation and such aggregates do rather well, even when they consist of only three to five people.

background	1 Advanced mobile technology continues to shape professional environments.
background	2 Smart cell phones, pocket computers and laptop computers reduce the need of users to remain close to a wired information system infrastructure and allow for task performance in many different contexts.
objective	3 Among the consequences are changes in technology requirements, such as the need to limit weight and size of the devices.
method	4 In the current paper, we focus on the factors that users find important in mobile devices.
method	5 Based on a content analysis of online user reviews that was followed by structural equation modeling, we found four factors to be significantly related with overall user evaluation, namely functionality, portability, performance, and usability.
method	6 Besides the practical relevance for technology developers and managers, our research results contribute to the discussion about the extent to which previously established theories of technology adoption and use are applicable to mobile technology.
result	7 We also discuss the methodological suitability of online user reviews for the assessment of user requirements, and the complementarity of automated and non-automated forms of content analysis.

background	1 Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA.
background	2 While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data.
objective	3 To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization.
method	4 In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services.
method	5 We propose the Big Data Governance Framework in this paper.
method	6 The Big Data governance framework presents criteria different from existing criteria at the data quality level.
method	7 It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services.
method	8 In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems.
method	9 This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea.
background	10 Big Data services in the public sector are an inevitable choice to improve the quality of people's life.

background	1 A systematic theory is introduced for finding the derivatives of complex-valued matrix functions with respect to a complex-valued matrix variable and the complex conjugate of this variable.
background	2 In the framework introduced, the differential of the complex-valued matrix function is used to identify the derivatives of this function.
result	3 Matrix differentiation results are derived and summarized in tables which can be exploited in a wide range of signal processing related situations

background	1 3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty.
background	2 Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination.
background	3 In general these methods require complex and inefficient pipelines for model building and fitting.
objective	4 In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans.
objective	5 Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model.
result	6 We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image.
result	7 We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions.
other	8 Code and models will be made available at http://aaronsplace.co.uk

background	1 E-government research has become a recognized research domain and many policies and strategies are formulated for e-government implementations.
background	2 Most of these target the next few years and limited attention has been giving to the long term.
background	3 The eGovRTD2020, a European Commission co-funded project, investigated the future research on e-government driven by changing circumstances and the evolution of technology.
objective	4 This project consists of an analysis of the state of play, a scenario-building, a gap analysis and a roadmapping activity.
method	5 In this paper the roadmapping methodology fitting the unique characteristics of the e-government field is presented and the results are briefly discussed.
method	6 The use of this methodology has resulted in the identification of a large number of e-government research themes.
method	7 It was found that a roadmapping methodology should match the unique characteristics of e-government.
result	8 The research shows the need of multidisciplinary research.

background	1 With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals’ lives.
background	2 For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques.
background	3 Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on.
objective	4 Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination.
method	5 They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information.
result	6 In this paper, we first discuss three causes of unfairness in machine learning.
method	7 We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models.
method	8 We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.

background	1 Aesthetic seems currently under represented in most current data visualization evaluation methodologies.
objective	2 This paper investigates the results of an online survey of 285 participants, measuring both perceived aesthetic as well as the efficiency and effectiveness of retrieval tasks across a set of 11 different data visualization techniques.
result	3 The data visualizations represent an identical hierarchical dataset, which has been normalized in terms of color, typography and layout balance.
method	4 This study measured parameters such as speed of completion, accuracy rate, task abandonment and latency of erroneous response.
result	5 Our findings demonstrate a correlation between latency in task abandonment and erroneous response time in relation to visualization's perceived aesthetic.
result	6 These results support the need for an increased recognition for aesthetic in the typical evaluation process of data visualization techniques.

objective	1 In this paper, a deep neural network (Behavior-CNN) is proposed to model pedestrian behaviors in crowded scenes, which has many applications in surveillance.
method	2 A pedestrian behavior encoding scheme is designed to provide a general representation of walking paths, which can be used as the input and output of CNN.
method	3 The proposed Behavior-CNN is trained with real-scene crowd data and then thoroughly investigated from multiple aspects, including the location map and location awareness property, semantic meanings of learned filters, and the influence of receptive fields on behavior modeling.
result	4 Multiple applications, including walking path prediction, destination prediction, and tracking, demonstrate the effectiveness of Behavior-CNN on pedestrian behavior modeling.

background	1 A different paradigm is needed for real-time command and control (C&C) problems.
background	2 Past approaches, using multiprocessors (MP), for real-time computing have had great difficulty in meeting real problem requirements.
background	3 We review some reasons why C&C problems that require a solution on a MP architecture may be intractable, and then show an architecture where these reasons for intractability are nonexistent.
method	4 We describe a polynomial time solution to the air traffic control (ATC) problem, which is a typical C&C problem.
method	5 This solution uses a static, non-preemptive table driven schedule using a SIMD architecture called an associative processor (AP).
result	6 The AP is an ideal processor for set and database operations since its single thread instruction stream can operate on an entire set of data with each instruction.
result	7 The AP eliminates multi-thread instructions, which account for much of the MP intractability mentioned above.

background	1 This paper introduces a novel algorithm that increases the efficiency of the current cloud-based smart-parking system and develops a network architecture based on the Internet-of-Things technology.
background	2 This paper proposed a system that helps users automatically find a free parking space at the least cost based on new performance metrics to calculate the user parking cost by considering the distance and the total number of free places in each car park.
background	3 This cost will be used to offer a solution of finding an available parking space upon a request by the user and a solution of suggesting a new car park if the current car park is full.
objective	4 The simulation results show that the algorithm helps improve the probability of successful parking and minimizes the user waiting time.
result	5 We also successfully implemented the proposed system in the real world.

background	1 The topic of cyber warfare is a vast one, with numerous sub topics receiving attention from the research community.
background	2 We first examine the most basic question of what cyber warfare is, comparing existing definitions to find common ground or disagreements.
background	3 We discover that there is no widely adopted definition and that the terms cyber war and cyber warfare are not well enough differentiated.
method	4 To address these issues, we present a definition model to help define both cyber warfare and cyber war.
method	5 The paper then identifies nine research challenges in cyber warfare and analyses contemporary work carried out in each.
background	6 We conclude by making suggestions on how the field may best be progressed by future efforts.
other	7 © 2014 Elsevier Ltd.
other	8 All rights reserved.

background	1 Industry 4.0 stands for the 4 Industrial revolution and the new paradigm of autonomous and decentralized control in production.
background	2 Products and production systems are enhanced to Cyber Physical Systems which have the capability to communicate with each other, to build ad-hoc networks and for self-control and self-optimization.
background	3 From the IT-perspective this involves a new level of networking, data integration and data processing in production.
background	4 Established technologies like Internet of Things, Cloud or Big Data are propagated solution-components of Industry 4.0.
background	5 So far, there is no founded elaboration of IT-requirements and no differentiated discussion on how solution-components fulfil these requirements.
method	6 This research uses the method of content analysis to extract requirements of Industry 4.0 from current research publications.
objective	7 Objective of analysis is a structured compilation of requirements regarding data processing.
method	8 The resulting category scheme enables further development of solution-components in the application domain of Industry 4.0.
result	9 Furthermore, this paper shows how the requirements can be matched to the capabilities of Big Data software solutions.
result	10 As a result, two general use cases for Big Data applications in Industry 4.0 were identified and characterized.

background	1 Improvement of traffic safety by cooperative vehicular applications is one of the most promising benefits of vehicular ad hoc networks (VANETs).
background	2 However, to properly develop such applications, the influence of different driving parameters on the event of vehicle collision must be assessed at an early design stage.
objective	3 In this paper, we derive a stochastic model for the number of accidents in a platoon of vehicles equipped with a warning collision notification system, which is able to inform all the vehicles about an emergency event.
method	4 In fact, the assumption of communications being used is key to simplify the derivation of a stochastic model.
method	5 The model enables the computation of the average number of collisions that occur in the platoon, the probabilities of the different ways in which the collisions may take place, as well as other statistics of interest.
method	6 Although an exponential distribution has been used for the traffic density, it is also valid for different probability distributions for traffic densities, as well as for other significant parameters of the model.
method	7 Moreover, the actual communication system employed is independent of the model since it is abstracted by a message delay variable, which allows it to be used to evaluate different communication technologies.
result	8 We validate the proposed model with Monte Carlo simulations.
result	9 With this model, one can quickly evaluate numerically the influence of different model parameters (vehicle density, velocities, decelerations, and delays) on the collision process and draw conclusions that shed relevant guidelines for the design of vehicular communication systems, as well as chain collision avoidance applications.
result	10 Illustrative examples of application are provided, although a systematic characterization and evaluation of different scenarios is left as future work.

other	1 0957-4174/$ see front matter 2012 Elsevier Ltd. A http://dx.doi.org/10.1016/j.eswa.2012.07.028 ⇑ Corresponding author.
other	2 Address: University of R Engineering, Department of Enterprise Engineering, Rome, Italy.
other	3 Tel: +39 0672597799; fax: +39 06725979 E-mail addresses: roberta.costa@uniroma2.it (R. C oma2.it (T. Menichini).
background	4 ‘‘If a tree falls in the forest and no one is around to hear it, does it make a sound?
result	5 ’’ and, paraphrasing the proverbial philosophy question, if a company has a strong CSR commitment but nobody recognizes it, does it produce any benefits?
background	6 Business returns from corporate social responsibility (CSR) practices, such as customers loyalty and company reputation, depend heavily on how stakeholders perceive the company social behavior, making the measure of stakeholder perception a key issue in the process of CSR assessment.
objective	7 In this paper the analysis of CSR activities, as perceived by stakeholders, is realized utilizing global reporting initiative (GRI) indicators structured under balanced scorecard (BSC) perspectives and sustainability dimensions.
method	8 We utilize a multi-criteria approach combined with fuzzy linguistic variables, in the variation of the 2-tuple, creating a hierarchy of CSR components with the purpose of integrating financial and non-financial sustainability dimensions and strategic perspectives.
result	9 The hierarchy provides a multidimensional model that allows to evaluate the multifaceted social behavior of a company: the same company can be perceived simultaneously as responsible or irresponsible depending on the considered dimension and perspective.
other	10 2012 Elsevier Ltd. All rights reserved.

background	1 User satisfaction with information systems (IS) is considered an important indicator of information systems success and has been the subject of numerous research studies since the field’s inception.
objective	2 In this paper, we review the user satisfaction research in the IS field.
method	3 We discuss the roots of user satisfaction research as it pertains to satisfaction studies in marketing research and how these studies have been used to inform the IS context.
method	4 We also discuss how the study of user satisfaction and use of the construct in IS research has evolved and matured over time.
result	5 Finally, we discuss antecedents and outcomes of user satisfaction identified in IS research and provide suggestions for future research.

background	1 This paper describes the implementation of an attack on the Bluetooth security mechanism.
method	2 Specifically, we describe a passive attack, in which an attacker can find the PIN used during the pairing process.
method	3 We then describe the cracking speed we can achieve through three optimizations methods.
method	4 Our fastest optimization employs an algebraic representation of a central cryptographic primitive (SAFER+) used in Bluetooth.
result	5 Our results show that a 4-digit PIN can be cracked in less than 0.3 sec on an old Pentium III 450MHz computer, and in 0.06 sec on a Pentium IV 3Ghz HT computer.

background	1 A new musical instrument classification method using convolutional neural networks (CNNs) is presented in this paper.
method	2 Unlike the traditional methods, we investigated a scheme for classifying musical instruments using the learned features from CNNs.
method	3 To create the learned features from CNNs, we not only used a conventional spectrogram image, but also proposed multiresolution recurrence plots (MRPs) that contain the phase information of a raw input signal.
method	4 Consequently, we fed the characteristic timbre of the particular instrument into a neural network, which cannot be extracted using a phase-blinded representations such as a spectrogram.
result	5 By combining our proposed MRPs and spectrogram images with a multi-column network, the performance of our proposed classifier system improves over a system that uses only a spectrogram.
result	6 Furthermore, the proposed classifier also outperforms the baseline result from traditional handcrafted features and classifiers.

background	1 We present an Alternating Direction Method of Multipliers (ADMM) algorithm for solving optimization problems with an ℓ1 regularized least-squares cost function subject to recursive equality constraints.
method	2 The considered optimization problem has applications in control, for example in ℓ1 regularized MPC.
method	3 The ADMM algorithm is easy to implement, converges fast to a solution of moderate accuracy, and enables separation of the optimization problem into sub-problems that may be solved in parallel.
method	4 We show that the most costly step of the proposed ADMM algorithm is equivalent to solving an LQ regulator problem with an extra linear term in the cost function, a problem that can be solved efficiently using a Riccati recursion.
method	5 We apply the ADMM algorithm to an example of ℓ1 regularized MPC.
result	6 The numerical examples confirm fast convergence to sufficient accuracy and a linear complexity in the MPC prediction horizon.

background	1 Information technology should have much to offer linguistics, not only through the opportunities offered by large-scale data analysis and the stimulus to develop formal computational models, but through the chance to use language in systems for automatic natural language processing.
background	2 The paper discusses these possibilities in detail, and then examines the actual work that has been done.
objective	3 It is evident that this has so far been primarily research within a new field, computational linguistics, which is largely motivated by the demands, and interest, of practical processing systems, and that information technology has had rather little influence on linguistics at large.
objective	4 There are different reasons for this, and not all good ones: information technology deserves more attention from linguists.

background	1 We consider learning representations of entities and relations in KBs using the neural-embedding approach.
objective	2 We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions.
method	3 Under this framework, we compare a variety of embedding models on the link prediction task.
method	4 We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase).
method	5 Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq ^ CityInCountrypb, cq ùñ Nationalitypa, cq.
result	6 We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication.
result	7 More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art confidence-based rule mining approach in mining horn rules that involve compositional reasoning.

background	1 Soflware Quality Assurance (SQA) organizations are responsible for assuring that defined processes are followed by project teams.
background	2 The multitude of tasks that are needed to execute a software lifecycle can overwhelm SQA resources making selection of tasks to audit similar to shooting at a moving target, blindfolded.
background	3 The Mission Operations Directorate In foramtion System (MODI$) Group, an organization within NASA's Johnson Space Center, has developed a statistical method to determine a representative sample size for the number of process tasks to be audited by SQA.
method	4 This method combines standard statistical sampling techniques with risk analysis to determine and appropriate sampling size.
result	5 The result is a sample of process tasks which is truly representative ofthe work performed.

objective	1 This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features.
method	2 The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor.
method	3 The performance of the proposed method is assessed in the face recognition problem under different challenges.
method	4 Other applications and several extensions are also discussed

other	1 0957-4174/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.eswa.2010.10.027 ⇑ Corresponding author.
other	2 Tel.
other	3 : +90 332 2234350; fax E-mail address: melekacar@yahoo.com (M. Acar B Prediction of stock price index movement is regarded as a challenging task of financial time series prediction.
result	4 An accurate prediction of stock price movement may yield profits for investors.
result	5 Due to the complexity of stock market data, development of efficient models for predicting is very difficult.
method	6 This study attempted to develop two efficient models and compared their performances in predicting the direction of movement in the daily Istanbul Stock Exchange (ISE) National 100 Index.
method	7 The models are based on two classification techniques, artificial neural networks (ANN) and support vector machines (SVM).
method	8 Ten technical indicators were selected as inputs of the proposed models.
method	9 Two comprehensive parameter setting experiments for both models were performed to improve their prediction performances.
result	10 Experimental results showed that average performance of ANN model (75.74%) was found significantly better than that of SVM model (71.52%).

background	1 The Bjøntegaard model is widely used to calculate the coding efficiency between different codecs.
background	2 However, this model might not be an accurate predictor of the true coding efficiency as it relies on PSNR measurements.
objective	3 Therefore, in this paper, we propose a model to calculate the average coding efficiency based on subjective quality scores, i.e., mean opinion scores (MOS).
method	4 We call this approach Subjective Comparison of ENcoders based on fItted Curves (SCENIC).
method	5 To consider the intrinsic nature of bounded rating scales, a logistic function is used to fit the rate-distortion (R-D) values.
method	6 The average MOS and bit rate differences are computed between the fitted R-D curves.
method	7 The statistical property of subjective scores is considered to estimate corresponding confidence intervals on the calculated average MOS and bit rate differences.
result	8 The proposed model is expected to report more realistic coding efficiency as PSNR is not always correlated with perceived visual quality.

objective	1 This paper discusses empirical and analytical rules to select a suitable grid resolution for output maps and based on the inherent properties of the input data.
background	2 The choice of grid resolution was related with the cartographic and statistical concepts: scale, computer processing power, positional accuracy, size of delineations, inspection density, spatial autocorrelation structure and complexity of terrain.
method	3 These were further related with the concepts from the general statistics and information theory such as Nyquist frequency concept from signal processing and equations to estimate the probability density function.
method	4 Selection of grid resolution was demonstrated using four datasets: (1) GPS positioning data— the grid resolution was related to the area of circle described by the error radius, (2) map of agricultural plots—the grid resolution was related to the size of smallest and narrowest plots, (3) point dataset from soil mapping—the grid resolution was related to the inspection density, nugget variation and range of spatial autocorrelation and (4) contour map used for production of digital elevation model—the grid resolution was related with the spacing between the contour lines i.e. complexity of terrain.
method	5 It was concluded that no ideal grid resolution exists, but rather a range of suitable resolutions.
method	6 One should at least try to avoid using resolutions that do not comply with the effective scale or inherent properties of the input dataset.
method	7 Three standard grid resolutions for output maps were finally recommended: (a) the coarsest legible grid resolution—this is the largest resolution that we should use in order to respect the scale of work and properties of a dataset; (b) the finest legible grid resolution—this is the smallest grid resolution that represents 95% of spatial objects or topography; and (c) recommended grid resolution—a compromise between the two.
objective	8 Objective procedures to derive the true optimal grid resolution that maximizes the predictive capabilities or information content of a map are further discussed.
method	9 This methodology can now be integrated within a GIS package to help inexperienced users select a suitable grid resolution without doing extensive data preprocessing.
other	10 r 2005 Elsevier Ltd.

background	1 All models are wrong, but some are useful.
background	2 2 Acknowledgements The authors of this guide would like to thank David Warde-Farley, Guillaume Alain and Caglar Gulcehre for their valuable feedback.
background	3 Special thanks to Ethan Schoonover, creator of the Solarized color scheme, 1 whose colors were used for the figures.
background	4 Feedback Your feedback is welcomed!
objective	5 We did our best to be as precise, informative and up to the point as possible, but should there be anything you feel might be an error or could be rephrased to be more precise or com-prehensible, please don't refrain from contacting us.
method	6 Likewise, drop us a line if you think there is something that might fit this technical report and you would like us to discuss – we will make our best effort to update this document.
other	7 Source code and animations The code used to generate this guide along with its figures is available on GitHub.
other	8 2 There the reader can also find an animated version of the figures.

background	1 The dominant approach for many NLP tasks are recurrent neura l networks, in particular LSTMs, and convolutional neural networks.
background	2 However , these architectures are rather shallow in comparison to the deep convolutional n etworks which are very successful in computer vision.
background	3 We present a new archite ctur for text processing which operates directly on the character level and uses o nly small convolutions and pooling operations.
objective	4 We are able to show that the performa nce of this model increases with the depth: using up to 29 convolutional layer s, we report significant improvements over the state-of-the-art on several public t ext classification tasks.
result	5 To the best of our knowledge, this is the first time that very de ep convolutional nets have been applied to NLP.

background	1 Bitcoin cryptocurrency demonstrated the utility of global consensus across thousands of nodes, changing the world of digital transactions forever.
background	2 In the early days of Bitcoin, the performance of its probabilistic proof-of-work (PoW) based consensus fabric, also known as blockchain, was not a major issue.
background	3 Bitcoin became a success story, despite its consensus latencies on the order of an hour and the theoretical peak throughput of only up to 7 transactions per second.
background	4 The situation today is radically different and the poor performance scalability of early PoW blockchains no longer makes sense.
method	5 Specifically, the trend of modern cryptocurrency platforms, such as Ethereum, is to support execution of arbitrary distributed applications on blockchain fabric, needing much better performance.
background	6 This approach, however, makes cryptocurrency platforms step away from their original purpose and enter the domain of database-replication protocols, notably, the classical state-machine replication, and in particular its Byzantine fault-tolerant (BFT) variants.
method	7 In this paper, we contrast PoW-based blockchains to those based on BFT state machine replication, focusing on their scalability limits.
method	8 We also discuss recent proposals to overcoming these scalability limits and outline key outstanding open problems in the quest for the “ultimate” blockchain fabric(s).

background	1 Potential consequences of disasters involve overwhelming economic losses, large affected populations and serious environmental damages.
background	2 Given these devastating effects, there is an increasing interest in developing measures in order to diminish the possible impact of disasters, which gave rise to the field of disaster operations management (DOM).
background	3 In this paper we review recent OR/MS research in DOM.
background	4 Our work is a continuation of a previous review from Altay and Green (2006).
objective	5 Our purpose is to evaluate how OR/MS research in DOM has evolved in the last years and to what extent the gaps identified by Altay and Green (2006) have been covered.
background	6 Our findings show no drastic changes or developments in the field of OR/MS in DOM since the publication of Altay and Green (2006).
method	7 Additionally to our comparative analysis, we present an original evaluation about the most common assumptions in recent OR/MS literature in DOM.
result	8 Based on our findings we provide future research directions in order to make improvements in the areas where lack of research is detected.

method	1 We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments.
method	2 We look at hierarchical self-organizing maps, and mixture models.
method	3 We review grid-based clustering, focusing on hierarchical density-based approaches.
method	4 Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.
other	5 C © 2011 Wiley Periodicals, Inc.

background	1 The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data.
method	2 Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors.
method	3 This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks.
method	4 This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.

background	1 A fully-integrated FMCW radar system for automotive applications operating at 77 GHz has been proposed.
background	2 Utilizing a fractional- synthesizer as the FMCW generator, the transmitter linearly modulates the carrier frequency across a range of 700 MHz.
method	3 The receiver together with an external baseband processor detects the distance and relative speed by conducting an FFT-based algorithm.
result	4 Millimeter-wave PA and LNA are incorporated on chip, providing sufficient gain, bandwidth, and sensitivity.
result	5 Fabricated in 65-nm CMOS technology, this prototype provides a maximum detectable distance of 106 meters for a mid-size car while consuming 243 mW from a 1.2-V supply.

background	1 The wound healing assay in vitro is widely used for research and discovery in biology and medicine.
background	2 This assay allows for observing the healing process in vitro in which the cells on the edges of the artificial wound migrate toward the wound area.
background	3 The influence of different culture conditions can be measured by observing the change in the size of the wound area.
background	4 For further investigation, more detailed measurements of the cell behaviors are required.
method	5 In this paper, we present an application of automatic cell tracking in phase-contrast microscopy images to wound healing assay.
method	6 The cell behaviors under three different culture conditions have been analyzed.
method	7 Our cell tracking system can track individual cells during the healing process and provide detailed spatio-temporal measurements of cell behaviors.
result	8 The application demonstrates the effectiveness of automatic cell tracking for quantitative and detailed analysis of the cell behaviors in wound healing assay in vitro.

background	1 Correspondence Harshit Gupta, School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA.
objective	2 Email: harshitg@gatech.edu Summary Internet of Things (IoT) aims to bring every object (eg, smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive volume of data that can overwhelm storage systems and data analytics applications.
objective	3 Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements.
objective	4 However, there are applications such as health monitoring and emergency response that require low latency, and delay that is caused by transferring data to the cloud and then back to the application can seriously impact their performances.
method	5 To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion.
method	6 To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed.
method	7 The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput.
method	8 To this end, we need an evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner.
result	9 In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in latency, network congestion, energy consumption, and cost.
method	10 We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies.

background	1 Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models.
background	2 Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complexmodels.
background	3 This class ofMCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available.
background	4 PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed.
background	5 Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code.
method	6 The lack of a domain specific language allows for great flexibility and direct interaction with the model.
method	7 This paper is a tutorial-style introduction to this software package.
result	8 Subjects Data Mining and Machine Learning, Data Science, Scientific Computing and Simulation

background	1 Support vector machines (SVMs) have been promising methods for classification and regression analysis due to their solid mathematical foundations, which include two desirable properties: margin maximization and nonlinear classification using kernels.
background	2 However, despite these prominent properties, SVMs are usually not chosen for large-scale data mining problems because their training complexity is highly dependent on the data set size.
method	3 Unlike traditional pattern recognition and machine learning, real-world data mining applications often involve huge numbers of data records.
background	4 Thus it is too expensive to perform multiple scans on the entire data set, and it is also infeasible to put the data set in memory.
background	5 This paper presents a method, Clustering-Based SVM (CB-SVM), that maximizes the SVM performance for very large data sets given a limited amount of resource, e.g., memory.
result	6 CB-SVM applies a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples.
result	7 These samples carry statistical summaries of the data and maximize the benefit of learning.
result	8 Our analyses show that the training complexity of CB-SVM is quadratically dependent on the number of support vectors, which is usually much less than that of the entire data set.
other	9 Our experiments on synthetic and real-world data sets show that CB-SVM is highly scalable for very large data sets and very accurate in terms of classification.

method	1 Automatic detection of pavement cracks is an important task in transportation maintenance for driving safety assurance.
objective	2 However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavement and possible shadows with similar intensity.
background	3 Inspired by recent success on applying deep learning to computer vision and medical problems, a deep-learning based method for crack detection is proposed in this paper.
result	4 A supervised deep convolutional neural network is trained to classify each image patch in the collected images.
result	5 Quantitative evaluation conducted on a data set of 500 images of size 3264 χ 2448, collected by a low-cost smart phone, demonstrates that the learned deep features with the proposed deep learning framework provide superior crack detection performance when compared with features extracted with existing hand-craft methods.

background	1 ÐHidden Markov models (HMMs) are stochastic models capable of statistical learning and classification.
background	2 They have been applied in speech recognition and handwriting recognition because of their great adaptability and versatility in handling sequential signals.
background	3 On the other hand, as these models have a complex structure and also because the involved data sets usually contain uncertainty, it is difficult to analyze the multiple observation training problem without certain assumptions.
background	4 For many years researchers have used Levinson's training equations in speech and handwriting applications, simply assuming that all observations are independent of each other.
objective	5 This paper presents a formal treatment of HMM multiple observation training without imposing the above assumption.
method	6 In this treatment, the multiple observation probability is expressed as a combination of individual observation probabilities without losing generality.
method	7 This combinatorial method gives one more freedom in making different dependence-independence assumptions.
method	8 By generalizing Baum's auxiliary function into this framework and building up an associated objective function using the Lagrange multiplier method, it is proven that the derived training equations guarantee the maximization of the objective function.
result	9 Furthermore, we show that Levinson's training equations can be easily derived as a special case in this treatment.
background	10 Index TermsÐHidden Markov model, forward-backward procedure, Baum-Welch algorithm, multiple observation training.

background	1 Mobile technology has become increasingly common in today’s everyday life.
background	2 However, mobile payment is surprisingly not among the frequently used mobile services, although technologically advanced solutions exist.
background	3 Apparently, there is still a lack of acceptance of mobile payment services among consumers.
method	4 The conceptual model developed and tested in this research thus focuses on factors determining consumers’ acceptance of mobile payment services.
result	5 The empirical results show particularly strong support for the effects of compatibility, individual mobility, and subjective norm.
result	6 Our study offers several implications for managers in regards to marketing mobile payment solutions to increase consumers’ intention to use these services.
other	7 2009 Elsevier B.V. All rights reserved.

background	1 Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs.
result	2 Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks.
background	3 Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively.
method	4 In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results.
method	5 Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected.
result	6 In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network.
result	7 We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features.
result	8 A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.

background	1 Computer graphics research has concentrated on creating photo-realistic images of synthetic objects.
objective	2 These images communicate surface shading and curvature, as well as the depth relationships of objects in a scene.
method	3 These renderings are traditionally represented by a rectangular array of pixels that tile the image plane.
method	4 As an alternative to photo-realism, it is possible to create abstract images using an ordered collection of brush strokes.
result	5 These abstract images filter and refine visual information before it is presented to the viewer.
result	6 By controlling the color, shape, size, and orientation of individual brush strokes, impressionistic paintings of computer generated or photographic images can easily be created.

background	1 Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs.
background	2 Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort.
background	3 With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features.
background	4 However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.
objective	5 In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems.
method	6 We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps.
result	7 Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models.
result	8 We have also open-sourced our implementation in TensorFlow.

objective	1 In this paper we discuss the development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis.
method	2 The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed.
result	3 The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying datasets.

background	1 Bitcoins have recently become an increasingly popular cryptocurrency through which users trade electronically and more anonymously than via traditional electronic transfers.
background	2 Bitcoin’s design keeps all transactions in a public ledger.
background	3 The sender and receiver for each transaction are identified only by cryptographic publickey ids.
background	4 This leads to a common misconception that it inherently provides anonymous use.
background	5 While Bitcoin’s presumed anonymity offers new avenues for commerce, several recent studies raise user-privacy concerns.
objective	6 We explore the level of anonymity in the Bitcoin system.
method	7 Our approach is two-fold: (i) We annotate the public transaction graph by linking bitcoin public keys to real people either definitively or statistically.
method	8 (ii) We run the annotated graph through our graph-analysis framework to find and summarize activity of both known and unknown users.

background	1 Hardware technologies for trusted computing, or trusted execution environments (TEEs), have rapidly matured over the last decade.
background	2 In fact, TEEs are at the brink of widespread commoditization with the recent introduction of Intel Software Guard Extensions (Intel SGX).
background	3 Despite such rapid development of TEE, software technologies for TEE significantly lag behind their hardware counterpart, and currently only a select group of researchers have the privilege of accessing this technology.
objective	4 To address this problem, we develop an open source platform, called OpenSGX, that emulates Intel SGX hardware components at the instruction level and provides new system software components necessarily required for full TEE exploration.
method	5 We expect that the OpenSGX framework can serve as an open platform for SGX research, with the following contributions.
method	6 First, we develop a fully functional, instruction-compatible emulator of Intel SGX for enabling the exploration of software/hardware design space, and development of enclave programs.
method	7 OpenSGX provides a platform for SGX development, meaning that it provides not just emulation but also operating system components, an enclave program loader/packager, an OpenSGX user library, debugging, and performance monitoring.
result	8 Second, to show OpenSGX’s use cases, we applied OpenSGX to protect sensitive information (e.g., directory) of Tor nodes and evaluated their potential performance impacts.
result	9 Therefore, we believe OpenSGX has great potential for broader communities to spark new research on soon-to-becommodity Intel SGX.

objective	1 This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification.
method	2 We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results.
result	3 Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.

background	1 Recently, increasing attention has been directed to the study of the emotional content of speech signals, and hence, many systems have been proposed to identify the emotional content of a spoken utterance.
background	2 This paper is a survey of speech emotion classification addressing three important aspects of the design of a speech emotion recognition system.
background	3 The first one is the choice of suitable features for speech representation.
background	4 The second issue is the design of an appropriate classification scheme and the third issue is the proper preparation of an emotional speech database for evaluating system performance.
background	5 Conclusions about the performance and limitations of current speech emotion recognition systems are discussed in the last section of this survey.
method	6 This section also suggests possible ways of improving speech emotion recognition systems.
result	7 PublishedIn: Pattern Recognition

background	1 Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences.
method	2 We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels.
method	3 Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size.
result	4 Due to their increased parallelism, they are up to 16 times faster at train and test time.
result	5 Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.

background	1 Directed links in social media could represent anything from intimate friendships to common interests, or even a passion for breaking news or celebrity gossip.
background	2 Such directed links determine the flow of information and hence indicate a user’s influence on others—a concept that is crucial in sociology and viral marketing.
background	3 In this paper, using a large amount of data collected from Twitter, we present an in-depth comparison of three measures of influence: indegree, retweets, and mentions.
objective	4 Based on these measures, we investigate the dynamics of user influence across topics and time.
method	5 We make several interesting observations.
objective	6 First, popular users who have high indegree are not necessarily influential in terms of spawning retweets or mentions.
result	7 Second, most influential users can hold significant influence over a variety of topics.
method	8 Third, influence is not gained spontaneously or accidentally, but through concerted effort such as limiting tweets to a single topic.
result	9 We believe that these findings provide new insights for viral marketing and suggest that topological measures such as indegree alone reveals very little about the influence of a user.

background	1 Design thinking (DT) is regarded as a system of three overlapping spaces—viability, desirability, and feasibility—where innovation increases when all three perspectives are addressed.
background	2 Understanding how innovation within teams can be supported by DT methods and tools captivates the interest of business communities.
objective	3 This paper aims to examine how DT methods and tools foster innovation in teams.
result	4 A case study approach, based on two workshops, examined three DT methods with a software tool.
result	5 The findings support the use of DT methods and tools as a way of incubating ideas and creating innovative solutions within teams when team collaboration and software limitations are balanced.
result	6 The paper proposes guidelines for utilizing DT methods and tools in innovation

background	1 Designing convolutional neural networks (CNN) models for mobile devices is challenging because mobile models need to be small and fast, yet still accurate.
background	2 Although significant effort has been dedicated to design and improve mobile models on all three dimensions, it is challenging to manually balance these trade-offs when there are so many architectural possibilities to consider.
objective	3 In this paper, we propose an automated neural architecture search approach for designing resourceconstrained mobile CNN models.
objective	4 We propose to explicitly incorporate latency information into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency.
method	5 Unlike in previous work, where mobile latency is considered via another, often inaccurate proxy (e.g., FLOPS), in our experiments, we directly measure real-world inference latency by executing the model on a particular platform, e.g., Pixel phones.
method	6 To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that permits layer diversity throughout the network.
result	7 Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks.
result	8 On the ImageNet classification task, our model achieves 74.0% top-1 accuracy with 76ms latency on a Pixel phone, which is 1.5× faster than MobileNetV2 (Sandler et al. 2018) and 2.4× faster than NASNet (Zoph et al. 2018) with the same top-1 accuracy.
result	9 On the COCO object detection task, our model family achieves both higher mAP quality and lower latency than MobileNets.

background	1 This paper describes the implementation of general multibody system dynamics on Scissor lift Mechanism (i.e. four bar parallel mechanism) within a bond graph modeling framework.
method	2 Scissor lifting mechanism is the first choice for automobiles and industries for elevation work.
background	3 The system has a one degree of freedom.
method	4 There are several procedures for deriving dynamic equations of rigid bodies in classical mechanics (i.e. Classic Newton-D'Alembert, Newton-Euler, Lagrange, Hamilton, kanes to name a few).
background	5 But these are labor-intensive for large and complicated systems thereby error prone.
method	6 Here the multibody dynamics model of the mechanism is developed in bond graph formalism because it offers flexibility for modeling of closed loop kinematic systems without any causal conflicts and control laws can be included.
method	7 In this work, the mechanism is modeled and simulated in order to evaluate several application-specific requirements such as dynamics, position accuracy etc.
method	8 The proposed multibody dynamics model of the mechanism offers an accurate and fast method to analyze the dynamics of the mechanism knowing that there is no such work available for scissor lifts.
result	9 The simulation gives a clear idea about motor torque sizing for different link lengths of the mechanism over a linear displacement.

background	1 Android is a modern and popular software platform for smartphones.
background	2 Among its predominant features is an advanced security model which is based on application-oriented mandatory access control and sandboxing.
background	3 This allows developers and users to restrict the execution of an application to the privileges it has (mandatorily) assigned at installation time.
background	4 The exploitation of vulnerabilities in program code is hence believed to be confined within the privilege boundaries of an application’s sandbox.
objective	5 However, in this paper we show that a privilege escalation attack is possible.
objective	6 We show that a genuine application exploited at runtime or a malicious application can escalate granted permissions.
result	7 Our results immediately imply that Android’s security model cannot deal with a transitive permission usage attack and Android’s sandbox model fails as a last resort against malware and sophisticated runtime attacks.


