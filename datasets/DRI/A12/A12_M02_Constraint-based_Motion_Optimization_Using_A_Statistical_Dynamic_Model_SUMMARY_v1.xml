<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<Document xmlns:gate="http://www.gate.ac.uk" name="A12_M02_Constraint-based_Motion_Optimization_Using_A_Statistical_Dynamic_Model_SUMMARY_v1.xml">


  
    85466c5c30862095d9d4d95af8bc76dbccf21092676aa1dbb628d1d721ff1e79
    3vt6
    http://dx.doi.org/10.1145/1276377.1276387
  
  
    
      
        <Title>Constraint-based Motion Optimization Using A Statistical Dynamic Model</Title>
      
      
        
          Jinxiang Chai ∗ Texas A&amp;M University
          ∗
        
      
      ∗ e-mail: jchai@cs.tamu.edu † e-mail: jkh@cs.cmu.edu
      
        
        Figure 1: Motions computed from spatial-temporal constraints.
      
      <Abstract>
<Sentence inAbstract="true">In this paper, we present a technique for generating animation from a variety of user-defined constraints.</Sentence> <Sentence inAbstract="true">We pose constraint-based motion synthesis as a maximum a posterior (MAP) problem and develop an optimization framework that generates natural motion satisfying user constraints.</Sentence> <Sentence inAbstract="true">The system automatically learns a statistical dynamic model from motion capture data and then enforces it as a motion prior.</Sentence> <Sentence inAbstract="true">This motion prior, together with user-defined constraints, comprises a trajectory optimization problem.</Sentence> <Sentence inAbstract="true">Solving this problem in the low-dimensional space yields optimal natural motion that achieves the goals specified by the user.</Sentence> <Sentence inAbstract="true">We demonstrate the effectiveness of this approach by generating whole-body and facial motion from a variety of spatial-temporal constraints.</Sentence> <Sentence inAbstract="true">CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—animation; I.3.6 [Computer Graphics]: Methodology and Techniques—interaction techniques Keywords: human body animation, facial animation, motion control, statistical dynamic models, spatial-temporal constraints, constraint-based motion synthesis, motion capture data</Sentence>
</Abstract>
      
        
          Jessica K. Hodgins † Carnegie Mellon University
          †
        
      
    
    
      
        <H1>1 Introduction</H1>
      
      <Sentence inAbstract="false" summaryRelevanceScore="5.00">Our objective in this paper is to design an animation system that allows users to easily create natural-looking character animation by specifying spatial-temporal constraints throughout the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">For
      example, a naive user might use a performance animation system to control the trajectories of the end-positions of the limbs of a character.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">A more skilled user might specify a small set of poses at key time instants.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The system then automatically finds a motion that best satisfies those constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">An ideal motion synthesis system should allow users to specify a variety of constraints either at isolated points or across the entire motion in order to accommodate users with different skill levels.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">One appealing solution to this problem is physically based optimization <CitSpan>[Witkin and Kass 1988]</CitSpan>, which allows the user to specify various constraints throughout the motion and relies on optimization to compute the physically valid motion that best satisfies these constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">Unfortunately, correct physics does not ensure that the motion will appear natural for characters with many degrees of freedom.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.00">Like physically based optimization, we formulate the problem as a trajectory optimization and consider the entire motion simultaneously.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.67">Instead of using the physical laws to generate physically correct animation, we rely on statistical models of human motion to generate a statistically plausible motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Our approach allows the user to generate a wide range of human body and facial animation by specifying spatial-temporal constraints throughout the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">The system automatically learns a statistical dynamic model from motion capture data and then enforces this model as a motion prior.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The statistical dynamic model plays a role similar to that played by the dynamics in physically based optimization because it constrains the motion to only part of the space of possible human motions.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.67">The statistical dynamic model, however, is usually lower dimensional than the dynamics model, making the optimization more efficient, less likely to be subject to local minima, and more likely to produce natural motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We demonstrate the effectiveness of this approach in two domains: human body animation and facial animation.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We show that the system can generate natural-looking animation from key-frame constraints, key-trajectory constraints, and a combination of these two constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, the user can generate a walking animation from a small set of key frames and foot contact constraints ( figure 1 top).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The user can also specify a small set of key trajectories for the root, hands and feet positions to generate a realistic jumping motion ( figure 1 bottom).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The user can fine tune the animation by incrementally modifying the constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, the user can create a slightly different jumping motion by adjusting the positions of both hands at the top of the jump.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The system can generate motions for a character whose skeletal model is markedly different from those of the subjects in the database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">We also show that the system can use a statistical dynamic model learned from a normal walking sequence to create new motion such as walking on a slope.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">The quality of the final animation produced by our system depends on the motion priors derived from the motion capture database and the number of user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We, therefore, evaluate how the database influences the final motion and how increasing or decreasing the number of user-defined constraints influences the final animation.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We also compare alternative techniques for generating animation from user-defined constraints such as linear interpolation, trajectory-based inverse kinematics, and inverse kinematics in a PCA subspace.</Sentence>
      
        <H1>2 Background</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="3.33">In this paper, we construct statistical models from motion capture data and then combine these models with trajectory optimization to generate a motion that satisfies user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Consequently, we discuss related work in constraint-based trajectory optimization and data-driven animation with an emphasis on statistical models.</Sentence>
        
          <H2>2.1 Constraint-based Trajectory Optimization</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="1.33">Trajectory optimization methods, which were first introduced to the graphics community by <CitSpan>Witkin and Kass [1988]</CitSpan>, provide a powerful framework for generating character animation from user-specified constraints, physics constraints, and an objective function that measures the performance of a generated motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Extending this approach to generate natural motion for a full human character has proved to be hard because the system is high dimensional, the physics constraints make it highly nonlinear, and defining an objective function that reliably measures the naturalness of human motion is difficult.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Much of the difficulty in solving this problem appears to result from the physics constraints because optimization without physics is effective for editing <CitSpan>[Gleicher 1998]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Therefore, one way to make the problem tractable is to simplify the governing physical laws.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Both Liu and Popović [2002] and Abe and his colleagues [2004]</CitSpan> showed that many dynamic effects can be preserved by enforcing patterns of linear and angular momentum during the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Reformulating the dynamics to avoid directly computing the torques also provides a significant performance improvement <CitSpan>[Fang and Pollard 2003]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Reducing the number of degrees of freedom to be optimized can also create tractable problems.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, <CitSpan>Popović and Witkin [1999]</CitSpan> showed that significant changes to motion capture data can be made by manually reducing the degrees of freedom to those most important for the task.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Safonova and her colleagues [2004]</CitSpan> demonstrated that an efficient optimization can be achieved in a behavior-specific, low-dimensional space without simplifying the dynamics.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">More recently, <CitSpan>Liu and her colleagues [2005]</CitSpan> introduced a novel optimization framework— Nonlinear Inverse Optimization—for optimizing appropriate parameters of the objective function from a small set of motion examples and then used the estimated parameters to synthesize a new locomotion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Our work also uses a trajectory optimization framework but replaces the physical dynamic model with a statistical dynamic model computed from a motion capture database.</Sentence>
        
        
          <H2>2.2 Data-driven Motion Synthesis</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="3.33">Our approach is also part of an alternative set of techniques that relies on motion data to constrain the search to natural looking motions.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, motion graphs can be used to resequence whole-body or facial motions (see, for example, <CitSpan>[Arikan and Forsyth 2002; Kovar et al. 2002; Lee et al. 2002; Zhang et al. 2004]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">These systems cannot match poses or satisfy such kinematic constraints as end effector constraints unless the motion database happens to contain a motion that satisfies those constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Motion interpolation, on the other hand, does allow isolated constraints to be satisfied (for example, <CitSpan>[Rose et al. 1998; Kovar and Gleicher 2004; Mukai and Kuriyama 2005]</CitSpan>).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">However, interpolation across a complete behavior does not have enough degrees of freedom to allow the specification of full pose constraints or end effector constraints across multiple frames.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Recently, interpolation and motion graphs have been combined to obtain some of the advantages of each approach <CitSpan>[Safonova and Hodgins 2007]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Statistical models of human motion have also been used for motion synthesis.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">A number of researchers have used variants of Hidden Markov Models (HMMs) to statistically represent human motion: either full-body movements <CitSpan>[Molina Tanco and Hilton 2000; Brand and Hertzmann 2000; Galata et al. 2001]</CitSpan> or speechdriven facial expressions <CitSpan>[Bregler et al. 1997; Brand 1999]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">HMMs learned from human motion data have been used to interpolate key frames <CitSpan>[Molina Tanco and Hilton 2000; Galata et al. 2001]</CitSpan>, synthesize a new style of motion <CitSpan>[Brand and Hertzmann 2000]</CitSpan>, and generate facial expressions from speech signals <CitSpan>[Bregler et al. 1997; Brand 1999]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Grzeszczuk and his colleagues[1998]</CitSpan> developed a neural network approximation of dynamics based on simulated data and use it to animate dynamic models such as fish and lunar landers.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Urtasun and her colleagues[2006]</CitSpan> learned linear motion models from pre-aligned motion data via Principal Component Analysis (PCA) and used them to track 3D human body movements from video by performing nonlinear optimization over a small sliding temporal window.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Switching linear dynamic system (SLDS) have also been used to model human motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Pavlović and his colleagues [2000]</CitSpan> present results for human motion synthesis, classification, and visual tracking using learned SLDS models.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Li and his colleagues [2002]</CitSpan> used SLDS to synthesize and edit disco dancing motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Our approach is also to learn a statistical dynamic model from human motion capture data; however, the dynamic behavior of our model is controlled by a continuous control state rather than a discrete hidden state as in HMMs and SLDS.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">This property led us to formulate the motion synthesis problem as a trajectory optimization problem.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.00">More importantly, our system allows the user to specify a variety of spatial-temporal constraints such as end effector constraints throughout the motion, a capability that has not been demonstrated by previous approaches.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">A number of researchers have developed statistical models for human poses and used them to solve the inverse kinematics problem.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">
<CitSpan>Grochow and colleagues [2004]</CitSpan> applied a global nonlinear dimensionality reduction technique, Gaussian Process Latent Variable Model, to human motion data and then used the learned statistical pose model to compute poses from a small set of user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Another solution for data-driven inverse kinematics is to interpolate a small set of preexisting examples using constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">This idea has been used to compute human body poses <CitSpan>[Rose et al. 2001]</CitSpan> and facial expressions <CitSpan>[Zhang et al. 2004]</CitSpan> from kinematic constraints at a single frame.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">These models lack temporal information and therefore cannot be used to generate an animation from sparse constraints such as key frames.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Local statistical models are sufficient if the user provides continuous control signals (the performance animation problem).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">
<CitSpan>Chai and colleagues [2003]</CitSpan> presented a real-time vision-based performance animation system that transforms a small set of automatically tracked facial features into facial animation by interpolating examples in a database at run time.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">They also used a series of local statistical pose models constructed at run time to reconstruct full-body motion from continuous, low-dimensional control signals obtained from video cameras <CitSpan>[Chai and Hodgins 2005]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">The statistical dynamic model used in this paper was motivated by the dynamic model used for video textures by <CitSpan>Soatto and his colleagues [2001]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">They showed that a sequence of images of such moving scenes as sea-waves, smoke, and whirlwinds can be modeled by second-order linear dynamic systems.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">They applied the learned dynamic systems to synthesize an “infinite length” texture sequence by sampling noise from a known Gaussian distribution.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We extend the model to learn an efficient and low-dimensional representation of human motion and use it to generate an animation that achieves the goal specified by the user.</Sentence>
        
      
      
        <H1>3 Overview</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="3.33">The key idea behind our approach is that motion priors learned from prerecorded motion data can be used to create natural human motion that matches constraints specified by the user.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The combination of the motion prior and the user’s constraints provides sufficient information to produce motion with a natural appearance.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The human body motion capture database (about 15 minutes) includes data of locomotion (jumping, running, walking, and hopping) and interacting with the environment (standing up/sitting down, reaching/picking up/placing an object).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The facial expression database (about 9 minutes) includes six basic facial expressions (happiness, surprise, disgust, fear, anger, sadness) and three facial movements related to everyday life (speaking, eating, and snoring).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The motion was captured with a Vicon motion capture system of 12 MX-40 cameras <CitSpan>[Vicon Systems 2004]</CitSpan> with 41 markers for full-body movements and 92 markers for facial expressions.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The motion was captured at 120Hz and then downsampled to 30Hz.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">We denote the set of motion capture data in the database as y 1:N = [y 1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, y N ], where y n , n = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, N, is the measurement of the character’s configuration in the nth frame.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">In facial animation, y n is the 3D positions of all vertices on the face model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">In human body animation, y n is the position and orientation of the root and the joint angles.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We preprocess the motion capture data by applying Principal Component Analysis (PCA) <CitSpan>[Bishop 1996]</CitSpan> to the motion capture data and obtain a reduced subspace representation for y n :</Sentence>
        
          1
          y n = C · x n + D
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">where the vector x n ∈ R d x is a low-dimensional representation of the character configuration y n ∈ R d y .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The matrix C is constructed from the eigenvectors corresponding to the largest eigenvalues of the covariance matrix of the data, and D is the mean of all example data, D = (Σ N n=1 y n )/N .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The dimensionality of the system state, d x , can be automatically determined by choosing the d x for which the singular values drop below a threshold.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The constraints defined by the user are represented by E = {e j |j = 1, ...</Sentence>, J}. <Sentence inAbstract="false" summaryRelevanceScore="1.0">The goal of our constraint-based motion synthesis problem is to create an animation, H, based on the constraints, E. We formulate the constraint-based motion synthesis in a maximum a posterior (MAP) framework and consider the entire motion simultaneously.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">From Bayes’ theorem, the goal of MAP is to infer the
        most likely motion, H, given the user-defined constraints, E:</Sentence>
        
          2
          p (E|H) p (H) arg max H p(H|E) = arg max H p (E) ∝ arg max H p(E|H)p(H)
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">where p(E)is the normalizing constant that ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">In our implementation, we minimize the negative log of p(H|E), yielding the following optimization for motion H: ˆ</Sentence>
        
          3
          H ˆ = arg min H − ln p(E|H) − ln p(H)
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">where the first term measures how well a motion sequence matches the user-specified constraints and the second term measures a priori likelihood of the motion sequence using the knowledge embedded in human motion data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The system contains three major components: Motion prior.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">The system first automatically learns a statistical dynamic model from motion capture data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">This model is then used to compute the motion prior, − ln p(H).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">User-defined Constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The user defines various forms of constraints, E, throughout the motion, which are then used to compute the likelihood term, − ln p(E|H).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The constraints could be any kinematic constraints such as position, orientation, or the distance between two points on the character.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">They could be specified either at isolated points (key frames) or across the whole motion (key trajectories).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Motion optimization.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The system uses trajectory optimization to automatically find an animation H ˆ that best satisfies the userspecified constraints while matching the statistical properties of the motion capture data: H ˆ = arg min H − ln p(E|H) − ln p(H).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">We describe the statistical model in the next section and then present the three components in detail in section 5.</Sentence>
      
      
        <H1>4 Motion Analysis</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">We use an m-order linear time-invariant system to describe the dynamical behavior of the captured motion in the low-dimensional space <CitSpan>[Ljung 1999]</CitSpan>: m</Sentence>
        
          4
          x n = A i x n−i + Bu n i=1
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">where m is the order of the linear dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">x n ∈ R d x and u n ∈ R d u are the system state and control input, and d u is the dimensionality of the control input u n .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">This formulation is similar to the linear time-invariant control system commonly adopted in the control community <CitSpan>[Palm 1999]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">However, the matrix B is not unique because the control input u t is unknown.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Therefore, any non-singular transformation of the matrix B represents the motion because BT and T −1 u n are also consistent with the dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">To remove this ambiguity, we assume that the matrix B is an orthogonal matrix.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Given the low-dimensional representation of the original motion capture data, x 1:N = [x 1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, x N ], we want to identify the statespace model, including system matrices {A i |i = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, m}, B, and the corresponding control input u m+1:N = [u m+1 , ...</Sentence>, u N ]. <Sentence inAbstract="false" summaryRelevanceScore="1.0">The matrices {A i |i = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, m} are dependent on the distribution of u n .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">To eliminate the ambiguity of the matrices A i , we seek to</Sentence>
        
          
          
        
        (a)
        
          Figure 2: The average reconstruction error of the linear time-invariant system computed by cross-validation techniques: (a) The average per frame reconstruction error for the walking test data as a function of the order of the dynamic system (m) and the number of dimensions of the control input (d u ); (b) The average per frame reconstruction error of the facial test data as a function of the order of the dynamic system (m) and the number of dimensions of the control input (d u ).
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">find the {A i |i = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, m} that minimize the sum of the squared control input u n :</Sentence>
        
          5
          A ˆ 1 , ..., A ˆ m = arg min A 1 ,...,A m n u n 2
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">The matrices A i can thus be uniquely found by computing the leastsquare solution:
        A ˆ 1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, A ˆ m = arg min A 1 ,...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">,Am n=m+1 N x n − i=1 m A i x n−i 2 <CitSpan>(6)</CitSpan> We use the estimated matrices {A i |i = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, m} to compute the control input term:</Sentence>
        
          7
          z n = x n − i=1 m A ˆ i x n−i , n = m + 1, ..., N
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">We form a d x × (N − m) matrix by stacking the estimated control inputs z n : z m+1 ...</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">z N = B· u m+1 ...</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">u N <CitSpan>(8)</CitSpan> Z U
        Equation <CitSpan>(8)</CitSpan> shows that without noise, the rank of the matrix Z is d u .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Therefore, we can automatically determine the dimensionality of the control input u n by computing the rank of matrix Z. When noise corrupts the motion capture data, the data matrix Z will not be exactly of rank d u .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">However, we can perform singular value decomposition (SVD) on the data matrix Z such that Z = W SV T , and then get the best possible rank d u approximation of the data matrix, factoring it into two matrices: B ˆ = W and U ˆ = SV T , where B ˆ is a d x × d u matrix and U ˆ is a d u × (T − m) matrix.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The dimensionality of the control input (d u ) can be automatically determined by choosing the d u for which the singular values drop below a threshold.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">Functionally, a statistical dynamic model is similar to a physical dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">For example, given initial values of the system state</Sentence>
        
          4
          (x 1:m = [x 1 , ..., x m ]), the linear dynamic model in Equation
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">can be used to generate an animation (x m+1:T = [x m+1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, x T ]) by sequentially choosing an appropriate value for the control input (u m+1:T = [u m+1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, u T ]), just as joint torques would be used to advance a physical model through time.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The main advantage
        (b)
        of using a statistical dynamic model for animation is that the dimensionality of the control input in a statistical dynamic model is usually much lower than a physical dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">Therefore, the statistical dynamic model might achieve faster convergence and be less subject to local minima.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The number of dimensions of the control input, d u , characterizes the complexity of our dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Figure 2 plots the reconstruction error of a walking test data set and a facial test data set as a function of the order of the dynamic system (m) and the number of dimensions of the control input, d u .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The walk data set is from multiple subjects and contains different styles.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The facial expression data are from the same subject and contain a variety of facial expressions such as “happy” and “sad.</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.33">” The average reconstruction error is the L 2 distance between the original test motion and the motion reconstructed from the linear time-invariant system and computed by cross-validation techniques.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We observe that the reconstruction error of the statistical model decreases as both the order of dynamic system and the number of dimensions of the control input increases.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">If we choose d u as “zero” (simply dropping off the control term), our model becomes the linear dynamic model used by <CitSpan>Soatto and colleagues [2001]</CitSpan> and has the largest reconstruction error.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">If d u is equal to the number of dimensions of the system state d x , the model can be used to represent an arbitrary motion sequence with zero error.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">In practice, human motion is highly coordinated, and the dimensionality of the control input for accurate motion representation, d u , is often much lower than the dimensionality of the system state, d x .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For the examples reported here, we set the dynamic order to three and the dimensionality of control input to four for human body animation (the reconstruction error is about 0.7 degrees/joint per frame); we set the dynamic order to two and the dimensionality of control input to one for facial movement (the reconstruction error is about 0.1 mm/vertex per frame).</Sentence>
      
      
        <H1>5 Constraint-based Motion Synthesis</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">Constraint-based motion synthesis provides the user with intuitive control over the resulting motion: the user specifies a desired motion with various forms of constraints, such as key frames, end effector target positions, or joint angle values; the system then auto-</Sentence>
        
          
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">(a)
        (b)</Sentence>
        
          Figure 3: Various form of spatial-temporal constraints: (a) key-frame constraints for creating full-body animation; (b) key-trajectory constraints where the user selects six points on the character and then specifies their 3D trajectories across the motion (from a performance animation interface); (c) the user picks six points on the face and their screen-space position constraints at some moment in time; (d) the user defines a distance between two facial points (the width of the mouth) and controls the distance throughout the motion.
        
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">matically finds the animation that best satisfies the user-specified constraints while matching the spatial-temporal properties of the motion capture data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">This section first derives the likelihood term, − ln p(E|H), based on the user-defined constraints, E. We then model the motion prior term, − ln p(H), using the learned statistical dynamic model.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Finally, we discuss how to optimize motion by combining both terms: H ˆ = arg min H − ln p(E|H) − ln p(H).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Like physically based optimization <CitSpan>[Witkin and Kass 1988]</CitSpan>, we represent the system state x t and the control signal u t independently.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The motion to be synthesized is therefore represented as a sequence of system states and control inputs H = (x 1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, x T , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, u m+1 , ...</Sentence>, u T ). <Sentence inAbstract="false" summaryRelevanceScore="1.0">The system allows the user to specify various forms of kinematic constraints E = {e j |j = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, J} throughout the motion or at isolated points in the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For facial animation, the user can specify the positions or orientations of any points on the face, or the distance between any two points.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For whole-body animation, the user can specify the positions or orientations of any points on the body, or joint angle values for any joints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Rather than requiring that constraints be specified in 3D, it is often more intuitive to specify where the projection of a point on the character should be located.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Therefore, the system also allows the user to specify the 2D projections of any 3D point on a user-defined screen space.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">This approach could be used for rotoscoping a video, or for a single camera performance animation.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The system allows the user to sketch out the motion in greater or lesser detail.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, a novice user might want to control the paths of specific joints or paths over a period of time using a performance animation system while a more skilled user might prefer using key frame constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Spatially, the constraints could provide either an exact configuration such as a full-body pose or a small subset of the joint angles or end-positions.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Temporally, the constraints could be instantaneous constraints for a particular frame, multiple-frame constraints, or continuous constraints over a period of time.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">User-defined constraints can be linear or nonlinear.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Linear constraints can be used to define joint angle constraints in human body animation and positions in facial animation.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The most common nonlinear constraints in human body animation might be end effector constraints, for example, foot contact constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">In facial animation, nonlinear constraints can be used to specify the distance between two points on the face or 2D projections of 3D facial points.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Figure 3 illustrates the user-defined constraints that were used to generate human body animation and facial animation.</Sentence>
        <H2>5.1 User-defined Constraints</H2>
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">Mathematically, we can model the likelihood term, − ln p(E|H), as
        (c)
        (d)
        follows:
        E constraints = − ln p(E|H) ∼ j=1 J β e j − f j (y 1 , ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, y T ) 2 ∼ j=1 J β e j − f j (Cx 1 + D, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, Cx T + D) 2 <CitSpan>(9)</CitSpan> where the function f j is usually a forward kinematics function and the parameter β is a constant specifying the importance of the constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The likelihood term evaluates how well the synthesized motion matches the constraints specified by the user.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">A good match between the motion and the user-defined constraints results in a low energy solution.</Sentence>
        
          <H2>5.2 Motion Priors</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="1.33">Many motions might satisfy the user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For example, when the user specifies a small set of key frames or key trajectories, the number of constraints is not sufficient to completely determine the whole motion sequence, x 1:T .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">To remove ambiguities, we would like to constrain the generated motion to lie in the space of natural human motions by imposing a prior on the generated motion:</Sentence>
          
            10
            E prior = − ln p(H)
          
          
            10
            = − ln p(x 1:T , u m+1:T )
          
          <Sentence inAbstract="false" summaryRelevanceScore="1.67">Based on the statistical dynamic equation (Equation 4), the current system state x t only depends on the previous system states x t−m:t−1 and the current control input u t .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">We have p(H) = p(x 1:T , u m+1:T ) T = t=m+1 p(x t |x t−1:t−m , u t ) · p(x 1:m , u m+1:T ) <CitSpan>(11)</CitSpan> We assume that the likelihood of the first term on the right side of Equation 11 is measured by the deviation of the statistical dynamic equation (Equation 4).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We have the corresponding energy term E prior dynamic = − ln T t=m+1 p(x t |x t−1:t−m , u t ) ∼ −α T t=m+1 x t − i=1 m A i x t−i − Bu t 2 <CitSpan>(12)</CitSpan> where α is a tuning parameter.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Conceptually, the dynamic prior can be thought as dimensionality reduction of the motion in a spatialtemporal domain.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">It significantly reduces the dimensionality of the motion from the space of x 1:T to the space of the initial state x 1:m and the control input u m+1:T .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The second term on the right side of Equation 11 computes the prior for the initial state, x 1:m , and control input, u m+1:T .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">We assume that both the initial state, x 1:m , and the control input, u t , are independent and identically distributed.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The energy term for the second term on the right side of Equation 11 can be simplified as follows:</Sentence>
          
            
            Figure 4: The horizontal axis shows the iteration number and the vertical axis shows the value of the objective function for wholebody optimization The three colored curves show the evolution of the objective function values with three different initial guesses. The optimization converges within 100 iterations.
          
          
            13
            E prior control = − ln p(x 1:m , u m+1:T ) m T = − t=1 ln p(x t ) − t=m+1 ln p(u t )
          
          <Sentence inAbstract="false" summaryRelevanceScore="1.67">We model the control input (u t ) as a mixture with K component Gaussian densities <CitSpan>[Bishop 1996]</CitSpan>:</Sentence>
          
            14
            K p(u t ) = Σ k=1 π k N(u t ; φ k , Λ k )
          
          <Sentence inAbstract="false" summaryRelevanceScore="1.0">where K is the number of Gaussian density models and π k is a mixing parameter that corresponds to the prior probability that u t was generated by the kth component.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The function N(u t ; φ j , Λ j ) denotes the multivariate normal density function with mean φ j and covariance matrix Λ j .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The parameters of the Gaussian mixture models (π k , φ k , Λ k ) are automatically estimated using an Expectation-Maximization (EM) algorithm <CitSpan>[Bishop 1996]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The training data are the values of control inputs { u n } computed from the original motion capture data ({y n |n = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, N }) (see section 4).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The density function of the initial states, p(x t ), t = 1, ...</Sentence>
<Sentence inAbstract="false" summaryRelevanceScore="1.0">, m, is also modeled as a mixture of multivariate Gaussian distributions whose parameters are learned from motion data, x 1:N , using the EM algorithm.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Note that we choose weak priors (static models) to model the priors for both initial states and control inputs so as not to restrict the type of motions the algorithm can generate.</Sentence>
        
        
          <H2>5.3 Motion Optimization</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="2.67">After combining the user-defined constraints and the motion prior, the constraint-based motion synthesis problem becomes the following unconstrained motion optimization problem:</Sentence>
          
            15
            arg min x , u E constraint + E prior dynamic + E prior control
          
          <Sentence inAbstract="false" summaryRelevanceScore="1.0">where x and u are the concatenation of the system states x t and the concatenation of the control signals u t over the entire motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">We follow a standard approach of representing x t and u t using cubic B-splines.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">We solve the optimization problem using sequential quadratic programming (SQP) <CitSpan>[Bazaraa et al. 1993]</CitSpan>, where each iteration solves a quadratic programming subproblem.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The Jacobian matrix and the Hessian matrix of the energy function are symbolically evaluated at each iteration.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We choose all initial values using random values between 0 and 1 except that a linear interpolation of the user-specified keyframe constraints is used for initialization.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">We found that the optimization procedure always converges quickly (usually less than 100 iterations and less than 30 seconds).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Typically, the objective function values decrease rapidly in the early iterations and then level off as they approach the optimal value.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Figure 4 shows the objective function values for three different initial guesses.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.00">Our optimization framework can also be applied to the problem of generating human body motions for a character whose skeletal model is markedly different from the subjects in the database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.67">User-defined constraints for motion retargeting can either be directly computed from the source motion or specified by the user.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">In our experiment, we extract foot positions from a source walking motion and then use it to generate a walking sequence for a new character.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We also add one term in the objective function that measures the difference between the source motion and retargeted motion:</Sentence>
          
            16
            E dif f = t=1 T y t source − Cx t − D 2
          
          <Sentence inAbstract="false" summaryRelevanceScore="1.0">where y t source is the source pose at frame t.</Sentence>
        
      
      
        <H1>6 Results</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="1.67">We test our system by generating both human body animation and facial animation from various forms of user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We also evaluate the performance of our algorithm in terms of the motion priors and user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We learn the statistical model for each individual behavior and use it to generate individual behavior based on user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Two kinds of constraints were used to generate most of the examples in this paper: key-frame constraints and key-trajectory constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We can also combine these two constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For example, a jumping motion can be created by specifying a start pose and the positions of both feet and root throughout the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The accompanying video demonstrates the effectiveness of our system for generating a number of individual behaviors, including walking, running, and jumping.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Our behavior-specific statistical motion model is capable of generating a rich variety of actions.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For example, we can use a small set of key frames and foot contacts to generate normal walking, climbing over an obstacle, a baby walking, and mickey-mouse style walking.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Figure 5 shows sample frames of the results.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.00">Our system can also synthesize motion that transitions from one behavior to another by using the statistical model learned from transition data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">In the accompanying video, we demonstrate that the user can generate a transition from walking to jumping, from walking to sitting down, and from walking to picking up an object (figure 6).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The accompanying video also shows that the system can generate motions for characters with skeletal dimensions different from those in the database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">Figure 7 shows sample frames of the results.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We also show that we can use motion priors learned from a small sequence of a normal walking motion (about 100 frames) to create walking on a slope and walking with small steps.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">The user can refine the animation by incrementally modifying the constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For example, the user can create a slightly different jumping motion by adjusting the positions of both hands at the top of the jump.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Figure 8 shows sample frames of the results.</Sentence>
        <H2>6.1 Full-body Animation</H2>
        
          
          Figure 5: Animation generated by a small set of key frames: (top) baby walking; (middle) running; (bottom) stylized walking.
        
        
          
          Figure 6: Our system can generate a transition from walking to jumping using a small set of key frames.
        
        
          
          Figure 7: Motion generality: (top) The system generates motion for a character whose skeletal dimensions are different from the subjects in the database; (bottom) The system modifies a normal walking motion to create a new motion–walking on a slope.
        
        
          
          Figure 8: The user can fine tune an animation by incrementally adding constraints: (top) jumping generated by the user using five key trajectories (both hands, both feet, and root); (bottom) a slightly different jumping motion generated after adjusting the positions of the hands at the top of the jump.
        
        
          <H2>6.2 Facial Animation</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="2.67">The system learns a single statistical model from the whole facial motion capture database and then uses it to create facial animation with a variety of spatial-temporal constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The following examples are illustrated in the accompanying video: Combination of keyframe and trajectory constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The user can generate realistic facial animation by combining sparse keyframe constraints (three key frames) and sparse trajectory constraints (one trajectory).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Sparse screen constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">The user selects six points on the face and specifies the 2D projections on the screen space at three key instants.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">This type of constraint could be extracted by rotoscoping.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Trajectory constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The user can achieve detailed control over facial movement by specifying the trajectories of a small set of 3D facial points.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The user can also use trajectories of a small set of high-level facial features (the mouth width and height and the openness of the eyes) to generate facial animation.</Sentence>
        
        
          <H2>6.3 Evaluation</H2>
          <Sentence inAbstract="false" summaryRelevanceScore="2.00">The quality of the final animation depends on the motion priors and the user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We, therefore, have designed a number of experiments to evaluate the performance of our algorithm: The importance of the motion priors.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">We evaluate the importance of motion priors by comparing our method against alternative constraint-based motion synthesis methods.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The first method is a simple linear interpolation of key frames.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The second method is trajectory-based inverse kinematics that minimizes the velocity changes of the motion in the original configuration space, y t , without any priors.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">The third method is a simple data-driven inverse kinematics algorithm that minimizes the velocity changes of the motion in a reduced PCA space, x t .</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We compare the methods using key-frame constraints and key-trajectory constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We keep the constraints constant and use a cubic spline to represent the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">The results of this comparison are shown in the video.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">Without the use of the statistical dynamic model, the system can not generate natural motions unless the user specifies a very detailed set of constraints across the entire motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Motion priors from different databases.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We evaluate how the database influences the final motion by keeping the user-defined constraints constant.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">We have experimented with both key-frame and key-trajectory constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">For key-frame constraints, the user defined a sparse set of walking constraints and used them to generate walking motion from the priors learned from a number of different databases.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">We compare the results for a database of general locomotion, running, hopping, jumping and walking.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The accompanying video shows that we can generate a good walking motion with a walking database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The quality of the animation becomes worse when we use a large and general locomotion database to generate walking.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">As would be expected, the system fails to generate a good walking motion if the motion prior is learned from running, hopping, or jumping data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We have tested the creation of jumping motion from key-trajectory jumping constraints when the prior is learned from a database of jumping, general locomotion, or walking.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Similarly, the prior from a walking database fails to generate a good jumping motion because of the mismatch between the prior and the user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Different numbers of constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.33">With an appropriate database, we compare the quality of motions generated by different numbers of constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">More specifically, we take one motion sequence out of the database and use it as a testing sequence.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We then compare the animations created by key frames that are spaced increasingly far apart in time.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We also compare the results by decreasing the number of key trajectories.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">The accompanying video shows that results become worse when we decrease the number of the userdefined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For example, the numerical error increases steadily (0.94, 1.06, 1.81 degrees per joint per frame) when the number of constraints is decreased (6, 4, 2 key frames).</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">We observe a noticeable foot sliding artifact on one foot when two key trajectories (root and one foot) are used to create a walking motion.</Sentence>
        
      
      
        <H1>7 Discussion</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="5.00">We have presented an approach for generating both full-body movement and facial expression from spatial-temporal constraints while matching the statistical properties of a database of captured motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="4.33">The system automatically learns a low-dimensional linear dynamic model from motion capture data and then enforces this as spatial-temporal priors to generate the motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="4.00">The statistical dynamic equations, together with an automatically derived objective function and user-defined constraints, comprise a trajectory optimization problem.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="4.67">Solving this optimization problem in the lowdimensional space yields optimal, natural motion that achieves the goals specified by the user.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="4.00">The system achieves a degree of generality beyond the motion capture data.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">For example, we have generated a motion using constraints that cannot be satisfied directly by any motion in the database and found that the quality of the reconstructed motion was acceptable.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Our video also demonstrates that the system can generate motion for characters whose skeletal models differ significantly from those in the database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.33">However, we have not yet attempted to assess how far the user’s constraints can stray from the motions in the database before the quality of the resulting animation declines to an unacceptable level.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">This statistically based optimization approach complements a physically based optimization approach and offers a few potential advantages.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">First, using a low-dimensional statistical dynamic model for the constrained optimization might achieve faster convergence and be less subject to local minima.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Second, our approach can generate slow and even stylized motions that have proven particularly difficult for physically based optimization.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">Third, the optimization does not require physical models.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Building anatomically accurate physical models for facial animation or whole-body motion remains challenging.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.67">There are two limitations of our approach: an appropriate database must be available and the user cannot specify such dynamic constraints as ground reaction forces or character mass.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.67">The main focus of this paper has been an exploration of the use of prior knowledge in motion capture data to generate natural motion that best satisfies user-defined constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">Another important issue for building any interactive animation system is to design an intuitive interface to specify the desired motion.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="2.00">In our experiments, most of keyframe constraints were modified from example poses in the database.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Foot contact constraints were specified by the user directly.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Key trajectory constraints were extracted from a performance interface using two video cameras <CitSpan>[Chai and Hodgins 2005]</CitSpan>.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.67">Alternatively, the user could rely on commercial animation software such as Maya to specify constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.00">This process is timeconsuming even for a professional artist; it is more difficult for a naive user to specify such constraints.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="3.33">One of immediate directions for future work is, therefore, to design intuitive interfaces that allow the user to specify spatial-temporal constraints quickly and easily.</Sentence>
      
      
        <H1>Acknowledgements</H1>
        <Sentence inAbstract="false" summaryRelevanceScore="1.0">The authors would like to thank Moshe Mahler for his help in modeling and rendering the images for this paper and Autodesk for the donation of Maya software.</Sentence> <Sentence inAbstract="false" summaryRelevanceScore="1.0">Partial support for this research was provided by NSF IIS-0326322.</Sentence>
      
      
        <H1>References</H1>
        
          A BE , Y., L IU , C. K., AND P OPOVI Ć , Z. 2004. Momentumbased parameterization of dynamic character motion. In Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 173–182.
          A RIKAN , O., AND F ORSYTH , D. A. 2002. Interactive motion generation from examples. In ACM Transactions on Graphics. 21(3):483–490.
          B AZARAA , M. S., S HERALI , H. D., AND S HETTY , C. 1993. Nonlinear Programming: Theory and Algorithms. John Wiley and Sons Ltd. 2nd Edition.
          B ISHOP , C. 1996. Neural Network for Pattern Recognition. Cambridge University Press.
          B RAND , M., AND H ERTZMANN , A. 2000. Style machines. In Proceedings of ACM ACM SIGGRAPH 2000. 183–192.
          B RAND , M. E. 1999. Voice puppetry. In Proceedings of ACM SIGGRAPH 1999. 21–28.
          B REGLER , C., C OVELL , M., AND S LANEY , M. 1997. Video rewrite: Driving visual speech with audio. In Proceedings of ACM SIGGRAPH 1997. 353-360.
          C HAI , J., AND H ODGINS , J. 2005. Performance animation from low-dimensional control signals. In ACM Transactions on Graphics. 24(3):686–696.
          C HAI , J., X IAO , J., AND H ODGINS , J. 2003. Vision-based control of 3d facial animation. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 193-206.
          F ANG , A., AND P OLLARD , N. S. 2003. Efficient synthesis of physically valid human motion. In ACM Transactions on Graphics. 22(3):417–426.
          G ALATA , A., J OHNSON , N., AND H OGG , D. 2001. Learning variable length markov models of behavior. In Computer Vision and Image Understanding (CVIU) Journal. 81(3):398-413.
          G LEICHER , M. 1998. Retargeting motion to new characters. In Proceedings of ACM SIGGRAPH 1998. 33-42.
          G ROCHOW , K., M ARTIN , S. L., H ERTZMANN , A., AND P OPOVI C  ́ , Z. 2004. Style-based inverse kinematics. In ACM Transactions on Graphics. 23(3):522–531.
          G RZESZCZUK , R., T ERZOPOULOS , D., AND H INTON , G. 1998. Neuroanimator: Fast neural network emulation and control of physics-based models. In Proceedings of ACM SIGGRAPH 1998. 9-20.
          K OVAR , L., AND G LEICHER , M. 2004. Automated extraction and parameterization of motions in large data sets. In ACM Transactions on Graphics. 23(3):559–568.
          K OVAR , L., G LEICHER , M., AND P IGHIN , F. 2002. Motion graphs. In ACM Transactions on Graphics. 21(3):473–482.
          L EE , J., C HAI , J., R EITSMA , P., H ODGINS , J., AND P OLLARD , N. 2002. Interactive control of avatars animated with human motion data. In ACM Transactions on Graphics. 21(3):491–500.
          L I , Y., W ANG , T., AND S HUM , H.-Y. 2002. Motion texture: A two-level statistical model for character synthesis. In ACM Transactions on Graphics. 21(3):465–472.
          L IU , C. K., AND P OPOVI Ć , Z. 2002. Synthesis of complex dynamic character motion from simple animations. In ACM Transactions on Graphics. 21(3):408–416.
          L IU , K., H ERTZMANN , A., AND P OPOVI Ć , Z. 2005. Learning physics-based motion style with nonlinear inverse optimization. In ACM Transactions on Graphics. 23(3):1071–1081.
          L JUNG , L. 1999. System identification: Theory for the user. Prentice Hall PTR. 2nd Edition.
          M OLINA T ANCO , L., AND H ILTON , A. 2000. Realistic synthesis of novel human movements from a database of motion capture examples. In Proceedings of the Workshop on Human Motion. 137-142.
          M UKAI , T., AND K URIYAMA , S. 2005. Geostatistical motion interpolation. In ACM Transactions on Graphics. 24(3):1062– 1070.
          P ALM , W. J. 1999. Modeling, analysis, and control of dynamic systems. Wiley Publishers. 2nd Edition.
          P AVLOVI C  ́ , V., R EHG , J. M., AND M AC C ORMICK , J. 2000. Learning switching linear models of human motion. In Advances in Neural Information Processing Systems 13, 981–987.
          P OPOVI C  ́ , Z., AND W ITKIN , A. P. 1999. Physically based motion transformation. In Proceedings of ACM SIGGRAPH 1999. 1120.
          R OSE , C., C OHEN , M. F., AND B ODENHEIMER , B. 1998. Verbs and adverbs: Multidimensional motion interpolation. In IEEE Computer Graphics and Applications. 18(5):32–40.
          R OSE , C. F., S LOAN , P.-P. J., AND C OHEN , M. F. 2001. Artistdirected inverse-kinematics using radial basis function interpolation. In Computer Graphics Forum. 20(3):239-250.
          S AFONOVA , A., AND H ODGINS , J. K. 2007. Construction and optimal search of interpolated motion graphs. In ACM Transactions on Graphics. 26(3).
          S AFONOVA , A., H ODGINS , J., AND P OLLARD , N. 2004. Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces. In ACM Transactions on Graphics. 23(3):514–521.
          S OATTO , S., D ORETTO , G., AND W U , Y. N. 2001. Dynamic textures. In Proceedings of International Conference on Computer Vision (ICCV’01). 2:439–446.
          U RTASUN , R., F LEET , D. J., AND F UA , P. 2006. Temporal motion models for monocular and multiview 3d human body tracking. In Computer Vision and Image Understanding (CVIU). 104(2):157177.
          V ICON S YSTEMS , 2004. http://www.vicon.com .
          W ITKIN , A., AND K ASS , M. 1988. Spacetime constraints. In Proceedings of ACM SIGGRAPH 1998. 159–168.
          Z HANG , L., S NAVELY , N., C URLESS , B., AND S EITZ , S. M. 2004. Spacetime faces: high resolution capture for modeling and animation. In ACM Transactions on Graphics. 23(3):548–558.
        
      
    
  

</Document>
